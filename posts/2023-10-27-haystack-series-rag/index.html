<!DOCTYPE html>
<html lang="en">
<head>
  <title>RAG Pipelines from scratch ¬∑ Sara Zan</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="color-scheme" content="light dark">
  <meta name="author" content="Sara Zan">
  <meta name="description" content="Sara Zan's Blog">
  <meta name="keywords" content="blog,developer,personal,python,llm,nlp,swe,software-engineering,open-source,ai,genai">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="RAG Pipelines from scratch ¬∑ Sara Zan">
  <meta name="twitter:description" content="Sara Zan's Blog">
  <meta property="og:url" content="https://www.zansara.dev/posts/2023-10-27-haystack-series-rag/">
  <meta property="og:site_name" content="Sara Zan">
  <meta property="og:title" content="RAG Pipelines from scratch">
  <meta property="og:description" content="Sara Zan's Blog">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
  <meta name="msvalidate.01" content="CD2BB9B57B16AF914327870432D856C1" />
  <meta name="yandex-verification" content="a886d3d5d2b57cb5" />
    <meta name="image" content="/posts/2023-10-27-haystack-series-rag/cover.png">
  <meta name="og:image" content="/posts/2023-10-27-haystack-series-rag/cover.png">
  <meta name="twitter:image" content="https://www.zansara.dev/posts/2023-10-27-haystack-series-rag/cover.png">
  <link rel="canonical" href="https://www.zansara.dev/posts/2023-10-27-haystack-series-rag/">
  <link rel="stylesheet" href="/css/style.css" media="screen">
  <link rel="icon" type="image/svg+xml" href="/assets/avatar/avatar.svg" sizes="any">
  <link rel="icon" type="image/png" href="/assets/avatar/avatar.png" sizes="32x32">
  <link rel="apple-touch-icon" href="/assets/avatar/avatar.png">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+HK:wght@200..900&family=Noto+Serif+Hebrew:wght@100..900&family=Noto+Naskh+Arabic:wght@400..700&family=Noto+Serif+JP&family=Noto+Serif+KR&family=Noto+Serif+SC&family=Noto+Serif+TC&family=Noto+Serif+Thai:wght@100..900&family=Noto+Serif:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
  <script data-goatcounter="https://zansaradev.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
</head>

<body>

  <!-- Theme toggle -->
  <input type="checkbox" id="theme-toggle" hidden>
  <label for="theme-toggle">
    <svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
      <circle cx="12" cy="12" r="10" fill="currentColor" opacity="0.3"/>
      <path d="M12 2 A10 10 0 0 1 12 22 Z" fill="currentColor"/>
    </svg>
  </label>

  <!-- Load theme immediately to avoid flash -->
  <script>
    (function() {
      const themeToggle = document.getElementById('theme-toggle');
      const savedTheme = localStorage.getItem('theme');
      if (savedTheme === 'dark') {
        themeToggle.checked = true;
      }
    })();
  </script>

  <main>

    <nav style="padding: 20px 0 10px 0; display: flex; flex-direction: column; align-items: center; gap: 10px; border-bottom: 1px solid var(--border);">
  <a href="/" style="color: var(--text); text-decoration: none; font-size: 25px; margin: 10px 0;">
    <img src="/assets/avatar/avatar.svg" style="width: 1em; height: 1em; margin-right: 5px; margin-bottom: -2px;">
    Sara Zan's Blog
  </a>
  <div style="display: flex; flex-flow: wrap; gap: 0; justify-content: center;">
    <a href="/about" style="color: var(--text); text-decoration: none; margin: 0 10px;">About</a>
<a href="/posts/" style="color: var(--text); text-decoration: none; margin: 0 10px;">Posts</a>
<a href="/projects/" style="color: var(--text); text-decoration: none; margin: 0 10px;">Projects</a>
<a href="/publications/" style="color: var(--text); text-decoration: none; margin: 0 10px;">Publications</a>
<a href="/talks/" style="color: var(--text); text-decoration: none; margin: 0 10px;">Talks</a>
  </div>
</nav>


    <section>
  <article>
    <header>
      <h1 style="text-align: left;">RAG Pipelines from scratch</h1>
      
<span style="color: var(--muted-text);">Let&#x27;s build a simple RAG Pipeline with Haystack 2.0 by just connecting three components: a Retriever, a PromptBuilder and a Generator.</span>
<br>
      <time style="font-style: italic; line-height: 0.8; font-size: medium; color: var(--muted-text);" datetime="2023-10-27T00:00:00Z">by <a href="/">Sara Zan</a>, October 27, 2023</time>
    </header>

    <img style="width:100%; margin: 20px 0 0 0;" src="/posts/2023-10-27-haystack-series-rag/cover.png" alt="Featured image" />

    <p><em>Last updated: 18/01/2024 - Read it on the <a href="https://haystack.deepset.ai/blog/rag-pipelines-from-scratch" target="_blank" rel="noopener noreferrer">Haystack Blog</a>.</em></p>
<hr />
<p>Retrieval Augmented Generation (RAG) is quickly becoming an essential technique to make LLMs more reliable and effective at answering any question, regardless of how specific. To stay relevant in today's NLP landscape, Haystack must enable it.</p>
<p>Let's see how to build such applications with Haystack 2.0, from a direct call to an LLM to a fully-fledged, production-ready RAG pipeline that scales. At the end of this post, we will have an application that can answer questions about world countries based on data stored in a private database. At that point, the knowledge of the LLM will be only limited by the content of our data store, and all of this can be accomplished without fine-tuning language models.</p>
<div class="notice info">
üí° I recently gave a talk about RAG applications in Haystack 2.0, so if you prefer videos to blog posts, you can find the recording <a href="/talks/2023-10-12-office-hours-rag-pipelines/">here</a>. Keep in mind that the code shown might be outdated.
</div>

<h2>What is RAG?</h2>
<p>The idea of Retrieval Augmented Generation was first defined in a <a href="https://arxiv.org/abs/2005.11401" target="_blank" rel="noopener noreferrer">paper</a> by Meta in 2020. It was designed to solve a few of the inherent limitations of seq2seq models (language models that, given a sentence, can finish writing it for you), such as:</p>
<ul>
<li>Their internal knowledge, as vast as it may be, will always be limited and at least slightly out of date.</li>
<li>They work best on generic topics rather than niche and specific areas unless they're fine-tuned on purpose, which is a costly and slow process.</li>
<li>All models, even those with subject-matter expertise, tend to "hallucinate": they confidently produce false statements backed by apparently solid reasoning.</li>
<li>They cannot reliably cite their sources or tell where their knowledge comes from, which makes fact-checking their replies nontrivial.</li>
</ul>
<p>RAG solves these issues of "grounding" the LLM to reality by providing some relevant, up-to-date, and trusted information to the model together with the question. In this way, the LLM doesn't need to draw information from its internal knowledge, but it can base its replies on the snippets provided by the user.</p>
<p><img alt="RAG Paper diagram" src="/posts/2023-10-27-haystack-series-rag/rag-paper-image-inv.png" title="A visual representation of RAG from the original paper"  class="invertible" /></p>
<p>As you can see in the image above (taken directly from the original paper), a system such as RAG is made of two parts: one that finds text snippets that are relevant to the question asked by the user and a generative model, usually an LLM, that rephrases the snippets into a coherent answer for the question.</p>
<p>Let's build one of these with Haystack 2.0!</p>
<div class="notice info">
üí° Do you want to see this code in action? Check out the Colab notebook <a href="https://colab.research.google.com/drive/1FkDNS3hTO4oPXHFbXQcldls0kf-KTq-r?usp=sharing" target="_blank" rel="noopener noreferrer">here</a> or the gist <a href="https://gist.github.com/ZanSara/0af1c2ac6c71d0a723c179cc6ec1ac41" target="_blank" rel="noopener noreferrer">here</a>.
</div>

<div class="notice warning">

<p>‚ö†Ô∏è Warning: This code was tested on <code>haystack-ai==2.0.0b5</code>. Haystack 2.0 is still unstable, so later versions might introduce breaking changes without notice until Haystack 2.0 is officially released. The concepts and components however stay the same.</p>

</div>

<h2>Generators: Haystack's LLM components</h2>
<p>As every NLP framework that deserves its name, Haystack supports LLMs in different ways. The easiest way to query an LLM in Haystack 2.0 is through a Generator component: depending on which LLM and how you intend to query it (chat, text completion, etc...), you should pick the appropriate class.</p>
<p>We're going to use <code>gpt-3.5-turbo</code> (the model behind ChatGPT) for these examples, so the component we need is <a href="https://docs.haystack.deepset.ai/v2.0/docs/openaigenerator" target="_blank" rel="noopener noreferrer"><code>OpenAIGenerator</code></a>. Here is all the code required to use it to query OpenAI's <code>gpt-3.5-turbo</code> :</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">haystack.components.generators</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAIGenerator</span>

<span class="n">generator</span> <span class="o">=</span> <span class="n">OpenAIGenerator</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">)</span>
<span class="n">generator</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What&#39;s the official language of France?&quot;</span><span class="p">)</span>
<span class="c1"># returns {&quot;replies&quot;: [&#39;The official language of France is French.&#39;]}</span>
</code></pre></div>

<p>You can select your favorite OpenAI model by specifying a <code>model_name</code> at initialization, for example, <code>gpt-4</code>. It also supports setting an <code>api_base_url</code> for private deployments, a <code>streaming_callback</code> if you want to see the output generated live in the terminal, and optional <code>kwargs</code> to let you pass whatever other parameter the model understands, such as the number of answers (<code>n</code>), the temperature (<code>temperature</code>), etc.</p>
<p>Note that in this case, we're passing the API key to the component's constructor. This is unnecessary: <code>OpenAIGenerator</code> can read the value from the <code>OPENAI_API_KEY</code> environment variable and also from the <code>api_key</code> module variable of <a href="https://github.com/openai/openai-python#usage" target="_blank" rel="noopener noreferrer"><code>openai</code>'s SDK</a>.</p>
<p>Right now, Haystack supports HuggingFace models through the <a href="https://docs.haystack.deepset.ai/v2.0/docs/huggingfacelocalgenerator" target="_blank" rel="noopener noreferrer"><code>HuggingFaceLocalGenerator</code></a> and <a href="https://docs.haystack.deepset.ai/v2.0/docs/huggingfacetgigenerator" target="_blank" rel="noopener noreferrer"><code>HuggingFaceTGIGenerator</code></a> components, and many more LLMs are coming soon.</p>
<h2>PromptBuilder: structured prompts from templates</h2>
<p>Let's imagine that our LLM-powered application also comes with some pre-defined questions that the user can select instead of typing in full. For example, instead of asking them to type <code>What's the official language of France?</code>, we let them select <code>Tell me the official languages</code> from a list, and they simply need to type "France" (or "Wakanda" for a change - our chatbot needs some challenges too).</p>
<p>In this scenario, we have two pieces of the prompt: a variable (the country name, like "France") and a prompt template, which in this case is <code>"What's the official language of {{ country }}?"</code></p>
<p>Haystack offers a component that can render variables into prompt templates: it's called <a href="https://docs.haystack.deepset.ai/v2.0/docs/promptbuilder" target="_blank" rel="noopener noreferrer"><code>PromptBuilder</code></a>. As the generators we've seen before, also <code>PromptBuilder</code> is nearly trivial to initialize and use.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">haystack.components.builders</span><span class="w"> </span><span class="kn">import</span> <span class="n">PromptBuilder</span>

<span class="n">prompt_builder</span> <span class="o">=</span> <span class="n">PromptBuilder</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="s2">&quot;What&#39;s the official language of {{ country }}?&quot;</span><span class="p">)</span>
<span class="n">prompt_builder</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">country</span><span class="o">=</span><span class="s2">&quot;France&quot;</span><span class="p">)</span>
<span class="c1"># returns {&#39;prompt&#39;: &quot;What&#39;s the official language of France?&quot;}</span>
</code></pre></div>

<p>Note how we defined a variable, <code>country</code>, by wrapping its name in double curly brackets. PromptBuilder lets you define any input variable that way: if the prompt template was <code>"What's the official language of {{ nation }}?"</code>, the <code>run()</code> method of <code>PromptBuilder</code> would have expected a <code>nation</code> input.</p>
<p>This syntax comes from <a href="https://jinja.palletsprojects.com/en/3.0.x/intro/" target="_blank" rel="noopener noreferrer">Jinja2</a>, a popular templating library for Python. If you have ever used Flask, Django, or Ansible, you will feel at home with <code>PromptBuilder</code>. Instead, if you never heard of any of these libraries, you can check out the <a href="https://jinja.palletsprojects.com/en/3.0.x/templates/" target="_blank" rel="noopener noreferrer">syntax</a> on Jinja's documentation. Jinja has a powerful templating language and offers way more features than you'll ever need in prompt templates, ranging from simple if statements and for loops to object access through dot notation, nesting of templates, variables manipulation, macros, full-fledged import and encapsulation of templates, and more.</p>
<h2>A Simple Generative Pipeline</h2>
<p>With these two components, we can assemble a minimal pipeline to see how they work together. Connecting them is trivial: <code>PromptBuilder</code> generates a <code>prompt</code> output, and <code>OpenAIGenerator</code> expects an input with the same name and type.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">haystack</span><span class="w"> </span><span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">haystack.components.generators</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAIGenerator</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">haystack.components.builders</span><span class="w"> </span><span class="kn">import</span> <span class="n">PromptBuilder</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">add_component</span><span class="p">(</span><span class="s2">&quot;prompt_builder&quot;</span><span class="p">,</span> <span class="n">PromptBuilder</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="s2">&quot;What&#39;s the official language of {{ country }}?&quot;</span><span class="p">))</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">add_component</span><span class="p">(</span><span class="s2">&quot;llm&quot;</span><span class="p">,</span> <span class="n">OpenAIGenerator</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">))</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="s2">&quot;prompt_builder&quot;</span><span class="p">,</span> <span class="s2">&quot;llm&quot;</span><span class="p">)</span>

<span class="n">pipe</span><span class="o">.</span><span class="n">run</span><span class="p">({</span><span class="s2">&quot;prompt_builder&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;country&quot;</span><span class="p">:</span> <span class="s2">&quot;France&quot;</span><span class="p">}})</span>
<span class="c1"># returns {&quot;llm&quot;: {&quot;replies&quot;: [&#39;The official language of France is French.&#39;] }}</span>
</code></pre></div>

<p>Here is the pipeline graph:</p>
<p><img alt="Simple LLM pipeline" src="/posts/2023-10-27-haystack-series-rag/simple-llm-pipeline-inv.png"  class="invertible" /></p>
<h2>Make the LLM cheat</h2>
<p>Building the Generative part of a RAG application was very simple! So far, we only provided the question to the LLM, but no information to base its answers on. Nowadays, LLMs possess a lot of general knowledge, so questions about famous countries such as France or Germany are easy for them to reply to correctly. However, when using an app about world countries, some users may be interested in knowing more about obscure or defunct microstates that don't exist anymore. In this case, ChatGPT is unlikely to provide the correct answer without any help.</p>
<p>For example, let's ask our pipeline something <em>really</em> obscure.</p>
<div class="codehilite"><pre><span></span><code><span class="n">pipe</span><span class="o">.</span><span class="n">run</span><span class="p">({</span><span class="s2">&quot;prompt_builder&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;country&quot;</span><span class="p">:</span> <span class="s2">&quot;the Republic of Rose Island&quot;</span><span class="p">}})</span>
<span class="c1"># returns {</span>
<span class="c1">#     &quot;llm&quot;: {</span>
<span class="c1">#         &quot;replies&quot;: [</span>
<span class="c1">#             &#39;The official language of the Republic of Rose Island was Italian.&#39;</span>
<span class="c1">#         ]</span>
<span class="c1">#     }</span>
<span class="c1"># }</span>
</code></pre></div>

<p>The answer is an educated guess but is not accurate: although it was located just outside of Italy's territorial waters, according to <a href="https://en.wikipedia.org/wiki/Republic_of_Rose_Island" target="_blank" rel="noopener noreferrer">Wikipedia</a> the official language of this short-lived micronation was Esperanto.</p>
<p>How can we get ChatGPT to reply to such a question correctly? One way is to make it "cheat" by providing the answer as part of the question. In fact, <code>PromptBuilder</code> is designed to serve precisely this use case.</p>
<p>Here is our new, more advanced prompt:</p>
<div class="codehilite"><pre><span></span><code>Given the following information, answer the question.
Context: {{ context }}
Question: {{ question }}
</code></pre></div>

<p>Let's build a new pipeline using this prompt!</p>
<div class="codehilite"><pre><span></span><code><span class="n">context_template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Given the following information, answer the question.</span>
<span class="s2">Context: {{ context }}</span>
<span class="s2">Question: {{ question }}</span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="n">language_template</span> <span class="o">=</span> <span class="s2">&quot;What&#39;s the official language of {{ country }}?&quot;</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">add_component</span><span class="p">(</span><span class="s2">&quot;context_prompt&quot;</span><span class="p">,</span> <span class="n">PromptBuilder</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="n">context_template</span><span class="p">))</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">add_component</span><span class="p">(</span><span class="s2">&quot;language_prompt&quot;</span><span class="p">,</span> <span class="n">PromptBuilder</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="n">language_template</span><span class="p">))</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">add_component</span><span class="p">(</span><span class="s2">&quot;llm&quot;</span><span class="p">,</span> <span class="n">OpenAIGenerator</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">))</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="s2">&quot;language_prompt&quot;</span><span class="p">,</span> <span class="s2">&quot;context_prompt.question&quot;</span><span class="p">)</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="s2">&quot;context_prompt&quot;</span><span class="p">,</span> <span class="s2">&quot;llm&quot;</span><span class="p">)</span>

<span class="n">pipe</span><span class="o">.</span><span class="n">run</span><span class="p">({</span>
    <span class="s2">&quot;context_prompt&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="s2">&quot;Rose Island had its own government, currency, post office, and commercial establishments, and the official language was Esperanto.&quot;</span><span class="p">}</span>
    <span class="s2">&quot;language_prompt&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;country&quot;</span><span class="p">:</span> <span class="s2">&quot;the Republic of Rose Island&quot;</span><span class="p">}</span>
<span class="p">})</span>
<span class="c1"># returns {</span>
<span class="c1">#     &quot;llm&quot;: {</span>
<span class="c1">#         &quot;replies&quot;: [</span>
<span class="c1">#             &#39;The official language of the Republic of Rose Island is Esperanto.&#39;</span>
<span class="c1">#         ]</span>
<span class="c1">#     }</span>
<span class="c1"># }</span>
</code></pre></div>

<p>Let's look at the graph of our Pipeline:</p>
<p><img alt="Double PromptBuilder pipeline" src="/posts/2023-10-27-haystack-series-rag/double-promptbuilder-pipeline-inv.png"  class="invertible" /></p>
<p>The beauty of <code>PromptBuilder</code> lies in its flexibility. It allows users to chain instances together to assemble complex prompts from simpler schemas: for example, we used the output of the first <code>PromptBuilder</code> as the value of <code>question</code> in the second prompt.</p>
<p>However, in this specific scenario, we can build a simpler system by merging the two prompts into one.</p>
<div class="codehilite"><pre><span></span><code>Given the following information, answer the question.
Context: {{ context }}
Question: What&#39;s the official language of {{ country }}?
</code></pre></div>

<p>Using this new prompt, the resulting pipeline becomes again very similar to our first.</p>
<div class="codehilite"><pre><span></span><code><span class="n">template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Given the following information, answer the question.</span>
<span class="s2">Context: {{ context }}</span>
<span class="s2">Question: What&#39;s the official language of {{ country }}?</span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">add_component</span><span class="p">(</span><span class="s2">&quot;prompt_builder&quot;</span><span class="p">,</span> <span class="n">PromptBuilder</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="n">template</span><span class="p">))</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">add_component</span><span class="p">(</span><span class="s2">&quot;llm&quot;</span><span class="p">,</span> <span class="n">OpenAIGenerator</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">))</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="s2">&quot;prompt_builder&quot;</span><span class="p">,</span> <span class="s2">&quot;llm&quot;</span><span class="p">)</span>

<span class="n">pipe</span><span class="o">.</span><span class="n">run</span><span class="p">({</span>
    <span class="s2">&quot;prompt_builder&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="s2">&quot;Rose Island had its own government, currency, post office, and commercial establishments, and the official language was Esperanto.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;country&quot;</span><span class="p">:</span> <span class="s2">&quot;the Republic of Rose Island&quot;</span>
    <span class="p">}</span>
<span class="p">})</span>
<span class="c1"># returns {</span>
<span class="c1">#     &quot;llm&quot;: {</span>
<span class="c1">#         &quot;replies&quot;: [</span>
<span class="c1">#             &#39;The official language of the Republic of Rose Island is Esperanto.&#39;</span>
<span class="c1">#         ]</span>
<span class="c1">#     }</span>
<span class="c1"># }</span>
</code></pre></div>

<p><img alt="PromptBuilder with two inputs pipeline" src="/posts/2023-10-27-haystack-series-rag/double-variable-promptbuilder-pipeline-inv.png"  class="invertible" /></p>
<h2>Retrieving the context</h2>
<p>For now, we've been playing with prompts, but the fundamental question remains unanswered: where do we get the correct text snippet for the question the user is asking? We can't expect such information as part of the input: we need our system to be able to fetch this information independently, based uniquely on the query. </p>
<p>Thankfully, retrieving relevant information from large <a href="https://en.wikipedia.org/wiki/Text_corpus" target="_blank" rel="noopener noreferrer">corpora</a> (a technical term for extensive collections of data, usually text) is a task that Haystack excels at since its inception: the components that perform this task are called <a href="https://docs.haystack.deepset.ai/v2.0/docs/retrievers" target="_blank" rel="noopener noreferrer">Retrievers</a>.</p>
<p>Retrieval can be performed on different data sources: to begin, let's assume we're searching for data in a local database, which is the use case that most Retrievers are geared towards.</p>
<p>Let's create a small local database to store information about some European countries. Haystack offers a neat object for these small-scale demos: <code>InMemoryDocumentStore</code>. This document store is little more than a Python dictionary under the hood but provides the same exact API as much more powerful data stores and vector stores, such as <a href="https://github.com/deepset-ai/haystack-core-integrations/tree/main/document_stores/elasticsearch" target="_blank" rel="noopener noreferrer">Elasticsearch</a> or <a href="https://haystack.deepset.ai/integrations/chroma-documentstore" target="_blank" rel="noopener noreferrer">ChromaDB</a>. Keep in mind that the object is called "Document Store" and not simply "datastore" because what it stores is Haystack's Document objects: a small dataclass that helps other components make sense of the data that they receive.</p>
<p>So, let's initialize an <code>InMemoryDocumentStore</code> and write some <code>Documents</code> into it.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">haystack.dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">Document</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">haystack.document_stores.in_memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">InMemoryDocumentStore</span>

<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">Document</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;German is the official language of Germany.&quot;</span><span class="p">),</span> 
    <span class="n">Document</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;The capital of France is Paris, and its official language is French.&quot;</span><span class="p">),</span>
    <span class="n">Document</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Italy recognizes a few official languages, but the most widespread one is Italian.&quot;</span><span class="p">),</span>
    <span class="n">Document</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Esperanto has been adopted as official language for some microstates as well, such as the Republic of Rose Island, a short-lived microstate built on a sea platform in the Adriatic Sea.&quot;</span><span class="p">)</span>
<span class="p">]</span>
<span class="n">docstore</span> <span class="o">=</span> <span class="n">InMemoryDocumentStore</span><span class="p">()</span>
<span class="n">docstore</span><span class="o">.</span><span class="n">write_documents</span><span class="p">(</span><span class="n">documents</span><span class="o">=</span><span class="n">documents</span><span class="p">)</span>

<span class="n">docstore</span><span class="o">.</span><span class="n">filter_documents</span><span class="p">()</span>
<span class="c1"># returns [</span>
<span class="c1">#     Document(content=&quot;German is the official language of Germany.&quot;), </span>
<span class="c1">#     Document(content=&quot;The capital of France is Paris, and its official language is French.&quot;),</span>
<span class="c1">#     Document(content=&quot;Esperanto has been adopted as official language for some microstates as well, such as the Republic of Rose Island, a short-lived microstate built on a sea platform in the Adriatic Sea.&quot;),</span>
<span class="c1">#     Document(content=&quot;Italy recognizes a few official languages, but the most widespread one is Italian.&quot;),</span>
<span class="c1"># ]</span>
</code></pre></div>

<p>Once the document store is set up, we can initialize a retriever. In Haystack 2.0, each document store comes with its own set of highly optimized retrievers: <code>InMemoryDocumentStore</code> offers two, one based on BM25 ranking and one based on embedding similarity.</p>
<p>Let's start with the BM25-based retriever, which is slightly easier to set up. Let's first use it in isolation to see how it behaves.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">haystack.components.retrievers.in_memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">InMemoryBM25Retriever</span>

<span class="n">retriever</span> <span class="o">=</span> <span class="n">InMemoryBM25Retriever</span><span class="p">(</span><span class="n">document_store</span><span class="o">=</span><span class="n">docstore</span><span class="p">)</span>
<span class="n">retriever</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="s2">&quot;Rose Island&quot;</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># returns [</span>
<span class="c1">#     Document(content=&quot;Esperanto has been adopted as official language for some microstates as well, such as the Republic of Rose Island, a short-lived microstate built on a sea platform in the Adriatic Sea.&quot;)</span>
<span class="c1"># ]</span>

<span class="n">retriever</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="s2">&quot;Rose Island&quot;</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># returns [</span>
<span class="c1">#     Document(content=&quot;Esperanto has been adopted as official language for some microstates as well, such as the Republic of Rose Island, a short-lived microstate built on a sea platform in the Adriatic Sea.&quot;)</span>
<span class="c1">#     Document(content=&quot;Italy recognizes a few official languages, but the most widespread one is Italian.&quot;),</span>
<span class="c1">#     Document(content=&quot;The capital of France is Paris, and its official language is French.&quot;),</span>
<span class="c1"># ]</span>
</code></pre></div>

<p>We see that <a href="https://docs.haystack.deepset.ai/v2.0/reference/retriever-api#inmemorybm25retriever" target="_blank" rel="noopener noreferrer"><code>InMemoryBM25Retriever</code></a> accepts a few parameters. <code>query</code> is the question we want to find relevant documents for. In the case of BM25, the algorithm only searches for exact word matches. The resulting retriever is very fast, but it doesn't fail gracefully: it can't handle spelling mistakes, synonyms, or descriptions of an entity. For example, documents containing the word "cat" would be considered irrelevant against a query such as "felines".</p>
<p><code>top_k</code> controls the number of documents returned. We can see that in the first example, only one document is returned, the correct one. In the second, where <code>top_k = 3</code>, the retriever is forced to return three documents even if just one is relevant, so it picks the other two randomly. Although the behavior is not optimal, BM25 guarantees that if there is a document that is relevant to the query, it will be in the first position, so for now, we can use it with <code>top_k=1</code>.</p>
<p>Retrievers also accepts a <code>filters</code> parameter, which lets you pre-filter the documents before retrieval. This is a powerful technique that comes useful in complex applications, but for now we have no use for it. I will talk more in detail about this topic, called metadata filtering, in a later post.</p>
<p>Let's now make use of this new component in our Pipeline. </p>
<h2>Our first RAG Pipeline</h2>
<p>The retriever does not return a single string but a list of Documents. How do we put the content of these objects into our prompt template?</p>
<p>It's time to use Jinja's powerful syntax to do some unpacking on our behalf.</p>
<div class="codehilite"><pre><span></span><code>Given the following information, answer the question.

Context: 
{% for document in documents %}
    {{ document.content }}
{% endfor %}

Question: What&#39;s the official language of {{ country }}?
</code></pre></div>

<p>Notice how, despite the slightly alien syntax for a Python programmer, what the template does is reasonably evident: it iterates over the documents and, for each of them, renders their <code>content</code> field.</p>
<p>With all these pieces set up, we can finally put them all together.</p>
<div class="codehilite"><pre><span></span><code><span class="n">template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Given the following information, answer the question.</span>

<span class="s2">Context: </span>
<span class="s2">{</span><span class="si">% f</span><span class="s2">or document in documents %}</span>
<span class="s2">    {{ document.content }}</span>
<span class="s2">{</span><span class="si">% e</span><span class="s2">ndfor %}</span>

<span class="s2">Question: What&#39;s the official language of {{ country }}?</span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span>

<span class="n">pipe</span><span class="o">.</span><span class="n">add_component</span><span class="p">(</span><span class="s2">&quot;retriever&quot;</span><span class="p">,</span> <span class="n">InMemoryBM25Retriever</span><span class="p">(</span><span class="n">document_store</span><span class="o">=</span><span class="n">docstore</span><span class="p">))</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">add_component</span><span class="p">(</span><span class="s2">&quot;prompt_builder&quot;</span><span class="p">,</span> <span class="n">PromptBuilder</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="n">template</span><span class="p">))</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">add_component</span><span class="p">(</span><span class="s2">&quot;llm&quot;</span><span class="p">,</span> <span class="n">OpenAIGenerator</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">))</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="s2">&quot;retriever&quot;</span><span class="p">,</span> <span class="s2">&quot;prompt_builder.documents&quot;</span><span class="p">)</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="s2">&quot;prompt_builder&quot;</span><span class="p">,</span> <span class="s2">&quot;llm&quot;</span><span class="p">)</span>

<span class="n">pipe</span><span class="o">.</span><span class="n">run</span><span class="p">({</span>
   <span class="s2">&quot;retriever&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="n">country</span><span class="p">},</span>
    <span class="s2">&quot;prompt_builder&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;country&quot;</span><span class="p">:</span> <span class="s2">&quot;the Republic of Rose Island&quot;</span>
    <span class="p">}</span>
<span class="p">})</span>
<span class="c1"># returns {</span>
<span class="c1">#     &quot;llm&quot;: {</span>
<span class="c1">#         &quot;replies&quot;: [</span>
<span class="c1">#             &#39;The official language of the Republic of Rose Island is Esperanto.&#39;</span>
<span class="c1">#         ]</span>
<span class="c1">#     }</span>
<span class="c1"># }</span>
</code></pre></div>

<p><img alt="BM25 RAG Pipeline" src="/posts/2023-10-27-haystack-series-rag/bm25-rag-pipeline-inv.png"  class="invertible" /></p>
<p>Congratulations! We've just built our first, true-to-its-name RAG Pipeline. </p>
<h2>Scaling up: Elasticsearch</h2>
<p>So, we now have our running prototype. What does it take to scale this system up for production workloads?</p>
<p>Of course, scaling up a system to production readiness is no simple task that can be addressed in a paragraph. Still, we can start this journey with one component that can readily be improved: the document store. </p>
<p><code>InMemoryDocumentStore</code> is clearly a toy implementation: Haystack supports much more performant document stores that make more sense to use in a production scenario. Since we have built our app with a BM25 retriever, let's select <a href="https://haystack.deepset.ai/integrations/elasticsearch-document-store" target="_blank" rel="noopener noreferrer">Elasticsearch</a> as our production-ready document store of choice.</p>
<div class="notice warning">
‚ö†Ô∏è Warning: While ES is a valid document store to use in this scenario, nowadays if often makes more sense to choose a more specialized document store such as <a href="https://github.com/deepset-ai/haystack-core-integrations/tree/main/integrations/weaviate" target="_blank" rel="noopener noreferrer">Weaviate</a>, <a href="https://github.com/deepset-ai/haystack-core-integrations/tree/main/integrations/qdrant" target="_blank" rel="noopener noreferrer">Qdrant</a>, and so on. Check <a href="https://github.com/deepset-ai/haystack-core-integrations/tree/main" target="_blank" rel="noopener noreferrer">this page</a> to see which document stores are currently supported for Haystack 2.0.
</div>

<p>How do we use Elasticsearch on our pipeline? All it takes is to swap out <code>InMemoryDocumentStore</code> and <code>InMemoryBM25Retriever</code> with their Elasticsearch counterparts, which offer nearly identical APIs.</p>
<p>First, let's create the document store: we will need a slightly more complex setup to connect to the Elasticearch backend. In this example, we use Elasticsearch version 8.8.0, but every Elasticsearch 8 version should work.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">elasticsearch_haystack.document_store</span><span class="w"> </span><span class="kn">import</span> <span class="n">ElasticsearchDocumentStore</span>

<span class="n">host</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;ELASTICSEARCH_HOST&quot;</span><span class="p">,</span> <span class="s2">&quot;https://localhost:9200&quot;</span><span class="p">)</span>
<span class="n">user</span> <span class="o">=</span> <span class="s2">&quot;elastic&quot;</span>
<span class="n">pwd</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;ELASTICSEARCH_PASSWORD&quot;</span><span class="p">]</span>  <span class="c1"># You need to provide this value</span>

<span class="n">docstore</span> <span class="o">=</span> <span class="n">ElasticsearchDocumentStore</span><span class="p">(</span>
    <span class="n">hosts</span><span class="o">=</span><span class="p">[</span><span class="n">host</span><span class="p">],</span> 
    <span class="n">basic_auth</span><span class="o">=</span><span class="p">(</span><span class="n">user</span><span class="p">,</span> <span class="n">pwd</span><span class="p">),</span> 
    <span class="n">ca_certs</span><span class="o">=</span><span class="s2">&quot;/content/elasticsearch-8.8.0/config/certs/http_ca.crt&quot;</span>
<span class="p">)</span>
</code></pre></div>

<p>Now, let's write again our four documents into the store. In this case, we specify the duplicate policy, so if the documents were already present, they would be overwritten. All Haystack document stores offer three policies to handle duplicates: <code>FAIL</code> (the default), <code>SKIP</code>, and <code>OVERWRITE</code>.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">haystack.document_stores</span><span class="w"> </span><span class="kn">import</span> <span class="n">DuplicatePolicy</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">Document</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;German is the official language of Germany.&quot;</span><span class="p">),</span> 
    <span class="n">Document</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;The capital of France is Paris, and its official language is French.&quot;</span><span class="p">),</span>
    <span class="n">Document</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Italy recognizes a few official languages, but the most widespread one is Italian.&quot;</span><span class="p">),</span>
    <span class="n">Document</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Esperanto has been adopted as official language for some microstates as well, such as the Republic of Rose Island, a short-lived microstate built on a sea platform in the Adriatic Sea.&quot;</span><span class="p">)</span>
<span class="p">]</span>
<span class="n">docstore</span><span class="o">.</span><span class="n">write_documents</span><span class="p">(</span><span class="n">documents</span><span class="o">=</span><span class="n">documents</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">DuplicatePolicy</span><span class="o">.</span><span class="n">OVERWRITE</span><span class="p">)</span>
</code></pre></div>

<p>Once this is done, we are ready to build the same pipeline as before, but using <code>ElasticsearchBM25Retriever</code>.</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">elasticsearch_haystack.bm25_retriever</span><span class="w"> </span><span class="kn">import</span> <span class="n">ElasticsearchBM25Retriever</span>

<span class="n">template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Given the following information, answer the question.</span>

<span class="s2">Context: </span>
<span class="s2">{</span><span class="si">% f</span><span class="s2">or document in documents %}</span>
<span class="s2">    {{ document.content }}</span>
<span class="s2">{</span><span class="si">% e</span><span class="s2">ndfor %}</span>

<span class="s2">Question: What&#39;s the official language of {{ country }}?</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">add_component</span><span class="p">(</span><span class="s2">&quot;retriever&quot;</span><span class="p">,</span> <span class="n">ElasticsearchBM25Retriever</span><span class="p">(</span><span class="n">document_store</span><span class="o">=</span><span class="n">docstore</span><span class="p">))</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">add_component</span><span class="p">(</span><span class="s2">&quot;prompt_builder&quot;</span><span class="p">,</span> <span class="n">PromptBuilder</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="n">template</span><span class="p">))</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">add_component</span><span class="p">(</span><span class="s2">&quot;llm&quot;</span><span class="p">,</span> <span class="n">OpenAIGenerator</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">))</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="s2">&quot;retriever&quot;</span><span class="p">,</span> <span class="s2">&quot;prompt_builder.documents&quot;</span><span class="p">)</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="s2">&quot;prompt_builder&quot;</span><span class="p">,</span> <span class="s2">&quot;llm&quot;</span><span class="p">)</span>

<span class="n">pipe</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="s2">&quot;elasticsearch-rag-pipeline.png&quot;</span><span class="p">)</span>

<span class="n">country</span> <span class="o">=</span> <span class="s2">&quot;the Republic of Rose Island&quot;</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">run</span><span class="p">({</span>
    <span class="s2">&quot;retriever&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="n">country</span><span class="p">},</span>
    <span class="s2">&quot;prompt_builder&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;country&quot;</span><span class="p">:</span> <span class="n">country</span><span class="p">}</span>
<span class="p">})</span>
<span class="c1"># returns {</span>
<span class="c1">#     &quot;llm&quot;: {</span>
<span class="c1">#         &quot;replies&quot;: [</span>
<span class="c1">#             &#39;The official language of the Republic of Rose Island is Esperanto.&#39;</span>
<span class="c1">#         ]</span>
<span class="c1">#     }</span>
<span class="c1"># }</span>
</code></pre></div>

<p><img alt="Elasticsearch RAG Pipeline" src="/posts/2023-10-27-haystack-series-rag/elasticsearch-rag-pipeline-inv.png"  class="invertible" /></p>
<p>That's it! We're now running the same pipeline over a production-ready Elasticsearch instance.</p>
<h2>Wrapping up</h2>
<p>In this post, we've detailed some fundamental components that make RAG applications possible with Haystack: Generators, the PromptBuilder, and Retrievers. We've seen how they can all be used in isolation and how you can make Pipelines out of them to achieve the same goal. Last, we've experimented with some of the (very early!) features that make Haystack 2.0 production-ready and easy to scale up from a simple demo with minimal changes.</p>
<p>However, this is just the start of our journey into RAG. Stay tuned!</p>
<hr />
<p><em>Next: <a href="/posts/2023-11-05-haystack-series-minimal-indexing">Indexing data for RAG applications</a></em></p>
<p><em>Previous: <a href="/posts/2023-10-26-haystack-series-canals">Canals: a new concept of Pipeline</a></em></p>
<p><em>See the entire series here: <a href="/series/haystack-2.0-series/">Haystack 2.0 series</a></em></p>
<p><small><em>Cover image from <a href="https://it.wikipedia.org/wiki/File:Isoladellerose.jpg" target="_blank" rel="noopener noreferrer">Wikipedia</a></em></small></p>

  </article>
</section>


    <footer>
  <section>
    ¬©
    2023 -
    2026 by &MediumSpace; <a href="/"><img src="/assets/avatar/avatar.svg" style="width: 1em; height: 1em; margin-right: 5px;"> Sara Zan</a>
  </section>
</footer>


  </main>

  

  <!-- Theme toggle persistence -->
  <script>
    (function() {
      const themeToggle = document.getElementById('theme-toggle');

      // Load saved theme preference
      const savedTheme = localStorage.getItem('theme');
      if (savedTheme === 'dark') {
        themeToggle.checked = true;
      }

      // Save theme preference on change
      themeToggle.addEventListener('change', function() {
        if (this.checked) {
          localStorage.setItem('theme', 'dark');
        } else {
          localStorage.setItem('theme', 'light');
        }
      });
    })();
  </script>

</body>
</html>
