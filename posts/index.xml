<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Sara Zan</title>
    <link>https://www.zansara.dev/posts/</link>
    <description>Recent content in Posts on Sara Zan</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Wed, 29 Oct 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://www.zansara.dev/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Making sense of KV Cache optimizations, Ep. 4: System-level</title>
      <link>https://www.zansara.dev/posts/2025-10-29-kv-caching-optimizations-system-level/</link>
      <pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2025-10-29-kv-caching-optimizations-system-level/</guid>
      <description>&lt;p&gt;In the previous posts we&amp;rsquo;ve seen &lt;a href=&#34;https://www.zansara.dev/posts/2025-10-23-kv-caching/&#34; &gt;what the KV cache is&lt;/a&gt; and what types of &lt;a href=&#34;https://www.zansara.dev/posts/2025-10-26-kv-caching-optimizations-intro/&#34; &gt;KV Cache management optimizations&lt;/a&gt; exist according to a &lt;a href=&#34;https://arxiv.org/abs/2412.19442&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recent survey&lt;/a&gt;. In this post we are going to focus on &lt;strong&gt;system-level&lt;/strong&gt; KV cache optimizations.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-a-system-level-optimization&#34;&gt;&#xA;  What is a system-level optimization?&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Real hardware is not only made of &amp;ldquo;memory&amp;rdquo; and &amp;ldquo;compute&amp;rdquo;, but is made of several different hardware and OS level elements, each with its specific tradeoff between speed, throughput, latency, and so on. Optimizing the KV cache to leverages this differences is the core idea of the optimizazions we&amp;rsquo;re going to see in this post.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Making sense of KV Cache optimizations, Ep. 3: Model-level</title>
      <link>https://www.zansara.dev/posts/2025-10-28-kv-caching-optimizations-model-level/</link>
      <pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2025-10-28-kv-caching-optimizations-model-level/</guid>
      <description>&lt;p&gt;In the previous posts we&amp;rsquo;ve seen &lt;a href=&#34;https://www.zansara.dev/posts/2025-10-23-kv-caching/&#34; &gt;what the KV cache is&lt;/a&gt; and what types of &lt;a href=&#34;https://www.zansara.dev/posts/2025-10-26-kv-caching-optimizations-intro/&#34; &gt;KV Cache management optimizations&lt;/a&gt; exist according to a &lt;a href=&#34;https://arxiv.org/abs/2412.19442&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recent survey&lt;/a&gt;. In this post we are going to focus on &lt;strong&gt;model-level&lt;/strong&gt; KV cache optimizations.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-a-model-level-optimization&#34;&gt;&#xA;  What is a model-level optimization?&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;We call a model-level optimization any modification of the architecture of the LLM that enables a more efficient reuse of the KV cache. In most cases, to apply these method to an LLM you need to either retrain or at least finetune the model, so it&amp;rsquo;s not easy to apply and is usually baked in advance in of-the-shelf models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Making sense of KV Cache optimizations, Ep. 2: Token-level</title>
      <link>https://www.zansara.dev/posts/2025-10-27-kv-caching-optimizations-token-level/</link>
      <pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2025-10-27-kv-caching-optimizations-token-level/</guid>
      <description>&lt;p&gt;In the previous post we&amp;rsquo;ve seen &lt;a href=&#34;https://www.zansara.dev/posts/2025-10-23-kv-caching/&#34; &gt;what the KV cache is&lt;/a&gt; and what types of &lt;a href=&#34;https://www.zansara.dev/posts/2025-10-26-kv-caching-optimizations-intro/&#34; &gt;KV cache management optimizations&lt;/a&gt; exist according to a &lt;a href=&#34;https://arxiv.org/abs/2412.19442&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recent survey&lt;/a&gt;. In this post we are going to focus on &lt;strong&gt;token-level&lt;/strong&gt; KV cache optimizations.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-a-token-level-optimization&#34;&gt;&#xA;  What is a token-level optimization?&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;The survey defined token-level optimizations every technique that focuses exclusively on improving the KV cache management based on the &lt;strong&gt;characteristics and patterns of the KV pairs&lt;/strong&gt;, without considering enhancements from model architecture improvements or system parallelization techniques.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Making sense of KV Cache optimizations, Ep. 1: An overview</title>
      <link>https://www.zansara.dev/posts/2025-10-26-kv-caching-optimizations-intro/</link>
      <pubDate>Sun, 26 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2025-10-26-kv-caching-optimizations-intro/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://www.zansara.dev/posts/2025-10-23-kv-caching/&#34; &gt;KV cache&lt;/a&gt; is an essential mechanism to avoid the quadratic time complexity of LLM inference and make modern LLMs usable despite huge parameters count and context lengths. However, simply caching everything indiscriminately is not a successful strategy. By swapping time for space complexity, now our problem is &lt;strong&gt;GPU memory&lt;/strong&gt;. Adding more memory can only bring you so far: at some point, you&amp;rsquo;re going to need much more efficient ways to decide what to cache, when and how. But classic cache management techniques were not designed for LLMs, and they often fall short.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How does prompt caching work?</title>
      <link>https://www.zansara.dev/posts/2025-10-23-kv-caching/</link>
      <pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2025-10-23-kv-caching/</guid>
      <description>&lt;hr&gt;&#xA;&lt;p&gt;&lt;em&gt;This is episode 3 of a series of shorter blog posts answering questions I received during the course of my work and reflect common misconceptions and doubts about various generative AI technologies. You can find the whole series here: &lt;a href=&#34;https://www.zansara.dev/series/practical-questions&#34; &gt;Practical Questions&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;In the previous post we saw what is prompt caching, what parts of the prompts is useful to cache, and explained at a high level why it&amp;rsquo;s so effective. In this post I want to go one step further and explain &lt;em&gt;how&lt;/em&gt; in practice inference engines cache prompt prefixes. How can you take a complex system like an LLM, cache some of its computations mid-prompt, and reload them?&lt;/p&gt;</description>
    </item>
    <item>
      <title>What is prompt caching?</title>
      <link>https://www.zansara.dev/posts/2025-10-17-prompt-caching/</link>
      <pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2025-10-17-prompt-caching/</guid>
      <description>&lt;hr&gt;&#xA;&lt;p&gt;&lt;em&gt;This is episode 2 of a series of shorter blog posts answering questions I received during the course of my work and reflect common misconceptions and doubts about various generative AI technologies. You can find the whole series here: &lt;a href=&#34;https://www.zansara.dev/series/practical-questions&#34; &gt;Practical Questions&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;A common piece of advice to improve speed and reduce cost of inference in LLMs is to use prompt caching. However, it&amp;rsquo;s often not clear what this means. What exactly is cached? When and why the improvements are really impactful? Understanding prompt caching starts with a deeper awareness of how computation and costs scale with large contexts.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Why using a reranker?</title>
      <link>https://www.zansara.dev/posts/2025-10-09-rerankers/</link>
      <pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2025-10-09-rerankers/</guid>
      <description>&lt;hr&gt;&#xA;&lt;p&gt;&lt;em&gt;This is episode 1 of a series of shorter blog posts answering questions I received during the course of my work and reflect common misconceptions and doubts about various generative AI technologies. You can find the whole series here: &lt;a href=&#34;https://www.zansara.dev/series/practical-questions&#34; &gt;Practical Questions&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;Retrieval-Augmented Generation (RAG) systems are essential to connect large language models  with external knowledge sources. While in theory the retrieval step is enough to gather documents that are relevant to the user&amp;rsquo;s request, it&amp;rsquo;s often recommended to add an additional ranking step, the &lt;em&gt;reranking&lt;/em&gt;, to further filter the results.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Trying to play &#34;Guess Who&#34; with an LLM</title>
      <link>https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/</link>
      <pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/</guid>
      <description>&lt;style&gt;&#xA;    p img {&#xA;        max-width: 500px;&#xA;    }&#xA;&#xA;    p:has(&gt; img) {&#xA;        text-align:center!important;&#xA;    }&#xA;&#xA;    pre {&#xA;        overflow: wrap;&#xA;    }&#xA;&lt;/style&gt;&#xA;&#xA;&#xA;&lt;p&gt;A few days ago I came to a realization. Modern LLMs can do a lot of things: they can &lt;a href=&#34;https://www.anthropic.com/news/claude-for-chrome&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;use a browser&lt;/a&gt; just like a human, they can (&lt;a href=&#34;https://dynomight.net/chess/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sometimes&lt;/a&gt;) &lt;a href=&#34;https://maxim-saplin.github.io/llm_chess/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;play chess&lt;/a&gt;, and they seem to be so smart that they apparently can be trusted as personal assistants: they can read and reply to emails, organize trips, do shopping online on your behalf, and so on.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Can you really interrupt an LLM?</title>
      <link>https://www.zansara.dev/posts/2025-06-02-can-you-really-interrupt-an-llm/</link>
      <pubDate>Mon, 02 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2025-06-02-can-you-really-interrupt-an-llm/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;&#xA;  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2025-06-02-can-you-really-interrupt-an-llm/Can%20you%20really%20interrupt%20an%20LLM%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/0382cc35-21be-4455-8ff7-dfa6ce99a4f4?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;&#xA;  &lt;audio controls style=&#34;width:100%;&#34;&gt;&#xA;    &lt;source src=&#34;https://www.zansara.dev/posts/2025-06-02-can-you-really-interrupt-an-llm/Can%20you%20really%20interrupt%20an%20LLM%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;&#xA;  &lt;/audio&gt; &#xA;  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;&#xA;    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;&#xA;    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;p&gt;With the recent release of &lt;a href=&#34;https://support.anthropic.com/en/articles/11101966-using-voice-mode-on-claude-mobile-apps&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Voice Mode&lt;/a&gt; for &lt;a href=&#34;https://www.anthropic.com/claude&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Claude&lt;/a&gt;, it seems like Voice AI is a solved problem. Now that LLMs can speak natively, there&amp;rsquo;s apparently no more need for any of the &lt;a href=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/&#34; &gt;complex voice pipelines&lt;/a&gt; that used to be necessary last year: no need to do voice activity detection, no need to pipe data from the speech-to-text model to the LLM and then back to the text-to-speech engine at blazing speed in order to achieve a natural conversation flow. Modern LLMs can &lt;a href=&#34;https://vimeo.com/945587944&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;laugh and sing&lt;/a&gt;: what else could we need?&lt;/p&gt;</description>
    </item>
    <item>
      <title>A simple vibecoding exercise</title>
      <link>https://www.zansara.dev/posts/2025-05-21-vibecoding/</link>
      <pubDate>Wed, 21 May 2025 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2025-05-21-vibecoding/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;&#xA;  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2025-05-21-vibecoding/A%20simple%20vibecoding%20exercise%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/d422a5f5-80b9-432f-a40c-68003e520044?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;&#xA;  &lt;audio controls style=&#34;width:100%;&#34;&gt;&#xA;    &lt;source src=&#34;https://www.zansara.dev/posts/2025-05-21-vibecoding/A%20simple%20vibecoding%20exercise%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;&#xA;  &lt;/audio&gt; &#xA;  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;&#xA;    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;&#xA;    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;p&gt;Sometimes, after an entire day of coding, the last thing you want to do is to code some more. It would be so great if I could just sit down and enjoy some Youtube videos&amp;hellip;&lt;/p&gt;&#xA;&lt;p&gt;Being abroad, most of the videos I watch are in a foreign language, and it helps immensely to have subtitles when I&amp;rsquo;m not in the mood for hard focus. However, Youtube subtitles are often terrible or missing entirely.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Llama Models in the EU</title>
      <link>https://www.zansara.dev/posts/2025-05-16-llama-eu-ban/</link>
      <pubDate>Fri, 16 May 2025 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2025-05-16-llama-eu-ban/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;&#xA;  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2025-05-16-llama-eu-ban/Using%20Llama%20Models%20in%20the%20EU%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/a3c834c1-2832-4b5e-8f3b-27ae94f33dfe?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;&#xA;  &lt;audio controls style=&#34;width:100%;&#34;&gt;&#xA;    &lt;source src=&#34;https://www.zansara.dev/posts/2025-05-16-llama-eu-ban/Using%20Llama%20Models%20in%20the%20EU%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;&#xA;  &lt;/audio&gt; &#xA;  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;&#xA;    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;&#xA;    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://ai.meta.com/blog/llama-4-multimodal-intelligence/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Llama 4 family&lt;/a&gt; has been released over a month ago and I finally found some time to explore it. Or so I wished to do, until I realized one crucial issue with these models:&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;They are banned in the EU.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Apparently Meta can’t be bothered to comply with EU regulations on AI, and therefore opted for a wide ban that should prevent such laws to apply to them. Of course, while this limitation is technically valid for each and every person and company domiciled in the EU, the problem arises primarily for companies that want to use Llama 4 to offer services and for researchers planning to work with these models, be it for evaluation, fine-tuning, distillation or other derivative work. Always keep in mind that I’m not a lawyer, so nothing of what I’m writing here constitutes as legal advice.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Beyond the hype of reasoning models: debunking three common misunderstandings</title>
      <link>https://www.zansara.dev/posts/2025-05-12-beyond-hype-reasoning-models/</link>
      <pubDate>Mon, 12 May 2025 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2025-05-12-beyond-hype-reasoning-models/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;&#xA;  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2025-05-12-beyond-hype-reasoning-models/Beyond%20the%20hype%20of%20reasoning%20models:%20debunking%20three%20common%20misunderstandings%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/9a99c7a3-fda2-49ac-b27e-ed2f0e4e93b9?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;&#xA;  &lt;audio controls style=&#34;width:100%;&#34;&gt;&#xA;    &lt;source src=&#34;https://www.zansara.dev/posts/2025-05-12-beyond-hype-reasoning-models/Beyond%20the%20hype%20of%20reasoning%20models:%20debunking%20three%20common%20misunderstandings%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;&#xA;  &lt;/audio&gt; &#xA;  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;&#xA;    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;&#xA;    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;p&gt;With the release of OpenAI’s o1 and similar models such as DeepSeek R1, Gemini 2.0 Flash Thinking, Phi 4 Reasoning and more, a new type of LLMs entered the scene: the so-called reasoning models. With their unbelievable scores in the toughest benchmarks for machine intelligence, reasoning models immediately got the attention of most AI enthusiasts, sparking speculations about their capabilities and what those could mean for the industry.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building Reliable Voice Bots with Open Source Tools - Part 2</title>
      <link>https://www.zansara.dev/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/</link>
      <pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;&#xA;  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools/Building%20Reliable%20Voice%20Bots%20with%20Open%20Source%20Tools%20-%20Part%201%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/b61c5d80-3b5c-42d7-92b7-bd60607ce454?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;&#xA;  &lt;audio controls style=&#34;width:100%;&#34;&gt;&#xA;    &lt;source src=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools/Building%20Reliable%20Voice%20Bots%20with%20Open%20Source%20Tools%20-%20Part%201%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;&#xA;  &lt;/audio&gt; &#xA;  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;&#xA;    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;&#xA;    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;This is part two of the write-up of my talk at &lt;a href=&#34;https://www.zansara.dev/talks/2024-09-05-building-voice-agents-with-open-source-tools/&#34; &gt;ODSC Europe 2024&lt;/a&gt; and &lt;a href=&#34;https://www.zansara.dev/talks/2024-10-29-odsc-west-voice-agents/&#34; &gt;ODSC West 2024&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;In the last few years, the world of voice agents saw dramatic leaps forward in the state of the art of all its most basic components. Thanks mostly to OpenAI, bots are now able to understand human speech almost like a human would, they&amp;rsquo;re able to speak back with completely naturally sounding voices, and are able to hold a free conversation that feels extremely natural.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building Reliable Voice Bots with Open Source Tools - Part 1</title>
      <link>https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/</link>
      <pubDate>Fri, 20 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;&#xA;  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools/Building%20Reliable%20Voice%20Bots%20with%20Open%20Source%20Tools%20-%20Part%201%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/b01db173-8803-4fb1-a1da-702c0cf5e451?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;&#xA;  &lt;audio controls style=&#34;width:100%;&#34;&gt;&#xA;    &lt;source src=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools/Building%20Reliable%20Voice%20Bots%20with%20Open%20Source%20Tools%20-%20Part%201%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;&#xA;  &lt;/audio&gt; &#xA;  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;&#xA;    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;&#xA;    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;This is part one of the write-up of my talk at &lt;a href=&#34;https://www.zansara.dev/talks/2024-09-05-building-voice-agents-with-open-source-tools/&#34; &gt;ODSC Europe 2024&lt;/a&gt; and &lt;a href=&#34;https://www.zansara.dev/talks/2024-10-29-odsc-west-voice-agents/&#34; &gt;ODSC West 2024&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;In the last few years, the world of voice agents saw dramatic leaps forward in the state of the art of all its most basic components. Thanks mostly to OpenAI, bots are now able to understand human speech almost like a human would, they&amp;rsquo;re able to speak back with completely naturally sounding voices, and are able to hold a free conversation that feels extremely natural.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Agent Compass</title>
      <link>https://www.zansara.dev/posts/2024-06-10-the-agent-compass/</link>
      <pubDate>Mon, 10 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2024-06-10-the-agent-compass/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;&#xA;  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/The%20Agent%20Compass%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/317a5326-a8c5-418b-8ea1-5c7e664b8ea3?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;&#xA;  &lt;audio controls style=&#34;width:100%;&#34;&gt;&#xA;    &lt;source src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/The%20Agent%20Compass%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;&#xA;  &lt;/audio&gt; &#xA;  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;&#xA;    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;&#xA;    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;p&gt;The concept of Agent is one of the vaguest out there in the post-ChatGPT landscape. The word has been used to identify systems that seem to have nothing in common with one another, from complex autonomous research systems down to a simple sequence of two predefined LLM calls. Even the distinction between Agents and techniques such as RAG and prompt engineering seems blurry at best.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Generating creatures with Teranoptia</title>
      <link>https://www.zansara.dev/posts/2024-05-06-teranoptia/</link>
      <pubDate>Mon, 06 May 2024 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2024-05-06-teranoptia/</guid>
      <description>&lt;style&gt;&#xA;    @font-face {&#xA;        font-family: teranoptia;&#xA;        src: url(&#34;/posts/2024-05-06-teranoptia/teranoptia/fonts/Teranoptia-Furiae.ttf&#34;);&#xA;    }&#xA;&#xA;    .teranoptia {&#xA;        font-size: 5rem;&#xA;        font-family: teranoptia;&#xA;        hyphens: none!important;&#xA;        line-height: 70px;&#xA;    }&#xA;&#xA;    .small {&#xA;        font-size:3rem;&#xA;        line-height: 40px;&#xA;    }&#xA;&#xA;    .glyphset {&#xA;        display: flex;&#xA;        flex-wrap: wrap;&#xA;    }&#xA;    .glyphset div {&#xA;        margin: 3px;&#xA;    }&#xA;    .glyphset div p {&#xA;        text-align: center;&#xA;    }&#xA;&#xA;&lt;/style&gt;&#xA;&#xA;&#xA;&lt;p&gt;Having fun with fonts doesn&amp;rsquo;t always mean obsessing over kerning and ligatures. Sometimes, writing text is not even the point!&lt;/p&gt;&#xA;&lt;p&gt;You don&amp;rsquo;t believe it? Type something in here.&lt;/p&gt;</description>
    </item>
    <item>
      <title>RAG, the bad parts (and the good!)</title>
      <link>https://www.zansara.dev/posts/2024-04-29-odsc-east-rag/</link>
      <pubDate>Mon, 29 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2024-04-29-odsc-east-rag/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;&#xA;  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-04-29-odsc-east-rag/RAG,%20the%20bad%20parts%20%28and%20the%20good!%29%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/1e960694-4227-4da4-9a79-158d4ab1fd35?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;&#xA;  &lt;audio controls style=&#34;width:100%;&#34;&gt;&#xA;    &lt;source src=&#34;https://www.zansara.dev/posts/2024-04-29-odsc-east-rag/RAG,%20the%20bad%20parts%20%28and%20the%20good!%29%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;&#xA;  &lt;/audio&gt; &#xA;  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;&#xA;    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;&#xA;    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;This is a writeup of my talk at &lt;a href=&#34;https://www.zansara.dev/talks/2024-04-25-odsc-east-rag/&#34; &gt;ODSC East 2024&lt;/a&gt; and &lt;a href=&#34;https://www.zansara.dev/talks/2024-07-10-europython-rag/&#34; &gt;EuroPython 2024&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;If you&amp;rsquo;ve been at any AI or Python conference this year, there&amp;rsquo;s one acronym that you&amp;rsquo;ve probably heard in nearly every talk: it&amp;rsquo;s RAG. RAG is one of the most used techniques to enhance LLMs in production, but why is it so? And what are its weak points?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Explain me LLMs like I&#39;m five: build a story to help anyone get the idea</title>
      <link>https://www.zansara.dev/posts/2024-04-14-eli5-llms/</link>
      <pubDate>Sun, 14 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2024-04-14-eli5-llms/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;&#xA;  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-04-14-eli5-llms/Explain%20me%20LLMs%20like%20I%27m%20five:%20build%20a%20story%20to%20help%20anyone%20get%20the%20idea%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/22f971d2-4a48-4b15-8f74-8afd8e4a0f1b?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;&#xA;  &lt;audio controls style=&#34;width:100%;&#34;&gt;&#xA;    &lt;source src=&#34;https://www.zansara.dev/posts/2024-04-14-eli5-llms/Explain%20me%20LLMs%20like%20I%27m%20five:%20build%20a%20story%20to%20help%20anyone%20get%20the%20idea%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;&#xA;  &lt;/audio&gt; &#xA;  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;&#xA;    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;&#xA;    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;p&gt;These days everyone&amp;rsquo;s boss seems to want some form of GenAI in their products. That doesn&amp;rsquo;t always make sense: however, understanding when it does and when it doesn&amp;rsquo;t is not obvious even for us experts, and nearly impossible for everyone else.&lt;/p&gt;&#xA;&lt;p&gt;How can we help our colleagues understand the pros and cons of this tech, and figure out when and how it makes sense to use it?&lt;/p&gt;</description>
    </item>
    <item>
      <title>ClozeGPT: Write Anki cloze cards with a custom GPT</title>
      <link>https://www.zansara.dev/posts/2024-02-28-create-anki-cloze-cards-with-custom-gpt/</link>
      <pubDate>Wed, 28 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2024-02-28-create-anki-cloze-cards-with-custom-gpt/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;&#xA;  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-02-28-create-anki-cloze-cards-with-custom-gpt/ClozeGPT:%20Write%20Anki%20cloze%20cards%20with%20a%20custom%20GPT%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/10ebaf1f-dc2f-41ce-a9d6-0ad29238d96e?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;&#xA;  &lt;audio controls style=&#34;width:100%;&#34;&gt;&#xA;    &lt;source src=&#34;https://www.zansara.dev/posts/2024-02-28-create-anki-cloze-cards-with-custom-gpt/ClozeGPT:%20Write%20Anki%20cloze%20cards%20with%20a%20custom%20GPT%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;&#xA;  &lt;/audio&gt; &#xA;  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;&#xA;    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;&#xA;    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;p&gt;As everyone who has been serious about studying with &lt;a href=&#34;https://apps.ankiweb.net/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anki&lt;/a&gt; knows, the first step of the journey is writing your own flashcards. Writing the cards yourself is often cited as the most straigthforward way to make the review process more effective. However, this can become a big chore, and not having enough cards to study is a sure way to not learn anything.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Is RAG all you need? A look at the limits of retrieval augmentation</title>
      <link>https://www.zansara.dev/posts/2024-02-20-is-rag-all-you-need-odsc-east-2024-teaser/</link>
      <pubDate>Wed, 21 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2024-02-20-is-rag-all-you-need-odsc-east-2024-teaser/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;&#xA;  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-02-20-is-rag-all-you-need-odsc-east-2024-teaser/Is%20RAG%20all%20you%20need%20A%20look%20at%20the%20limits%20of%20retrieval%20augmentation%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/ca710e7b-5af7-4733-b3d8-c012758bd171?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;&#xA;  &lt;audio controls style=&#34;width:100%;&#34;&gt;&#xA;    &lt;source src=&#34;https://www.zansara.dev/posts/2024-02-20-is-rag-all-you-need-odsc-east-2024-teaser/Is%20RAG%20all%20you%20need%20A%20look%20at%20the%20limits%20of%20retrieval%20augmentation%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;&#xA;  &lt;/audio&gt; &#xA;  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;&#xA;    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;&#xA;    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;p&gt;Retrieval Augmented Generation (RAG) is by far one of the most popular and effective techniques to bring LLMs to production. Introduced by a Meta &lt;a href=&#34;https://arxiv.org/abs/2005.11401&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; in 2021, it since took off and evolved to become a field in itself, fueled by the immediate benefits that it provides: lowered risk of hallucinations, access to updated information, and so on. On top of this, RAG is relatively cheap to implement for the benefit it provides, especially when compared to costly techniques like LLM finetuning. This makes it a no-brainer for a lot of usecases, to the point that nowadays every production system that uses LLMs in production seems to be implemented as some form of RAG.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Headless WiFi setup on Raspberry Pi OS &#34;Bookworm&#34; without the Raspberry Pi Imager</title>
      <link>https://www.zansara.dev/posts/2024-01-06-raspberrypi-headless-bookworm-wifi-config/</link>
      <pubDate>Sat, 06 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2024-01-06-raspberrypi-headless-bookworm-wifi-config/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;&#xA;  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-01-06-raspberrypi-headless-bookworm-wifi-config/Headless%20WiFi%20setup%20on%20Raspberry%20Pi%20OS%20Bookworm%20without%20the%20Raspberry%20Pi%20Imager%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/6a234a55-265b-4a94-82e7-57bbb05b738e?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;&#xA;  &lt;audio controls style=&#34;width:100%;&#34;&gt;&#xA;    &lt;source src=&#34;https://www.zansara.dev/posts/2024-01-06-raspberrypi-headless-bookworm-wifi-config/Headless%20WiFi%20setup%20on%20Raspberry%20Pi%20OS%20Bookworm%20without%20the%20Raspberry%20Pi%20Imager%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;&#xA;  &lt;/audio&gt; &#xA;  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;&#xA;    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;&#xA;    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;p&gt;Setting up a Raspberry Pi headless without the Raspberry Pi Imager used to be a fairly simple process for the average Linux user, to the point where a how-to and a few searches on the Raspberry Pi forums would sort the process out. After flashing the image with &lt;code&gt;dd&lt;/code&gt;, creating &lt;code&gt;ssh&lt;/code&gt; in the boot partition and populating &lt;code&gt;wpa_supplicant.conf&lt;/code&gt; was normally enough to get started.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The World of Web RAG</title>
      <link>https://www.zansara.dev/posts/2023-11-09-haystack-series-simple-web-rag/</link>
      <pubDate>Thu, 09 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2023-11-09-haystack-series-simple-web-rag/</guid>
      <description>&lt;p&gt;&lt;em&gt;Last updated: 18/01/2024&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;In an earlier post of the Haystack 2.0 series, we&amp;rsquo;ve seen how to build RAG and indexing pipelines. An application that uses these two pipelines is practical if you have an extensive, private collection of documents and need to perform RAG on such data only. However, in many cases, you may want to get data from the Internet: from news outlets, documentation pages, and so on.&lt;/p&gt;&#xA;&lt;p&gt;In this post, we will see how to build a Web RAG application: a RAG pipeline that can search the Web for the information needed to answer your questions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Indexing data for RAG applications</title>
      <link>https://www.zansara.dev/posts/2023-11-05-haystack-series-minimal-indexing/</link>
      <pubDate>Sun, 05 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2023-11-05-haystack-series-minimal-indexing/</guid>
      <description>&lt;p&gt;&lt;em&gt;Last updated: 18/01/2024&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;In the &lt;a href=&#34;https://www.zansara.dev/posts/2023-10-27-haystack-series-rag&#34; &gt;previous post&lt;/a&gt; of the Haystack 2.0 series, we saw how to build RAG pipelines using a generator, a prompt builder, and a retriever with its document store. However, the content of our document store wasn&amp;rsquo;t extensive, and populating one with clean, properly formatted data is not an easy task. How can we approach this problem?&lt;/p&gt;&#xA;&lt;p&gt;In this post, I will show you how to use Haystack 2.0 to create large amounts of documents from a few web pages and write them a document store that you can then use for retrieval.&lt;/p&gt;</description>
    </item>
    <item>
      <title>RAG Pipelines from scratch</title>
      <link>https://www.zansara.dev/posts/2023-10-27-haystack-series-rag/</link>
      <pubDate>Fri, 27 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2023-10-27-haystack-series-rag/</guid>
      <description>&lt;p&gt;&lt;em&gt;Last updated: 18/01/2024 - Read it on the &lt;a href=&#34;https://haystack.deepset.ai/blog/rag-pipelines-from-scratch&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haystack Blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;Retrieval Augmented Generation (RAG) is quickly becoming an essential technique to make LLMs more reliable and effective at answering any question, regardless of how specific. To stay relevant in today&amp;rsquo;s NLP landscape, Haystack must enable it.&lt;/p&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s see how to build such applications with Haystack 2.0, from a direct call to an LLM to a fully-fledged, production-ready RAG pipeline that scales. At the end of this post, we will have an application that can answer questions about world countries based on data stored in a private database. At that point, the knowledge of the LLM will be only limited by the content of our data store, and all of this can be accomplished without fine-tuning language models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A New Approach to Haystack Pipelines</title>
      <link>https://www.zansara.dev/posts/2023-10-26-haystack-series-canals/</link>
      <pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2023-10-26-haystack-series-canals/</guid>
      <description>&lt;p&gt;&lt;em&gt;Updated on 21/12/2023&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;As we have seen in &lt;a href=&#34;https://www.zansara.dev/posts/2023-10-15-haystack-series-pipeline/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the previous episode of this series&lt;/a&gt;, Haystack&amp;rsquo;s Pipeline is a powerful concept that comes with its set of benefits and shortcomings. In Haystack 2.0, the pipeline was one of the first items that we focused our attention on, and it was the starting point of the entire rewrite.&lt;/p&gt;&#xA;&lt;p&gt;What does this mean in practice? Let&amp;rsquo;s look at what Haystack Pipelines in 2.0 will be like, how they differ from their 1.x counterparts, and the pros and cons of this new paradigm.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Haystack&#39;s Pipeline - A Deep Dive</title>
      <link>https://www.zansara.dev/posts/2023-10-15-haystack-series-pipeline/</link>
      <pubDate>Sun, 15 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2023-10-15-haystack-series-pipeline/</guid>
      <description>&lt;p&gt;If you&amp;rsquo;ve ever looked at Haystack before, you must have come across the &lt;a href=&#34;https://docs.haystack.deepset.ai/docs/pipelines&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pipeline&lt;/a&gt;, one of the most prominent concepts of the framework. However, this abstraction is by no means an obvious choice when it comes to NLP libraries. Why did we adopt this concept, and what does it bring us?&lt;/p&gt;&#xA;&lt;p&gt;In this post, I go into all the details of how the Pipeline abstraction works in Haystack now, why it works this way, and its strengths and weaknesses. This deep dive into the current state of the framework is also a premise for the next episode, where I will explain how Haystack 2.0 addresses this version&amp;rsquo;s shortcomings.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Why rewriting Haystack?!</title>
      <link>https://www.zansara.dev/posts/2023-10-11-haystack-series-why/</link>
      <pubDate>Wed, 11 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2023-10-11-haystack-series-why/</guid>
      <description>&lt;p&gt;Before even diving into what Haystack 2.0 is, how it was built, and how it works, let&amp;rsquo;s spend a few words about the whats and the whys.&lt;/p&gt;&#xA;&lt;p&gt;First of all, &lt;em&gt;what is&lt;/em&gt; Haystack?&lt;/p&gt;&#xA;&lt;p&gt;And next, why on Earth did we decide to rewrite it from the ground up?&lt;/p&gt;&#xA;&lt;h3 id=&#34;a-pioneer-framework&#34;&gt;&#xA;  A Pioneer Framework&#xA;  &#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Haystack is a relatively young framework, its initial release dating back to &lt;a href=&#34;https://github.com/deepset-ai/haystack/releases/tag/0.1.0&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 28th, 2019&lt;/a&gt;. Back then, Natural Language Processing was a field that had just started moving its first step outside of research labs, and Haystack was one of the first libraries that promised enterprise-grade, production-ready NLP features. We were proud to enable use cases such as &lt;a href=&#34;https://medium.com/deepset-ai/what-semantic-search-can-do-for-you-ea5b1e8dfa7f&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;semantic search&lt;/a&gt;, &lt;a href=&#34;https://medium.com/deepset-ai/semantic-faq-search-with-haystack-6a03b1e13053&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FAQ matching&lt;/a&gt;, document similarity, document summarization, machine translation, language-agnostic search, and so on.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Haystack 2.0: What is it?</title>
      <link>https://www.zansara.dev/posts/2023-10-10-haystack-series-intro/</link>
      <pubDate>Tue, 10 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2023-10-10-haystack-series-intro/</guid>
      <description>&lt;p&gt;December is finally approaching, and with it the release of a &lt;a href=&#34;https://github.com/deepset-ai/haystack&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haystack&lt;/a&gt; 2.0. At &lt;a href=&#34;https://www.deepset.ai/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;deepset&lt;/a&gt;, we’ve been talking about it for months, we’ve been iterating on the core concepts what feels like a million times, and it looks like we’re finally getting ready for the approaching deadline.&lt;/p&gt;&#xA;&lt;p&gt;But what is it that makes this release so special?&lt;/p&gt;&#xA;&lt;p&gt;In short, Haystack 2.0 is a complete rewrite. A huge, big-bang style change. Almost no code survived the migration unmodified: we’ve been across the entire 100,000+ lines of the codebase and redone everything in under a year. For our small team, this is a huge accomplishment.&lt;/p&gt;</description>
    </item>
    <item>
      <title>An (unofficial) Python SDK for Verbix</title>
      <link>https://www.zansara.dev/posts/2023-09-10-python-verbix-sdk/</link>
      <pubDate>Sun, 10 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2023-09-10-python-verbix-sdk/</guid>
      <description>&lt;p&gt;PyPI package: &lt;a href=&#34;https://pypi.org/project/verbix-sdk/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://pypi.org/project/verbix-sdk/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;GitHub Repo: &lt;a href=&#34;https://github.com/ZanSara/verbix-sdk&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/ZanSara/verbix-sdk&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Minimal Docs: &lt;a href=&#34;https://github.com/ZanSara/verbix-sdk/blob/main/README.md&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/ZanSara/verbix-sdk/blob/main/README.md&lt;/a&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;As part of a larger side project which is still in the works (&lt;a href=&#34;https://github.com/ebisu-flashcards&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ebisu Flashcards&lt;/a&gt;), these days I found myself looking for some decent API for verbs conjugations in different languages. My requirements were &amp;ldquo;simple&amp;rdquo;:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Supports many languages, including Italian, Portuguese and Hungarian&lt;/li&gt;&#xA;&lt;li&gt;Conjugates irregulars properly&lt;/li&gt;&#xA;&lt;li&gt;Offers an API access to the conjugation tables&lt;/li&gt;&#xA;&lt;li&gt;Refuses to conjugate anything except for known verbs&lt;/li&gt;&#xA;&lt;li&gt;(Optional) Highlights the irregularities in some way&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Surprisingly these seem to be a shortage of good alternatives in this field. All websites that host polished conjugation data don&amp;rsquo;t seem to offer API access (looking at you, &lt;a href=&#34;https://conjugator.reverso.net&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reverso&lt;/a&gt; &amp;ndash; you&amp;rsquo;ll get your own post one day), and most of the simples ones use heuristics to conjugate, which makes them very prone to errors. So for now I ended up choosing &lt;a href=&#34;https://verbix.com&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Verbix&lt;/a&gt; to start from.&lt;/p&gt;</description>
    </item>
    <item>
      <title>My Dotfiles</title>
      <link>https://www.zansara.dev/posts/2021-12-11-dotfiles/</link>
      <pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/posts/2021-12-11-dotfiles/</guid>
      <description>&lt;p&gt;GitHub Repo: &lt;a href=&#34;https://github.com/ZanSara/dotfiles&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/ZanSara/dotfiles&lt;/a&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;What Linux developer would I be if I didn&amp;rsquo;t also have my very own dotfiles repo?&lt;/p&gt;&#xA;&lt;p&gt;After many years of iterations I finally found a combination that lasted quite a while, so I figured it&amp;rsquo;s time to treat them as a real project. It was originally optimized for my laptop, but then I realized it works quite well on my three-monitor desk setup as well without major issues.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
