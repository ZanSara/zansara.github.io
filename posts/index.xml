<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Sara Zan</title><link>https://www.zansara.dev/posts/</link><description>Recent content in posts on Sara Zan</description><generator>Custom Script</generator><language>en</language><lastBuildDate>Wed, 04 Feb 2026 00:00:00 +0000</lastBuildDate><item><title>How does LLM memory work?</title><link>https://www.zansara.dev/posts/2026-02-04-how-does-llm-memory-work/</link><pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/posts/2026-02-04-how-does-llm-memory-work/</guid><description>&lt;p&gt;&lt;em&gt;This is episode 6 of a series of shorter blog posts answering questions I received during the course of my work and reflect common misconceptions and doubts about various generative AI technologies. You can find the whole series here: &lt;a href="/series/practical-questions"&gt;Practical Questions&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;People often talk about an LLM "remembering" (or more often "forgetting") things. But how is that possible? LLMs are stateless algorithms that don't inherently have the ability to "remember" anything they see after their training is over. They don't have anything like databases, caches, logs. At inference time, LLMs produce the next token based only on its trained parameters and whatever text you include in the current request.&lt;/p&gt;
&lt;p&gt;So what is "memory" in the context of LLM inference?&lt;/p&gt;
&lt;h2&gt;The chat history&lt;/h2&gt;
&lt;p&gt;When you're having a conversation with an LLM, the LLM does not remember what you've said in your previous messages. Every time it needs to generate a new token it &lt;strong&gt;re-reads everything&lt;/strong&gt; that happened in the conversation so far, plus everything it has generated up to that point, to be able to decide what's the most likely next token. LLMs don't have any internal state: everything is recomputed from scratch for each output token.&lt;/p&gt;
&lt;div class="notice info"&gt;

&lt;p&gt;üí° Methods exist to reduce the time complexity of LLM inference, mostly in the form of smart caching techniques (usually called &lt;a href="/posts/2025-10-17-prompt-caching/"&gt;prompt caching&lt;/a&gt;), but that's a story &lt;a href="/posts/2025-10-23-kv-caching/"&gt;for another blog post&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;This means that the chat history is not part of the LLM, but it's &lt;strong&gt;managed by the application built on top of it&lt;/strong&gt;. It's the app's responsibility to store the chat history across turns and send it back to the LLM each time the user adds a new message to it. &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2026-02-04-how-does-llm-memory-work/naive-chat-history-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;The storage of the chat history is the simplest implementation of what "memory" means for an LLM. We can call it &lt;strong&gt;short-term memory&lt;/strong&gt; and it allows the LLM to have a coherent conversation for many turns.&lt;/p&gt;
&lt;p&gt;However, this approach has a limit: the length of the conversation.&lt;/p&gt;
&lt;h2&gt;The context window&lt;/h2&gt;
&lt;p&gt;LLMs can only process a fixed maximum amount of text at once. This limit is called &lt;strong&gt;context window&lt;/strong&gt; and includes both the user's input (which in turn includes all the chat history up to that point) plus the output tokens the LLM is generating. This is an unavoidable limitation of the architecture of Transformer-based LLMs (which includes all the LLMs you're likely to ever come across). &lt;/p&gt;
&lt;p&gt;So, what happens when the context window fills up? In short, the &lt;strong&gt;LLM will crash&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;To prevent a hard system crash, various LLM applications handle context window overflows differently. The two most basic approaches are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Hard failure (common in APIs):&lt;/strong&gt; If you exceed the model‚Äôs context window, the request fails.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Truncation/sliding window (common in chat apps):&lt;/strong&gt; The application drops older parts of the conversation so the latest turns fit. This means that for each new token you or the LLM are adding to the chat, an older token disappears from the history, and the LLM "forgets" it. In practice, during a conversation this may look like the LLM forgetting older topics of conversation, or losing sight of its original goal, or forgetting the system prompt and other custom instruction you might have given at the start of the chat.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2026-02-04-how-does-llm-memory-work/context-window-overflow-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;However, both of these are just patches over the fundamental problem that LLMs can't remember more than the content of their context window. How do we get around that to achieve long-term memory?&lt;/p&gt;
&lt;h2&gt;LLM memory is context engineering&lt;/h2&gt;
&lt;p&gt;Making LLMs able to remember very long conversations is a &lt;strong&gt;context engineering&lt;/strong&gt; problem: the science of choosing what to put in the LLM's context window at each inference pass. The context window is a limited resource, and the best LLMs applications out there usually shine due to their superior approach to context engineering. The more you can compress the right information into the smallest possible context, the faster, better and cheaper your AI system will be.&lt;/p&gt;
&lt;p&gt;In the case of long-term memory, the core of the problem is choosing what to remember and how to make it fit into the context window. There are three common approaches: &lt;strong&gt;summarization&lt;/strong&gt;, &lt;strong&gt;scratchpad/state&lt;/strong&gt;, and &lt;strong&gt;RAG&lt;/strong&gt;. These are not mutually exclusive, you can mix and match them as needed.&lt;/p&gt;
&lt;h3&gt;Summarization&lt;/h3&gt;
&lt;p&gt;In the case of summarization-style memory, the idea is to "compress the past" to make it fit the context window. You keep recent messages verbatim, but you also maintain a rolling summary of older conversations and/or older messages in the same conversation. When the chat gets long, you update the summary and discard raw older turns.&lt;/p&gt;
&lt;p&gt;This is a pragmatic fit for simple chatbots: most users don't expect perfect recall, but are happy with an LLM that sort of remembers a summary of what they talked about in the past. It's also rather cheap and very simple to implement, which makes it a perfect fit for a quick, initial implementation.&lt;/p&gt;
&lt;p&gt;The main issue with summarization memory is that LLMs often don't know what details must be remembered and what can be discarded, so they're likely to forget some important details and this might frustrate the users. &lt;/p&gt;
&lt;p&gt;In short, summarization memory achieves something very like human memory: infinitely compressible but likely to lose details in arbitrary ways. This works for role-playing chatbots for example, but not for personal assistants that are supposed to remember everything perfectly.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2026-02-04-how-does-llm-memory-work/summarization-memory-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;h3&gt;Scratchpad&lt;/h3&gt;
&lt;p&gt;In order to overcome the fallacies of human memory, people use post-its and notebooks to store important details that can't be forgotten. Turns out that LLMs can do this too! This is called &lt;strong&gt;scratchpad / state&lt;/strong&gt; approach and means that the LLM is now in charge of maintaining a small, structured "state" that represents what the assistant should not forget, such as user preferences current goals, open tasks, todo lists, key decisions, definitions and terminology agreed upon, and more.&lt;/p&gt;
&lt;p&gt;This approach can be implemented in two ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;by giving a scratchpad tool to the LLMs, where the model can choose to write, edit or delete its content at all times,&lt;/li&gt;
&lt;li&gt;by having a separate LLM regularly review the conversation and populate the scratchpad.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In either case, the scratchpad content is then added to the conversation history (for example in the system prompt or in other dedicated sections) and older conversation messages are dropped.&lt;/p&gt;
&lt;p&gt;This approach is far more controllable than summaries, because the LLM can be instructed carefully as of what it's critical to remember and how to save it into the scratchpad. Not only, but the users themselves can be allowed to read and edit the scratchpad to check what the LLM remembers, add more information, or even correct errors.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2026-02-04-how-does-llm-memory-work/scratchpad-memory-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;h3&gt;RAG Memory&lt;/h3&gt;
&lt;p&gt;But what if the scratchpad becomes itself huge and occupies a large share of the context window, or even overflows it? For agents that need to take on huge tasks (for example coding agents and deep research systems) the scratchpad approach might not be enough. &lt;/p&gt;
&lt;p&gt;In this case we can start to treat memory as yet another data source and perform RAG over the scratchpad and/or the conversation history, stored in a vector DB and indexed regularly.&lt;/p&gt;
&lt;p&gt;The advantage of RAG memory is that you can reuse all well-known patterns for RAG, with the only difference that the content to be retrieved is the chat history itself and/or the LLM's notes. &lt;/p&gt;
&lt;p&gt;However, RAG memory suffers from the shortcomings of retrieval: as the retrieval pipeline is never absolutely perfect, you can't expect perfect recall. You'll have to pay attention to the quality of the memory retrieval, evaluate it carefully and regularly, and so on. This adds a new dimension to your agent's evaluation and in general quite a bit of complexity.&lt;/p&gt;
&lt;p&gt;In addition, you may run into an additional problem that's unique to RAG memory: &lt;strong&gt;context stuffing&lt;/strong&gt;. Context stuffing is the presence of retrieved snippets of context that look like prompts: they can cause problems because they might confuse the LLM into following the instruction contained in the retrieved snippet instead of the user's instruction. &lt;/p&gt;
&lt;p&gt;While context stuffing can happen with malicious context snippets in regular RAG, it's also very likely to happen accidentally when implementing RAG-based memory that searches directly into the chat history. This happens because all the retrieved snippets were indeed user's prompts in the past! In this case, it's essential to make sure that the prompt identifies clearly the retrieved snippets as context and not prompts.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2026-02-04-how-does-llm-memory-work/rag-memory-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;That's it! With any of these three approaches, your LLM-base application is now able to remember things long-term.&lt;/p&gt;
&lt;p&gt;However, don't forget that the moment when you add memory to your LLM powered application, you're now &lt;strong&gt;storing user data&lt;/strong&gt;, with all the problems that this brings. You will need to take care of retention, user control over the memorized data, you'll be storing PII and secrets, and in many cases this process needs to be compliant to whatever policy for data retention you may be subject to.&lt;/p&gt;</description></item><item><title>From RAG to AI Agent</title><link>https://www.zansara.dev/posts/2026-01-07-from-rag-to-ai-agent/</link><pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/posts/2026-01-07-from-rag-to-ai-agent/</guid><description>&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;If you're interested in this topic, you can check out the &lt;a href="https://colab.research.google.com/drive/1YVN5GrmZMM7qpI-dbeV-HIYig9XoSk6W?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;Colab notebook&lt;/a&gt; as well.&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;2025 was the year of LLM reasoning. Most LLM providers focused on improving the ability of their LLMs to reason, make decisions, and carry out long-horizon tasks with the least possible amount of human intervention. RAG pipelines, so hyped in the last couple of years, are now a thing of the past: the focus shifted on AI agents, a term that only recently seems to have acquired a &lt;a href="https://simonwillison.net/2025/Sep/18/agents/" target="_blank" rel="noopener noreferrer"&gt;relatively well-defined meaning&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An LLM agent runs tools in a loop to achieve a goal.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While simple, the concept at a first glance might seem to you very far from the one of RAG. But is it?&lt;/p&gt;
&lt;p&gt;In this post I want to show you how you can extend your RAG pipelines step by step to become agents without having to throw away everything you've built so far. In fact, if you have a very good RAG system today, your future agents are bound to have great research skills right away. You may even find that you may be already half-way through the process of converting your pipeline into an agent without knowing it.&lt;/p&gt;
&lt;p&gt;Let's see how it's done.&lt;/p&gt;
&lt;h3&gt;1. Start from basic RAG&lt;/h3&gt;
&lt;p&gt;Our starting point, what's usually called "basic RAG" to distinguish it from more advanced RAG implementations, is a system with a retrieval step (be it vector-based, keyword-based, web search, hybrid, or anything else) that occurs every time the user sends a message to an LLM. Its architecture might look like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2026-01-07-from-rag-to-ai-agent/basic-rag-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;Systems with more than one retriever and/or a reranker step also fall under this category. What's crucial to distinguish basic RAG from more "agentic" versions of it is the fact that the retrieval step runs &lt;em&gt;on every user message&lt;/em&gt; and that &lt;em&gt;the user message is fed directly to the retriever&lt;/em&gt;.&lt;/p&gt;
&lt;h3&gt;2. Add Query Rewrite&lt;/h3&gt;
&lt;p&gt;The first major step towards agentic behavior is the query rewrite step. RAG pipelines with query rewrite don't send the user's message directly to the retriever, but &lt;strong&gt;rewrite it&lt;/strong&gt; to improve the outcomes of the retrieval. &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2026-01-07-from-rag-to-ai-agent/query-rewrite-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;Query rewrite is a bit of a double-edged sword. In some cases it may make your RAG pipeline less reliable, because the LLM may misunderstand your intent and query the retriever with an unexpected prompt. It also introduces a delay, as there is one more round-trip to the LLM to make. However, a well implemented query rewrite step has a huge impact on &lt;strong&gt;follow-up questions&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Think about a conversation like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;User: What do the style guidelines say about the use of colors on our website?&lt;/p&gt;
&lt;p&gt;Assistant: The style guidelines say that all company websites should use a specific palette made of these colors: ....&lt;/p&gt;
&lt;p&gt;User: Why?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The first questions from the user is clear and detailed, so retrieval would probably return relevant results regardless of whether the query gets rewritten or not. However, the second question alone has far too little information to make sense on its own: sending the string "Why?" to a retriever is bound to return only garbage results, which may make the LLM respond something unexpected (and likely wrong). &lt;/p&gt;
&lt;p&gt;In this case, query rewrite fixes the issue by expanding the "Why?" into a more reasonable query, such as "What's the reason the company mandated a specific color palette?" or "Rationale behind the company's brand color palette selection". This query helps the retriever find the type of information that's actually relevant and provide good context for the answer.&lt;/p&gt;
&lt;h3&gt;3. Optional Retrieval&lt;/h3&gt;
&lt;p&gt;Once query rewrite is in place, the next step is to give the pipeline some very basic decisional power. Specifically, I'm talking about &lt;strong&gt;skipping retrieval&lt;/strong&gt; when it's not necessary.&lt;/p&gt;
&lt;p&gt;Think about a conversation like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;User: What do the style guidelines say about the use of colors on our website?&lt;/p&gt;
&lt;p&gt;Assistant: The style guidelines say that all company websites should use a specific palette made of these colors: ....&lt;/p&gt;
&lt;p&gt;User: List the colors as a table.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this case, the LLM needs no additional context to be able to do what the user asks: it's actually better if the retrieval is skipped in order to save time, resources, and avoid potential failures during retrieval that might confuse it (such as the retriever bringing up irrelevant context snippets).&lt;/p&gt;
&lt;p&gt;This means that even before query rewrite we should add another step, where the LLM gets to decide whether we should do any retrieval or not. The final architecture looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2026-01-07-from-rag-to-ai-agent/optional-retrieval-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;div class="notice"&gt;
üí°  Note that this is just a naive implementation. In practice, the decision of retrieving and the query rewrite may be done by the same LLM call to save time. You may also use different LLMs in parallel for different steps, leveraging smarter and more expensive LLMs for the decisional tasks and faster/cheaper ones for the query rewrite and the answer generation.
&lt;/div&gt;

&lt;p&gt;This is a critical step towards an AI agent: we are giving the LLM the power to take a decision, however simple the decision may look. This is the point where you should start to adapt your evaluation framework to measure how effective the LLM is at &lt;strong&gt;taking decisions&lt;/strong&gt;, rather than its skills at interpreting the retrieved context or the effectiveness of your retrieval step alone. This is what Agent evaluation frameworks will do for you (see the bottom of the article for some suggestions).&lt;/p&gt;
&lt;h3&gt;4. The Agentic Loop&lt;/h3&gt;
&lt;p&gt;Once we have this structure in place, we're ready to give the LLM even more autonomy by introducing an &lt;strong&gt;agentic loop&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;Since the LLM is now able to take the decision to retrieve or not retrieve based on the chat history, how about we let the LLM also review what context snippets were returned by the retriever, and decide whether the retrieval was successful or not?&lt;/p&gt;
&lt;p&gt;To build this agentic loop you should add a new step between the retrieval and the generation step, where the retrieved context is sent to the LLM for review. If the LLM believes the context is relevant to the question and sufficient to answer it, the LLM can decide to proceed to the answer generation. If not, the process loops back to the query rewrite stage, and the retrieval runs again with a different query in the hope that better context will be found.&lt;/p&gt;
&lt;p&gt;The resulting architecture looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2026-01-07-from-rag-to-ai-agent/agentic-loop-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;div class="notice"&gt;
üí°  Note that this is also a naive implementation. A few of these decisions can be packed together in a single pass and, again, you can use different LLMs for different tasks.
&lt;/div&gt;

&lt;p&gt;With the introduction of the agentic loop we've crossed the boundary of what constitutes an &lt;strong&gt;AI Agent&lt;/strong&gt;, even though it's still a very simple one. The LLM is now in charge of deciding when the retrieval is good enough, and it can try as many times as it wants (up to a threshold of your choosing) until it's satisfied with the outcome.&lt;/p&gt;
&lt;p&gt;If your retrieval step is well done and effective, this whole architecture may sound pointless. The LLM can hardly get better results by trying again if retrieval is already optimized and query rewriting is not making mistakes, so what's the point? In this case, the introduction of the agentic loop can be seen just as a necessary stepping stone towards the next upgrade: transforming retrieval into a tool.&lt;/p&gt;
&lt;h3&gt;5. Retrieval as a Tool&lt;/h3&gt;
&lt;p&gt;In many advanced RAG pipelines, retrieval of context and tool usage is seen as two very different operations. RAG is usually always on, highly custom, etc. while tools tend to be very small and simple, rarely called by the LLM, and sometimes implemented on standardized protocols like &lt;a href="https://modelcontextprotocol.io/" target="_blank" rel="noopener noreferrer"&gt;MCP&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This distinction is arbitrary and simply due to historical baggage. &lt;strong&gt;Retrieval can be a tool&lt;/strong&gt;, so it's best to treat it like one!&lt;/p&gt;
&lt;p&gt;Once you adopt this mindset, you'll see that the hints were there all along:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We made retrieval optional, so the LLM can choose to either call it or not - like every other tool&lt;/li&gt;
&lt;li&gt;Query rewrite is the LLM choosing what input to provide to the retriever - as it does when it decides to call any other tool&lt;/li&gt;
&lt;li&gt;The retriever returns output that goes into the chat history to be used for the answer's generation - like the output of all other tools.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Transforming retrieval into a tool simplifies our architecture drastically and moves us fully into AI Agent territory:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2026-01-07-from-rag-to-ai-agent/rag-as-tool-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;As you can see:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The decision step is now part of the LLM's answer generation, which can call it as many times as it wants thanks to the tool calling loop&lt;/li&gt;
&lt;li&gt;The query rewrite comes for free as the LLM invokes the retrieval tool&lt;/li&gt;
&lt;li&gt;The retriever's output goes into the chat history to be used to answer the user's request&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At this point it's time to address a common concern. You may have heard elsewhere that implementing retrieval as a tool makes the LLM "forget" to retrieve context when it should rather do it, so the effectiveness of your RAG worsens. This was very real a couple of years ago, but in my experience it's no longer relevant: modern LLMs are now trained to reach for tools all the time, so this problem has largely disappeared.&lt;/p&gt;
&lt;h3&gt;6. Add more tools&lt;/h3&gt;
&lt;p&gt;Congratulations! At this point you can call your system a true AI Agent. However, an agent with only a retrieval tool has limited use. It's time to add other tools!&lt;/p&gt;
&lt;p&gt;To begin with, if your retrieval pipeline has a lot of moving parts (hybrid retriever, web search, image search, SQL queries, etc...) you can consider separating each of them into separate search tools for the LLM to use, or to expose more parameters to let the LLM customize the output mix.&lt;/p&gt;
&lt;p&gt;Once that's done, adding other tools is trivial on a technical level, especially with protocols such as &lt;a href="https://modelcontextprotocol.io/" target="_blank" rel="noopener noreferrer"&gt;MCP&lt;/a&gt;. Using popular, open source MCPs may let you simplify your retrieval tool drastically: for example by leveraging &lt;a href="https://github.com/github/github-mcp-server" target="_blank" rel="noopener noreferrer"&gt;GitHub's MCP&lt;/a&gt; instead of doing code search yourself, or &lt;a href="https://github.com/atlassian/atlassian-mcp-server" target="_blank" rel="noopener noreferrer"&gt;Atlassian's MCPs&lt;/a&gt; instead of custom Jira/Confluence/BitBucket integrations, and so on.&lt;/p&gt;
&lt;p&gt;However, keep in mind that adding too many tools and MCPs can &lt;strong&gt;overwhelm the LLM&lt;/strong&gt;. You should carefully select which tools can expand the most your LLM's ability to solve your user's problems. For example, a GitHub MCP is irrelevant if only very few of your users are developers, and an image generation tool is useless if you're serving only developers. It's easy to overdo it, so make sure to review regularly the tools you make available to your LLM and add/remove them as necessary.&lt;/p&gt;
&lt;p&gt;And in the rare case in which you actually need a lot of tools, consider letting the user plug them in as needed (like the ChatGPT UI does), or adopt a &lt;a href="https://blog.cloudflare.com/code-mode/" target="_blank" rel="noopener noreferrer"&gt;more sophisticated tool calling approach&lt;/a&gt; to make sure to manage the context window effectively.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;That's it! You successfully transformed your RAG pipeline into a simple AI Agent. From here you can expand further by implementing planning steps, sub-agents, and more.&lt;/p&gt;
&lt;p&gt;However, before going further you should remember that your retrieval-oriented metrics now are not sufficient anymore to evaluate the decision making skills of your system. If you've been using a RAG-only eval framework such as RAGAS it's now a good time to move on to a more general-purpose or agent-oriented eval framework, such as &lt;a href="https://deepeval.com" target="_blank" rel="noopener noreferrer"&gt;DeepEval&lt;/a&gt;, &lt;a href="https://galileo.ai/" target="_blank" rel="noopener noreferrer"&gt;Galileo&lt;/a&gt;, &lt;a href="https://arize.com/" target="_blank" rel="noopener noreferrer"&gt;Arize.ai&lt;/a&gt; or any other AI Agent framework of your choice.&lt;/p&gt;
&lt;p&gt;Last but not least: if you want to see this entire process implemented in code, don't miss my workshop at the virtual &lt;a href="https://www.summit.ai/" target="_blank" rel="noopener noreferrer"&gt;Agentic AI Summit&lt;/a&gt; on the 21st of January, 2026! I'll be walking you through the entire process and show you some additional implementation details. See you there!&lt;/p&gt;</description></item><item><title>What are the "experts" in Mixture-of-Experts LLMs?</title><link>https://www.zansara.dev/posts/2025-12-11-what-are-moe-experts/</link><pubDate>Thu, 11 Dec 2025 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/posts/2025-12-11-what-are-moe-experts/</guid><description>&lt;p&gt;&lt;em&gt;This is episode 5 of a series of shorter blog posts answering questions I received during the course of my work and reflect common misconceptions and doubts about various generative AI technologies. You can find the whole series here: &lt;a href="/series/practical-questions"&gt;Practical Questions&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Nearly all popular LLMs share the same internal structure: they are decoder-only Transformers. However, they are not completely identical: in order to speed up training, increase intelligence or improve inference speed and cost, this base template is sometimes modified a bit.&lt;/p&gt;
&lt;p&gt;One popular variant is the so-called &lt;strong&gt;MoE (Mixture of Experts)&lt;/strong&gt; architecture: a neural network design that divides the model into multiple independent sub-networks called "experts". For each input, a routing algorithm (also called gating network) determines which experts to activate, so only a subset of the model's parameters is used during each inference pass. This leads to efficient scaling: models can grow significantly in parameter size without a proportional increase in computational resources per token or query. In short, it enables large models to perform as quickly as smaller ones without sacrificing accuracy.&lt;/p&gt;
&lt;p&gt;But what are these expert networks, and how are they built? One common misconception is that the "experts" of MoE are specialized in a well defined, recognizable type of task: that the model includes a "math expert", a "poetry expert", and so on‚Äã. The query would then be routed to the appropriate expert after the type of request is classified. &lt;/p&gt;
&lt;p&gt;However, this is not the case. Let's figure out how it works under the hood.&lt;/p&gt;
&lt;h2&gt;The MoE architecture&lt;/h2&gt;
&lt;p&gt;In order to understand MoE, you should first be familiar with the basic architecture of decoder-only Transformers. If the diagram below is not familiar to you, have a look at &lt;a href="https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse" target="_blank" rel="noopener noreferrer"&gt;this detailed description&lt;/a&gt; before diving in.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-12-11-what-are-moe-experts/decoder-only-transformer-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;The main change made by a MoE over the decoder-only transformer architecture is &lt;strong&gt;within the feed-forward component of the transformer block&lt;/strong&gt;. In the standard, non MoE architecture, the tokens pass one by one through a have a single feed-forward neural network. In a MoE instead, at this stage there are many feed-forward networks, each with their own weights: they are the "experts".&lt;/p&gt;
&lt;p&gt;This means that to create an MoE LLM we first need to convert the transformer‚Äôs feed-forward layers to these expert layers. Their internal structure is the same as the original, single network, but copied a few times, with the addition of a routing algorithm to select the expert to use for each input token to process.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-12-11-what-are-moe-experts/moe-decoding-step-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;The core of a routing algorithm is rather simple as well. First the token's embedding passes through a linear transformation (such as a fully connected layer) that outputs a vector as long as the number of experts we have in our system. Then, a softmax is applied and the top-k experts are selected. After the experts produce output, their results are then averaged (using their initial score as weight) and sent to the next decode layer.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-12-11-what-are-moe-experts/moe-router-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;Keep in mind that this is a simplification of the actual routing mechanism of real MoE models. If implemented as described here, through the training phase you would observe a &lt;strong&gt;routing collapse&lt;/strong&gt;: the routing network would learn to send all tokens to the same expert all the time, reducing your MoE model back to the equivalent of a regular decoder-only Transformer. To make the network learn to distribute the tokens in a more balanced fashion, you would need to add auxiliary loss functions that make the routing network learn to load balance the experts properly. For more details on this process (and much more on MoE in general) see &lt;a href="https://cameronrwolfe.substack.com/p/moe-llms" target="_blank" rel="noopener noreferrer"&gt;this detailed overview&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;So experts never specialize?&lt;/h2&gt;
&lt;p&gt;Yes and no. On the &lt;a href="https://arxiv.org/abs/2402.01739" target="_blank" rel="noopener noreferrer"&gt;OpenMoE paper&lt;/a&gt;, the authors investigated in detail whether experts do specialize in any recognizable domain, and they observed interesting results. In their case, experts do not tend to specialize in any particular domain; however, there is some level of expert specialization across natural languages and specific tasks.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-12-11-what-are-moe-experts/moe-not-specializing-domains-inv.jpg"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-12-11-what-are-moe-experts/moe-specializing-domains-inv.jpg"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;According to the authors, this specialization is due to the same tokens being sent to the same expert every time, regardless of the context in which it is used. Given that different languages use a very different set of tokens, it's natural to see this sort of specialization emerging, and the same can be said of specific tasks, where the jargon and the word frequency changes strongly. The paper defines this behavior as ‚ÄúContext-Independent Specialization‚Äù.&lt;/p&gt;
&lt;p&gt;It's important to stress again that whether this specialization occurs, and on which dimensions, is irrelevant to the effectiveness of this architecture. The core advantage of MoE is &lt;em&gt;not&lt;/em&gt; the presence of recognizable experts, but on the sparsity it introduces: with MoE you can scale up the parameters count without slowing down the inference speed of the resulting model, because not all weights will be used for all tokens.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The term "Mixture of Experts" can easily bring the wrong image into the mind of people unaccustomed with how neural networks, and Transformers in general, work internally. When discussing this type of models, I often find important to stress the difference between how the term "expert" is intended by a non technical audience and what it means in this context.&lt;/p&gt;
&lt;p&gt;If you want to learn more about MoEs and how they're implemented in practice, I recommend this  &lt;a href="https://cameronrwolfe.substack.com/p/moe-llms" target="_blank" rel="noopener noreferrer"&gt;this very detailed article&lt;/a&gt; by Cameron Wolfe, where he dissects the architecture in far more detail and adds plenty of examples and references to dig further.&lt;/p&gt;
&lt;p&gt;‚Äã&lt;/p&gt;</description></item><item><title>What's hybrid retrieval good for?</title><link>https://www.zansara.dev/posts/2025-11-04-hybrid-retrieval/</link><pubDate>Tue, 04 Nov 2025 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/posts/2025-11-04-hybrid-retrieval/</guid><description>&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;This is episode 4 of a series of shorter blog posts answering questions I received during the course of my work and reflect common misconceptions and doubts about various generative AI technologies. You can find the whole series here: &lt;a href="/series/practical-questions"&gt;Practical Questions&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;It has been a long time since TF-IDF or even BM25 were the state of the art for information retrieval. These days the baseline has moved to &lt;a href="/posts/2025-10-09-rerankers#bi-encoders-vs-cross-encoders"&gt;embedding similarity search&lt;/a&gt;, where each unit of information, be it a sentence, a paragraph or a page is first encoded in an embedding and then compared with the embedding of the user's query. &lt;/p&gt;
&lt;p&gt;From this baseline there are often two pieces of advice to help you increase the performance of your search system: one is to go the deep end with the embedding approach and consider a reranker, finetune your embedding model, and so on. The other, usually called hybrid retrieval or hybrid search, is to bring back good old keyword search algorithms and use them to complement your results. Often the best scenario is to use both of these enhancements, which nicely complement each other.&lt;/p&gt;
&lt;p&gt;But why would this arrangement help improve the results? Isn't embedding search strictly superior to keyword-based retrieval algorithms?&lt;/p&gt;
&lt;h2&gt;Semantic vs Lexical&lt;/h2&gt;
&lt;p&gt;When you embed a sentence, the resulting embedding encodes its &lt;em&gt;meaning&lt;/em&gt;, not its exact phrasing. That‚Äôs their strength! But it can often be a limitation as well. &lt;/p&gt;
&lt;p&gt;For example a semantic model can understand that "latest iPhone" is similar to "iPhone 17 Pro Max", which is great if the first sentence is a query and the second the search result. But a semantic model will also say that "iPhone 17 Pro Max" and "iPhone 11 Pro Max" are very similar, which is &lt;em&gt;not&lt;/em&gt; great if the first sentence is a query and the second a search result.&lt;/p&gt;
&lt;p&gt;In short, &lt;strong&gt;semantic&lt;/strong&gt; similarity is great if you are starting from a generic query and you want a set precise result all matching the generic description, or if you start from a general question and want to retrieve all very particular results that fall under the same general concept. For "latest iPhone", "iPhone 17 Pro Max", "iPhone 17 Pro" and ideally "iPhone Air" and  are all valid search results.&lt;/p&gt;
&lt;p&gt;On the other hand, &lt;strong&gt;lexical&lt;/strong&gt; similarity is what allows your system to retrieve extremely precise results in response to a very specific query. "latest iPhone" will return garbage results with a lexical algorithm such as BM25 (essentially any iPhone would match), but if the search string is "iPhone 17 Plus Max", BM25 will return the best results.&lt;/p&gt;
&lt;p&gt;To visualize it better, here's the expected results for each of the two queries in a dataset of iPhone names:&lt;/p&gt;
&lt;table style="width:100%; border: 2px solid black;"&gt;
&lt;tr&gt;
    &lt;th&gt;User Query&lt;/th&gt;
    &lt;th&gt;Semantic Search Results&lt;/th&gt;
    &lt;th&gt;Keyword Search Results&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;td&gt;"latest iPhone"&lt;/td&gt;
    &lt;td&gt;
        &lt;ol&gt;
            &lt;li&gt;iPhone 17 Pro
            &lt;li&gt;iPhone 17 Pro Max
            &lt;li&gt;iPhone Air
        &lt;ol&gt;
    &lt;/td&gt;
    &lt;td&gt;
        &lt;ol&gt;
            &lt;li&gt;iPhone 11 Pro Max
            &lt;li&gt;iPhone 4
            &lt;li&gt;iPhone SE
        &lt;ol&gt;
    &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;td&gt;"iPhone 17 Pro Max"&lt;/td&gt;
    &lt;td&gt;
        &lt;ol&gt;
            &lt;li&gt;iPhone 17 Pro
            &lt;li&gt;iPhone 17 Pro Max
            &lt;li&gt;iPhone Air
        &lt;ol&gt;
    &lt;/td&gt;
    &lt;td&gt;
        &lt;ol&gt;
            &lt;li&gt;iPhone 17 Pro Max
        &lt;ol&gt;
    &lt;/td&gt;
&lt;/tr&gt;

&lt;/table&gt;

&lt;p&gt;As you can see, the problem is that neither of the two approaches works best with both types of queries: each has its strong pros and cons and works best only on a subset of the questions your system may receive.&lt;/p&gt;
&lt;p&gt;So why not using them both?&lt;/p&gt;
&lt;h2&gt;Combining them&lt;/h2&gt;
&lt;p&gt;A hybrid search system is simply a system that does the same search twice: once with a keyword algorithm such as BM25, and once with vector search. But how to merge the two lists of results?&lt;/p&gt;
&lt;p&gt;The scores the documents come with are deeply incomparable. BM25 scores depends on terms frequency and keyword matching, and are not bound to any range. On the contrary, cosine similarity usually clusters between 0.5 and 0.9, which gets even narrower if the sequences are longer.&lt;/p&gt;
&lt;p&gt;That's where &lt;strong&gt;reciprocal rank fusion (RRF)&lt;/strong&gt; comes in. RRF is incredibly simple and boils down to this formula: &lt;code&gt;score(d) = sum( 1/(k + rank_method_i(d)) )&lt;/code&gt; . As you can see it works on the ranks, not scores, so it‚Äôs robust against scale differences and requires no normalization. Platforms like Elastic and Pinecone use it for production hybrid search due to its simplicity and reliability. Being so simple, the additional latency is negligible, which makes it suitable for real-time usecases.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-11-04-hybrid-retrieval/hybrid-search-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;Or, if you're less concerned about latency, you can consider adding a &lt;a href="/posts/2025-10-09-rerankers#bi-encoders-vs-cross-encoders"&gt;reranker&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Having two independent and complementary search techniques is the reason why adding a reranker to your hybrid pipeline is so effective. By using these two wildly different methods, it's not obvious whether even the rankings are comparable. Rerankers can have a more careful look at the retrieved documents and make sure the most relevant documents are to the top of the pile, allowing you to cut away the least relevant ones.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Hybrid search isn‚Äôt a patch for outdated systems, but a default strategy for any high-quality retrieval engine. Dense embeddings bring rich contextual understanding, while sparse retrieval ensures accuracy for unique identifiers, numeric codes, acronyms, or exact strings that embeddings gloss over. In a world where search systems must serve both humans and machine agents, hybrid search is the recall multiplier that guarantees we get both meaning and precision.&lt;/p&gt;</description></item><item><title>Making sense of KV Cache optimizations, Ep. 4: System-level</title><link>https://www.zansara.dev/posts/2025-10-29-kv-caching-optimizations-system-level/</link><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/posts/2025-10-29-kv-caching-optimizations-system-level/</guid><description>&lt;p&gt;In the previous posts we've seen &lt;a href="/posts/2025-10-23-kv-caching/"&gt;what the KV cache is&lt;/a&gt; and what types of &lt;a href="/posts/2025-10-26-kv-caching-optimizations-intro/"&gt;KV Cache management optimizations&lt;/a&gt; exist according to a &lt;a href="https://arxiv.org/abs/2412.19442" target="_blank" rel="noopener noreferrer"&gt;recent survey&lt;/a&gt;. In this post we are going to focus on &lt;strong&gt;system-level&lt;/strong&gt; KV cache optimizations.&lt;/p&gt;
&lt;h2&gt;What is a system-level optimization?&lt;/h2&gt;
&lt;p&gt;Real hardware is not only made of "memory" and "compute", but is made of several different hardware and OS level elements, each with its specific tradeoff between speed, throughput, latency, and so on. Optimizing the KV cache to leverages this differences is the core idea of the optimizazions we're going to see in this post.&lt;/p&gt;
&lt;p&gt;Here is an overview of the types of optimizations that exist today.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-10-29-kv-caching-optimizations-system-level/system-level-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://arxiv.org/pdf/2412.19442#figure.10" target="_blank" rel="noopener noreferrer"&gt;Source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As you can see in the diagram, they can be broadly grouped into three categories: memory management, scheduling strategies, and hardware-aware designs. These approaches are complementary and can be often used together, each addressing different aspects of performance, efficiency, and resource utilization tradeoffs.&lt;/p&gt;
&lt;p&gt;Let's see what's the idea behind each of these categories. We won't go into the details of the implementations of each: to learn more about a specific approach follow the links to the relevant sections of the survey, where you can find summaries and references.&lt;/p&gt;
&lt;h2&gt;Memory Management&lt;/h2&gt;
&lt;p&gt;Memory management techniques focus on using the different types of memory and storage available to the system in the most efficient way. There are two main approaches to this problem:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Architectural designs&lt;/strong&gt;, such as vLLM's &lt;strong&gt;PagedAttention&lt;/strong&gt; and vTensor. These strategies adapt operating system memory management ideas to to create memory allocation systems that optimize the use of physical memory as much as possible. For example, PagedAttention adapts OS-inspired paging concepts by partitioning KV caches into fixed-size blocks with non-contiguous storage, and vLLM implements a virtual memory-like system that manages these blocks through a sophisticated mapping mechanism.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prefix-aware designs&lt;/strong&gt; like &lt;strong&gt;ChunkAttention&lt;/strong&gt; and MemServe. These center around the design of datastructures optimized for maximising cache de-duplication and sharing of common prefixes. For example, ChunkAttention restructures KV cache management by breaking down traditional monolithic KV cache tensors into smaller, manageable chunks organized within a prefix tree structure, enabling efficient runtime detection and sharing of common prefixes across multiple requests.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general, there's a flurry of novel research focused on the way the KV cache is stored in memory. They bring classic OS memory management patterns and novel designs that leverage the properties of the KV cache at a memory layout level to increase the inference speed and memory consumption issues in a way that's transparent from the model's perspective. This makes these techniques widely applicable to many different LLMs and usually complementary to each other, which multiplies their effectiveness.&lt;/p&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href="https://arxiv.org/pdf/2412.19442#subsection.6.1" target="_blank" rel="noopener noreferrer"&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Scheduling&lt;/h2&gt;
&lt;p&gt;Scheduling techniques focus on maximizing cache hits and minimize cache lifetime by grouping and distributing requests appropriately. In this category we can find a few distinct approaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Prefix-aware&lt;/strong&gt; scheduling strategies, such as BatchLLM and RadixAttention. For example, unlike traditional LRU caches, BatchLLM identifies global prefixes and coordinates the scheduling of requests sharing common KV cache content. This ensures optimal KV cache reuse while minimizing cache lifetime: requests with identical prefixes are deliberately scheduled together to maximize KV cache sharing efficiency.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Preemptive&lt;/strong&gt; and &lt;strong&gt;fairness-oriented&lt;/strong&gt; scheduling, such as FastServe and FastSwitch. For example, FastServe implements a proactive cache management strategy coordinates cache movement between GPU and host memory, overlapping data transmission with computation to minimize latency impact. The scheduler also prioritizes jobs based on input length.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Layer-specific&lt;/strong&gt; and hierarchical scheduling approaches, such as LayerKV and CachedAttention. For example, LayerKV focuses on reducing time-to-first-token (TTFT) through a fine-grained, layer-specific KV cache block allocation and management strategy. It also includes an SLO-aware scheduler that optimizes cache allocation decisions based on service level objectives.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href="https://arxiv.org/pdf/2412.19442#subsection.6.2" target="_blank" rel="noopener noreferrer"&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Hardware-aware Design&lt;/h2&gt;
&lt;p&gt;These techiques focus on leveraging specific characteristics of the hardware in order to accelerate inference and increase efficiency. In this class of optimizazions we can find a few shared ideas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Single/Multi-GPU designs&lt;/strong&gt; focus on optimizing memory access patterns, GPU kernel designs for efficient attention computation, and parallel processing with load balancing. For example, shared prefix optimization approaches like HydraGen and DeFT focus on efficient GPU memory utilization through batched prefix computations and tree-structured attention patterns. Another example is distributed processing frameworks such as vLLM, that optimize multi-GPU scenarios through sophisticated memory management and synchronization mechanisms. Other techniques are phase-aware, like DistServe, which means that they separate prefill and decoding phases across GPU resources to optimize their distinct memory access patterns.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IO-based designs&lt;/strong&gt; optimize data movement across memory hierarchies through asynchronous I/O and intelligent prefetching mechanisms. &lt;br /&gt;
At the GPU level, approaches like FlashAttention optimize data movement between HBM and SRAM through tiling strategies and split attention computations. At the CPU-GPU boundary, systems like PartKVRec address tackles PCIe bandwidth bottlenecks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Heterogeneous designs&lt;/strong&gt; orchestrate computation and memory allocation across CPU-GPU tiers. Systems like NEO or FastDecode reditribute the workload by offloading to the CPU part of the attention computations, while others like FlexInfer introduce virtual memory abstractions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSD-based designs&lt;/strong&gt; have evolved from basic offloading approaches to more sophisticated designs. For example, FlexGen extends the memory hierarchy across GPU, CPU memory, and disk storage, optimizing high-throughput LLM inference on resource-constrained hardware. InstInfer instead leverages computational storage drives (CSDs) to perform in-storage attention computation, effectively bypassing PCIe bandwidth limitations. These techniques demonstrate how storage devices can be integrated into LLM inference systems either as memory hierarchy extensions or as computational resources.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href="https://arxiv.org/pdf/2412.19442#subsection.6.3" target="_blank" rel="noopener noreferrer"&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;System-level KV cache optimizations show that working across the stack can bring impressive speedups and manage physical resources more efficiently than it could ever be done at the LLM's abstraction level. Operating systems and hardware layouts offer plenty of space for optimizations of workloads that have somewhat predictable patterns such as attention computations and KV caching show, and these are just a few examples of what could be done in the near future.&lt;/p&gt;
&lt;p&gt;This is the end of our review. The original paper includes an additional section on long-context benchmarks which we're not going to cover, so head to &lt;a href="https://arxiv.org/pdf/2412.19442#section.7" target="_blank" rel="noopener noreferrer"&gt;the survey&lt;/a&gt; if you're interested in the topic.&lt;/p&gt;</description></item><item><title>Making sense of KV Cache optimizations, Ep. 3: Model-level</title><link>https://www.zansara.dev/posts/2025-10-28-kv-caching-optimizations-model-level/</link><pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/posts/2025-10-28-kv-caching-optimizations-model-level/</guid><description>&lt;p&gt;In the previous posts we've seen &lt;a href="/posts/2025-10-23-kv-caching/"&gt;what the KV cache is&lt;/a&gt; and what types of &lt;a href="/posts/2025-10-26-kv-caching-optimizations-intro/"&gt;KV Cache management optimizations&lt;/a&gt; exist according to a &lt;a href="https://arxiv.org/abs/2412.19442" target="_blank" rel="noopener noreferrer"&gt;recent survey&lt;/a&gt;. In this post we are going to focus on &lt;strong&gt;model-level&lt;/strong&gt; KV cache optimizations.&lt;/p&gt;
&lt;h2&gt;What is a model-level optimization?&lt;/h2&gt;
&lt;p&gt;We call a model-level optimization any modification of the architecture of the LLM that enables a more efficient reuse of the KV cache. In most cases, to apply these method to an LLM you need to either retrain or at least finetune the model, so it's not easy to apply and is usually baked in advance in of-the-shelf models.&lt;/p&gt;
&lt;p&gt;Here is an overview of the types of optimizations that exist today.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-10-28-kv-caching-optimizations-model-level/model-level-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://arxiv.org/pdf/2412.19442#figure.7" target="_blank" rel="noopener noreferrer"&gt;Source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let's see what's the idea behind each of these categories. We won't go into the details of the implementations of each: to learn more about a specific approach follow the links to the relevant sections of the survey, where you can find summaries and references.&lt;/p&gt;
&lt;h2&gt;Attention Grouping and Sharing&lt;/h2&gt;
&lt;p&gt;One common technique to reduce the size of the KV cache is to group and/or share attention on different levels. There's techniques being developed for different grades of attention grouping:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Intra-layer grouping&lt;/strong&gt;: focuses on grouping query, key, and value heads within individual layers&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cross-layer sharing&lt;/strong&gt;: shares key, value, or attention components across layers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At the &lt;strong&gt;intra-layer&lt;/strong&gt; level, the standard architecture of Transformers calls for full &lt;strong&gt;multi-headed attention&lt;/strong&gt; (MHA). As an alternative, it was proposed to have all attention heads share a single key and value, reducing dramatically the amount of compute and space needed. This technique, called &lt;strong&gt;multi-query attention&lt;/strong&gt; (MQA) is a radical strategy that would cause not just quality degradation, but also training instability. As a compromise, &lt;strong&gt;grouped-query attention&lt;/strong&gt; (GQA) was proposed by dividing the query heads into multiple groups, while each group shares its own keys and values. In addition, an uptraining process has been proposed to efficiently convert existing MHA models to GQA configurations by mean-pooling the key and value heads associated with each group. Empirical evaluations demonstrated that GQA models achieve performance close to the original MHA models.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-10-28-kv-caching-optimizations-model-level/attention-grouping-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A simplified illustration of different QKV grouping techniques: multi-headed attention (MHA), multi-query attention (MQA) and grouped-query attention (GQA).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Across layers&lt;/strong&gt;, cross-layer attention (CLA) was proposed to extends the idea of GQA. Its core idea is to share the key and value heads between adjacent layers. This achieves an additional 2√ó KV cache size reduction compared to MQA. Several other approaches exist to address cross-layer attention sharing, so check out &lt;a href="https://arxiv.org/pdf/2412.19442#subsection.5.1" target="_blank" rel="noopener noreferrer"&gt;the survey&lt;/a&gt; if you want to learn more.&lt;/p&gt;
&lt;p&gt;In general, the main issue in this line of research regards the model modifications that needs to be applied. Current approaches often fail to generalize well to architecture they were not initially designed on, while more static and general grouping/sharing strategies fail to capture important variations in the various heads and layers, leading to a loss of output quality. In addition, the need to retrain the LLM after the changes limits strongly the portability of these methods.&lt;/p&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href="https://arxiv.org/pdf/2412.19442#subsection.5.1" target="_blank" rel="noopener noreferrer"&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Architecture Alteration&lt;/h2&gt;
&lt;p&gt;Another approach is to make more high-level architectural changes to reduce the required cache size. There seems to be two main directions in this area:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Enhanced Attention&lt;/strong&gt;: methods that refine the attention mechanism for KV cache efficiency. An example is DeepSeek-V2, which introduced Multi-Head Latent Attention (MLA). This technique adopts a low-rank KV joint compression mechanism and replaces the full KV cache with compressed latent vectors. The model adopts trainable projection and expansion matrices to do the compression. This compression mechanism is what enables the model to handle sequences of up to 128K tokens. You can learn more about MLA in &lt;a href="https://magazine.sebastianraschka.com/i/168650848/multi-head-latent-attention-mla" target="_blank" rel="noopener noreferrer"&gt;this article&lt;/a&gt; by Sebastian Raschka.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Augmented Architecture&lt;/strong&gt;: methods that introduce structural changes for better KV management, for example novel decoder structures (such as YOCO, that included a self-decoder and a cross-decoder step).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many of these works build upon the broader landscape of efficient attention mechanisms (e.g., Linear Transformer, Performer, LinFormer, etc.) which already have &lt;a href="https://arxiv.org/abs/2404.14294" target="_blank" rel="noopener noreferrer"&gt;their own survey&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Although these approaches demonstrate significant progress in enabling longer context windows and faster inference, there are still big challenged ans unknowns. Some techniques in this category, for example, perform very well for some tasks but fail to generalize (for example they work well with RAG but not with non-RAG scenarios).&lt;/p&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href="https://arxiv.org/pdf/2412.19442#subsection.5.2" target="_blank" rel="noopener noreferrer"&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Non-Transformer Architecture&lt;/h2&gt;
&lt;p&gt;In this category we group all radical approaches that ditch the Transformers architecture partially or entirely and embrace alternative models, for example RNNs, which don't have quadratic computation bottlenecks at all and sidestep the problem entirely.&lt;/p&gt;
&lt;p&gt;In the case of completely independent architectures, notable examples are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/2312.00752" target="_blank" rel="noopener noreferrer"&gt;Mamba&lt;/a&gt;, based on state space sequence models (SSMs). Mamba improves SSMs by making parameters input-dependent, allowing information to be selectively propagated or forgotten along the sequence based on the current token. Mamba omits attention entirely.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/2305.13048" target="_blank" rel="noopener noreferrer"&gt;RWKV&lt;/a&gt; (Receptance Weighted Key Value) integrates a linear attention mechanism, enabling parallelizable training like transformers while retaining the efficient inference characteristics of RNNs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Efficient non-Transformers also have their own surveys, so check out the paper to learn more.&lt;/p&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href="https://arxiv.org/pdf/2412.19442#subsection.5.3" target="_blank" rel="noopener noreferrer"&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Model-level optimizations go from very light touches to the original Transformer model to architecture that have nothing to do with it, therefore not having any KV cache to deal with in the first place. In nearly all cases the principal barrier to adoption is the same: applying these techniques requires a &lt;strong&gt;full retraining of the model&lt;/strong&gt;, which can be impractical at best and prohibitively expensive at worst, even for users that have the right data and computing power. Model-level optimizations are mostly useful for LLM developers to get an intuition of the memory efficiency that can be expected from a model that includes one or more of these features out of the box.&lt;/p&gt;
&lt;p&gt;In the next post we're going to address &lt;a href="/posts/2025-10-29-kv-caching-optimizations-system-level"&gt;system-level&lt;/a&gt; optimizations. Stay tuned!&lt;/p&gt;</description></item><item><title>Making sense of KV Cache optimizations, Ep. 2: Token-level</title><link>https://www.zansara.dev/posts/2025-10-27-kv-caching-optimizations-token-level/</link><pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/posts/2025-10-27-kv-caching-optimizations-token-level/</guid><description>&lt;p&gt;In the previous post we've seen &lt;a href="/posts/2025-10-23-kv-caching/"&gt;what the KV cache is&lt;/a&gt; and what types of &lt;a href="/posts/2025-10-26-kv-caching-optimizations-intro/"&gt;KV cache management optimizations&lt;/a&gt; exist according to a &lt;a href="https://arxiv.org/abs/2412.19442" target="_blank" rel="noopener noreferrer"&gt;recent survey&lt;/a&gt;. In this post we are going to focus on &lt;strong&gt;token-level&lt;/strong&gt; KV cache optimizations.&lt;/p&gt;
&lt;h2&gt;What is a token-level optimization?&lt;/h2&gt;
&lt;p&gt;The survey defined token-level optimizations every technique that focuses exclusively on improving the KV cache management based on the &lt;strong&gt;characteristics and patterns of the KV pairs&lt;/strong&gt;, without considering enhancements from model architecture improvements or system parallelization techniques.&lt;/p&gt;
&lt;p&gt;Here is an overview of the types of optimizations that exist today.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-10-27-kv-caching-optimizations-token-level/token-level-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://arxiv.org/pdf/2412.19442#figure.3" target="_blank" rel="noopener noreferrer"&gt;Source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let's see what's the idea behind each of these categories. We won't go into the details of the implementations of each: to learn more about a specific approach follow the links to the relevant sections of the survey, where you can find summaries and references.&lt;/p&gt;
&lt;h3&gt;KV Cache Selection&lt;/h3&gt;
&lt;p&gt;One key characteristic of the attention matrix is &lt;strong&gt;sparsity&lt;/strong&gt;: most of its values are very close to zero, and just a few cells have meaningful values. Instead of retrieving a full matrix of attention values every time (and retrieve a ton of close-to-zero, nearly useless values), KV Cache selection techniques identify the most relevant token pair and cache those only, reducing memory utilization and inference latency.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-10-27-kv-caching-optimizations-token-level/sparse-attention-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A simplified view of a cache selection strategy. In this case, the KV cache tends to have its highest values clustered near the diagonal (because most tokens refer to other tokens that are relatively close), so most of the lower-left side of the matrix can be safely assumed to be zero. That reduces drastically the number of values to store.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The researches identified two main cache selection strategies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Static KV cache selection&lt;/strong&gt;. In this family of optimizations, the KV cache compression only happens during the first decoding pass (when most of the prompt is loaded in the LLM state, also called &lt;strong&gt;prefill phase&lt;/strong&gt;) and remain fixed during all subsequent decoding steps, with no more compressions as the inference proceeds.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic KV cache selection&lt;/strong&gt;, which continuously updates and compresses the KV cache during all inference passes, enabling adaptive cache management. In dynamic KV cache selection, KV cache tokens that are not selected may be either permanently evicted or offloaded to hierarchical caching devices such as CPU memory. While more efficient in terms of memory usage, real-time KV cache selection during decoding may incur substantial computational overhead, which is usually the focus of any new technique developed in this space.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The tradeoff between static and dynamic KV cache selection is again one of &lt;strong&gt;latency versus efficiency&lt;/strong&gt;, or time vs space usage. Static KV cache selection is faster and slightly less efficient; dynamic KV cache compression is more efficient in terms of memory usage but has a sensible impact on inference speed and may cause issues due to excessive compression, throwing away or putting in cold caches token pairs that are actually relevant. A clear consensus about where the sweet spot lays hasn't been found yet, and it's mostly still open to investigation.&lt;/p&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href="https://arxiv.org/pdf/2412.19442#subsection.4.1" target="_blank" rel="noopener noreferrer"&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;KV Cache Budget Allocation&lt;/h3&gt;
&lt;p&gt;LLMs are hierarchical, with several layers within layers of computations. Each of these layers is identical in structure, but during training the weights that they learn make some of these layers more important than others and more impactful on the output's quality.&lt;/p&gt;
&lt;p&gt;This means that not all of these steps should be compressed equally. If we could identify which layers are more impactful we could reduce the compression of the KV cache for these layers and increase it for the others. In this way the effects of compression on the output quality would be minimized.&lt;/p&gt;
&lt;p&gt;Budget allocation strategies tend either of these granularity levels:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Layer-wise budget allocation&lt;/strong&gt;, which assigns different compression ratios across the model's decoding layers&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Head-wise budget allocation&lt;/strong&gt;, which enables precise memory distribution across individual attention heads within each layer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Despite recent advances and growing attention in this subset of techniques, there are still big question marks about how to distribute this computing budget in an effective way. For instance, a notable discrepancy exists between pyramid-shaped allocation strategies that advocate larger budgets for lower layers, and retrieval head-based studies, which demonstrate that lower layers rarely exhibit retrieval head characteristics and thus require minimal cache resources. On top of this, there is a lack of comprehensive experimental comparisons, such as the compatibility and performance benefits of head-wise budget allocation strategies with state-of-the-art frameworks like vLLM.&lt;/p&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href="https://arxiv.org/pdf/2412.19442#subsection.4.2" target="_blank" rel="noopener noreferrer"&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;KV Cache Merging&lt;/h3&gt;
&lt;p&gt;The idea behind KV cache merging is to compress or consolidate separate KV caches into a single one without significantly degrading model accuracy. This stems from the observation that the various layers and attention heads often shows redundant patterns that could be merged into one single representation to improve compression.&lt;/p&gt;
&lt;p&gt;Just like with the budget allocation techniques, KV cache merging strategies can be categorized into two primary approaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Intra-layer merging&lt;/strong&gt;, which focuses on consolidating KV caches within individual layers to reduce memory usage per layer&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cross-layer merging&lt;/strong&gt;, which targets redundancy across layers to eliminate unnecessary duplication. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general, KV cache merging can be very effective at optimizing memory utilization in LLMs by consolidating KV caches while maintaining high model accuracy, and it's an active research direction that could provide more results in the near future by addressing narrower niches such as fine-tuning and adaptive merging strategies.&lt;/p&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href="https://arxiv.org/pdf/2412.19442#subsection.4.3" target="_blank" rel="noopener noreferrer"&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;KV Cache Quantization&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-10-27-kv-caching-optimizations-token-level/quantization-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A simplified example of cache quantization. Reducing the precision of the values from float to int8 can drastically reduce the memory needs of the cache and accelerate inference.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Quantization techniques aim to convert full-precision values into integers, reducing computational and storage requirements. Quantization has also been used on other aspects of the LLM inference and training processes, such as with model parameters and data features quantization. KV cache quantization works in a similar way: by reducing the precision of numerical representations (e.g., from FP32 to INT8 or INT4) we can drastically compress the size of the KV cache and achieve up to 4x or more memory savings with respect to the full-precision floating point representation.&lt;/p&gt;
&lt;p&gt;One of the main challenges of KV cache quantization is the presence of outliers, especially when quantizing to a very low-bit representation. These extreme values, when reduced to a smaller magnitude, can lead to a substantial performance degradation. &lt;/p&gt;
&lt;p&gt;Depending on how they address this issue, quantization techniques can be grouped into three types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fixed-precision quantization&lt;/strong&gt;, where all Keys and Values are quantized to the same bit-width.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mixed-precision quantization&lt;/strong&gt;, which assigns higher precision to critical parts of the cache while using lower precision for less important components.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Outlier redistribution&lt;/strong&gt;, which redistributes or smooths the outliers in Keys and Values to improve quantization quality. Some approaches to outliers redistribution include redistributing the outliers into newly appended virtual tokens or applying equivalent transformation functions to smooth the keys and values for improved quantization accuracy. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href="https://arxiv.org/pdf/2412.19442#subsection.4.4" target="_blank" rel="noopener noreferrer"&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;KV Cache Low-rank Decomposition&lt;/h3&gt;
&lt;p&gt;Existing studies have demonstrated that the majority of information within KV caches can be captured by a small subset of their singular elements or sub-matrices with a smaller dimension, called &lt;strong&gt;low-rank components&lt;/strong&gt;. Decomposing the matrix into low-rank components can effectively reduce memory requirements while preserving output quality by "picking out" the components of the KV matrix that matter the most and throwing out the rest.&lt;/p&gt;
&lt;p&gt;Currently there are three main ways to perform low-rank decomposition of the cached KV matrix:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Singular Value Decomposition (SVD)&lt;/strong&gt;: retains the most critical singular values.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tensor Decomposition&lt;/strong&gt;: factorizes KV matrices into smaller matrices/tensors.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learned Low-rank Approximation&lt;/strong&gt;: adaptive mechanisms to optimize compression based on learned low-rank representations. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Current methods primarily rely on fixed low-rank approximations applied uniformly across all layers or tokens, but future advancements could focus on dynamic rank adjustment, where the rank is tailored based on token importance, sequence length, or layer-specific properties.&lt;/p&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href="https://arxiv.org/pdf/2412.19442#subsection.4.5" target="_blank" rel="noopener noreferrer"&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This was just a brief overview of the various techniques that have been tested to compress the KV cache, but the exploration of the space between highest accuracy, fastest inference and strongest compression is far from complete. Most of these techniques optimize for just one or two of these properties, with no clear winner that beats them all. Expect a lot more experimentation in this field in the months and years to come.&lt;/p&gt;
&lt;p&gt;On the other hand, these are only compression techniques that apply at the token-level, without any support from the model architecture. For model-level approaches to the problem, check out the next post, where we continue exploring the survey to see how the basic architecture of the Transformer's decoding layer can be optimized to reduce the amount of values to cache in the first place.&lt;/p&gt;
&lt;p&gt;In the next post we're going to address &lt;a href="/posts/2025-10-28-kv-caching-optimizations-model-level"&gt;model-level&lt;/a&gt; optimizations. Stay tuned!&lt;/p&gt;</description></item><item><title>Making sense of KV Cache optimizations, Ep. 1: An overview</title><link>https://www.zansara.dev/posts/2025-10-26-kv-caching-optimizations-intro/</link><pubDate>Sun, 26 Oct 2025 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/posts/2025-10-26-kv-caching-optimizations-intro/</guid><description>&lt;p&gt;The &lt;a href="/posts/2025-10-23-kv-caching/"&gt;KV cache&lt;/a&gt; is an essential mechanism to avoid the quadratic time complexity of LLM inference and make modern LLMs usable despite huge parameters count and context lengths. However, simply caching everything indiscriminately is not a successful strategy. By swapping time for space complexity, now our problem is &lt;strong&gt;GPU memory&lt;/strong&gt;. Adding more memory can only bring you so far: at some point, you're going to need much more efficient ways to decide what to cache, when and how. But classic cache management techniques were not designed for LLMs, and they often fall short.&lt;/p&gt;
&lt;p&gt;With time, a veritable zoo of optimization strategies arose to get around this problem, and making sense of which optimizations can be applied to which model can be a challenge in itself. Fortunately a  &lt;a href="https://arxiv.org/abs/2412.19442" target="_blank" rel="noopener noreferrer"&gt;very comprehensive survey&lt;/a&gt; on KV caching recently collected all techniques that make up the state of the art in this field, giving practitioners a handy starting point to understand this field. The amount of techniques reviewed is staggering, so we're going to need more than one post to go through the most interesting approaches and compare them.&lt;/p&gt;
&lt;p&gt;For now, let's see how we can start to make sense of things.&lt;/p&gt;
&lt;h2&gt;The challenges&lt;/h2&gt;
&lt;p&gt;Most of the techniques we're going to see address one or more of these issues.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cache Eviction&lt;/strong&gt;: Determining which items to evict when the cache reaches its capacity. Popular policies like Least Recently Used (LRU) or Least Frequently Used (LFU) do not always align with LLM usage patterns, leading to suboptimal performance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory Management&lt;/strong&gt;: The memory required for the KV cache grows linearly with both the input length and the number of layers, which can quickly exceed the hardware memory limits. It's possible to overcome such limits by distributing the storage of this cache across different types of storage hardware (e.g., GPU, CPU or external memory), but this brings its own set of challenges.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Latency&lt;/strong&gt;: Accessing and updating the cache at each decoding step can introduce latency.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compression&lt;/strong&gt;: Compressing the KV cache can reduce memory usage but may degrade model performance if key information is lost.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Workloads&lt;/strong&gt;: Handling dynamic and unpredictable workloads, where access patterns and data requirements frequently change, requires adaptive caching strategies that can respond in real time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distributed Coordination&lt;/strong&gt;: In distributed KV caches, maintaining coordination across multiple nodes to ensure consistency, fault tolerance, and efficient resource usage adds significant complexity.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;A taxonomy&lt;/h2&gt;
&lt;p&gt;In order to make sense of the vast amount of known techniques, the authors categorized them into a comprehensive taxonomy.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-10-26-kv-caching-optimizations-intro/taxonomy-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://arxiv.org/pdf/2412.19442#figure.2" target="_blank" rel="noopener noreferrer"&gt;Source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It starts with three major categories: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Token-Level Optimization&lt;/strong&gt;: improving KV cache management efficiency by focusing on the fine-grained selection, organization, and compression at the token level. These techniques can be applied to any model, as they require no architectural changes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model-level Optimization&lt;/strong&gt;: designing an efficient model structure to optimize KV cache management. These optimizations are strictly model-dependent, because they're backed into the model's architecture.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;System-level Optimization&lt;/strong&gt;: optimizing the KV Cache management through techniques closer to the OS and/or the hardware. These techniques may require specialized hardware to implement, so they're not at everyone's reach.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Token-Level Optimizations&lt;/h2&gt;
&lt;p&gt;Token-level optimizations are the most readily accessible to most developers, as they require no dedicated support from the LLM and no specialized hardware. Therefore, these are usually the most interesting. In this category we find:&lt;br /&gt;
- &lt;strong&gt;KV cache selection&lt;/strong&gt;: focuses on prioritizing and storing only the most relevant tokens.&lt;br /&gt;
- &lt;strong&gt;KV cache budget allocation&lt;/strong&gt;: dynamically distributes memory resources across tokens to ensure efficient cache utilization under limited memory. &lt;br /&gt;
- &lt;strong&gt;KV cache merging&lt;/strong&gt;: reduces redundancy by combining similar or overlapping KV pairs.&lt;br /&gt;
- &lt;strong&gt;KV cache quantization&lt;/strong&gt;: minimizes the memory footprint by reducing the precision of cached KV pairs. &lt;br /&gt;
- &lt;strong&gt;KV cache low-rank decomposition&lt;/strong&gt;: uses low-rank decomposition techniques to reduce cache size.   &lt;/p&gt;
&lt;h2&gt;Model-Level Optimizations&lt;/h2&gt;
&lt;p&gt;Model-level optimizations, as the name says, are baked into the model's architecture and therefore are either not applicable or always present in the models you're running. These optimizations are usually interesting for people that design their own model architecture and train them, rather than developers that work with off-the-shelf models. In this category we find:&lt;br /&gt;
- &lt;strong&gt;Attention grouping and sharing methods&lt;/strong&gt;: examine the redundant functionality of keys and values and group and share KV cache within or across transformer layers. &lt;br /&gt;
- &lt;strong&gt;Architecture alterations&lt;/strong&gt;: emerge to design new attention mechanisms or construct extrinsic modules for KV optimization. &lt;br /&gt;
- &lt;strong&gt;Non-transformer architectures&lt;/strong&gt;: architectures that adopt other memory-efficient designs like recurrent neural networks to optimize the KV cache in traditional transformers.    &lt;/p&gt;
&lt;h2&gt;System-level Optimizations&lt;/h2&gt;
&lt;p&gt;These optimizations work across the stack to provide the best possible support to the LLM's inference, and they're sometimes baked into the inference engine, such as vLLM's PagedAttention. They occasionally require dedicated hardware and OS optimizations, so they're not always readily available for everyday experimentation. They include:&lt;br /&gt;
- &lt;strong&gt;Memory management&lt;/strong&gt;: focuses on architectural innovations like virtual memory adaptation, intelligent prefix sharing, and layer-aware resource allocation.&lt;br /&gt;
- &lt;strong&gt;Scheduling&lt;/strong&gt;: addresses diverse optimization goals through prefix-aware methods for maximizing cache reuse, preemptive techniques for fair context switching, and layer-specific mechanisms for fine-grained cache control.&lt;br /&gt;
- &lt;strong&gt;Hardware acceleration&lt;/strong&gt;: including single/multi-GPU, I/O-based solutions, heterogeneous computing and SSD-based solutions.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;KV cache optimization is still an open research area, with new techniques and improvements being published regularly. A good overview of what types of optimizations exist can help you make sense of the zoo of acronyms and claims being made about them, and give you the foundations you need to understand if a particular technique is relevant for your situation. Stay tuned for the &lt;a href="/posts/2025-10-27-kv-caching-optimizations-token-level"&gt;next posts&lt;/a&gt;, where we will dive deeper into each of these categories.&lt;/p&gt;</description></item><item><title>How does prompt caching work?</title><link>https://www.zansara.dev/posts/2025-10-23-kv-caching/</link><pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/posts/2025-10-23-kv-caching/</guid><description>&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;This is episode 3 of a series of shorter blog posts answering questions I received during the course of my work and reflect common misconceptions and doubts about various generative AI technologies. You can find the whole series here: &lt;a href="/series/practical-questions"&gt;Practical Questions&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;In the previous post we saw what is prompt caching, what parts of the prompts is useful to cache, and explained at a high level why it's so effective. In this post I want to go one step further and explain &lt;em&gt;how&lt;/em&gt; in practice inference engines cache prompt prefixes. How can you take a complex system like an LLM, cache some of its computations mid-prompt, and reload them?&lt;/p&gt;
&lt;p&gt;Let's find out.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Disclaimer: to avoid overly complex and specific diagrams, the size of the vector and matrices shown is not accurate neither in size nor in shape. Check the links at the bottom of the post for more detailed resources with more accurate diagrams.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;LLMs are autoregressive&lt;/h2&gt;
&lt;p&gt;Large Language Models are built on the Transformer architecture: a neural network design that excels at processing sequence data. Explaining the whole structure of a Transformer goes beyond the scope of this small post: if you're interested in the details, head to this &lt;a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer"&gt;amazing writeup&lt;/a&gt; by Jay Alammar about Transformers, or &lt;a href="https://jalammar.github.io/illustrated-gpt2/" target="_blank" rel="noopener noreferrer"&gt;this one about GPT-2&lt;/a&gt; if you're familiar with Transformers but you want to learn more about the decoder-only architecture (which includes all current LLMs).&lt;/p&gt;
&lt;p&gt;The point that interests us is that according to the original implementation, during inference the LLM generate text one token at a time in an &lt;em&gt;autoregressive&lt;/em&gt; fashion, meaning each new token is predicted based on &lt;strong&gt;all&lt;/strong&gt; previously generated tokens. After producing (or "decoding") a token, that token is appended to the input sequence and the model computes everything all over again to generate the next one. This loop continues until a stopping condition is reached (such as an end-of-sequence token or a length limit).&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-10-23-kv-caching/auto-regression-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In an autoregressive system, the output is generated token by token by appending the previous pass' output to its input and recomputing everything again. Starting from the token "This", the LLM produces "is" as output. Then the output is concatenated to the input in the string "This is", which is fed again to the LLM to produce "a", and so on until an [END] token is generated. That halts the loop.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This iterative process is very computationally expensive (quadratic time complexity in the number of tokens, so O(n^2) where n is the number of tokens), and the impact is felt especially for long sequences, because each step must account for an ever-growing history of generated tokens.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-10-23-kv-caching/auto-regression-2-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Simplified view of the increasing computation load. At each pass, the increasing length of the input sentence translated into larger matrices to be handled during inference, where each row corresponds to one input token. This means more computations and, in turn, slower inference.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;However, there seems to be an evident chance for optimization here. If we could store the internal state of the LLM after each token's generation and reuse it at the next step, we could save a lot of repeated computations.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-10-23-kv-caching/auto-regression-cached-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If we could somehow reuse part of the computations we did during earlier passes and only process new information as it arrives, not only the computation speed will increase dramatically, but it will stay constant during the process instead of slowing down as more tokens are generated.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This is not only true during a single request (because we won't be recomputing the whole state from the start of the message for every new token we're generating), but also across requests in the same chat (by storing the state at the end of the last assistant token) and across different chats as well (by storing the state of shared prefixes such as system prompts).&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-10-23-kv-caching/prefix-caching-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example of prefix caching in different scenarios (gray text is caches, black is processed). By caching the system prompt, its cache can be reused with every new chat. By also caching by longest prefix, the prompts may occasionally match across chats, although it depends heavily on your applications. In any case, caching the chat as it progresses keeps the number of new tokens to process during the chat to one, making inference much faster.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;But how can it be done? What exactly do we need to cache? To understand this we need to go one step deeper.&lt;/p&gt;
&lt;h2&gt;The inference process&lt;/h2&gt;
&lt;p&gt;At a high level, the inference process of a modern decoder-only Transformer such as a GPT works as follows.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tokenization&lt;/strong&gt;: The chat history is broken down into tokens by a tokenizer. This is a fast, deterministic process that transforms a single string into a list of sub-word fragments (the tokens) plus a bunch of signalling tokens (to delimit messages, to signal the end of the message, to distinguish different types of input or output tokens such as thinking tokens, function calls, system prompts, etc)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embedding&lt;/strong&gt;: the tokenized text passes through an embedding step, where each token is translated into an embedding (a 1-dimensional vector) using a lookup table. At this point, our input text has become a matrix of values with as many rows as tokens, and a fixed number of columns that depends on the LLM.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decoding&lt;/strong&gt;: this matrix is passed through a series of 12 identical decoding steps. Each of these blocks outputs a matrix of the same shape and size of the original one, but with updated contents. These steps are "reading" the prompt and accumulating information to select the next best token to generate.that is passed as input to the next step&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: After the last decoding step, a final linear output layer projects the matrix into an output vector. Its values are multiplied by the lookup table we used during the embedding step: this way we obtain a list of values that represents the probability of each token to be the "correct" next token.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sampling&lt;/strong&gt;: From this list, one of the top-k best tokens is selected as the next token, gets added to the chat history, and the loop restarts.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;End token&lt;/strong&gt;: the decoding stops when the LLM picks an END token or some other condition is met (for example, max output length).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-10-23-kv-caching/llm-inference-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Simplified representation of the inference steps needed for an LLM to generate each output token. The most complex by far is the decoding step, which we are going to analyze in more detail.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As you can see from this breakdown, the LLM computes its internal representation of the chat history through its decoding steps, and recomputes such representation for all tokens every time we want to generate a new one. So let's zoom in even more and check what's going on inside these decoding steps.&lt;/p&gt;
&lt;h2&gt;The decoding step&lt;/h2&gt;
&lt;p&gt;LLMs may have a variable number of decoding steps (although it's often 12), but they are all identical, except for the weights they contain. This means that we can look into one and then keep in mind that the same identical process is repeated several times.&lt;/p&gt;
&lt;p&gt;Each decoding step contains two parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a multi-headed, masked self-attention layer&lt;/li&gt;
&lt;li&gt;a feed-forward layer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first layer, the multi headed masked self attention, sound quite complicated. To make things easier, let's break it down into smaller concepts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Attention&lt;/strong&gt; is the foundation of the Transformers' incredible text understanding skills and can be roughly summarized as a technique that shows the model which tokens are the most relevant to the token we're processing right now.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Self attention&lt;/strong&gt; means that the tokens we're looking at belong to the same sentence we're processing (which is not the case, for example, during translation tasks where we have a source sentence and a translation).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Masked self attention&lt;/strong&gt; means that we're only looking at tokens that precede the one we're processing (which is not the case, for example, in encoder models such as BERT that encode the whole sentence at once).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Multi-headed&lt;/strong&gt; attention means that the same operation is performed several times with slightly different parameters. Each set of parameters is called an &lt;strong&gt;attention head&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To understand what attention does, let's take the sentence "I like apples because they're sweet". When processing the token "they", the masked self-attention layer will give a high score to "apples", because that's what "they" refers to. Keep in mind that "sweet" will not be considered while processing "they", because masked self-attention only includes tokens that precede the token in question.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-10-23-kv-caching/masked-self-attention-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A simplified visualization of a masked self-attention head. For each token, the attention calculations will assign a score to each preceding token. The score will be higher for all preceding tokens that have something to do with the current one, highlighting semantic relationships.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;The Q/K/V Matrices&lt;/h2&gt;
&lt;p&gt;Let's now look at how is this score calculated. Self-attention is implemented as a series of matrix multiplications that involves three matrices.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Q (Query matrix)&lt;/strong&gt;: The query is a representation of the tokens we are "paying attention to" (for example, "they". In practice all tokens will be computed at the same time, so we will be dealing with a Q matrix).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;K (Key matrix)&lt;/strong&gt;: Key vectors are like labels for all the other preceding tokens in the input. They‚Äôre what we match against in our search for relevant tokens (for example "I", "like", "apples", etc ). Each token will only see the keys of tokens that precede it, so the query of "they" will not be multiplied with the key for "sweet".&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;V (Value matrix)&lt;/strong&gt;: Value vectors are actual token representations. Once we‚Äôve scored how relevant each token is, these are the values we add up to represent the token we're paying attention to. In our example, this means that the vector for "they" will be computed as a weighted average of all the previous tokens ("I", "like", "apples", "because"), but "apples" will be weighted much higher than any other, so the end result for the token "they" will be very close to the value vector for "apples".&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These Q/K/V matrices are computed by multiplying the input of the decoding layer by three matrices (Wq, Wk and Wv) whose values are computed during training and constitute many of the LLM's parameters. These three matrices are addressed together as an attention head, as we mentioned earlier. Modern LLMs usually include several attention heads for each step, so you'll have several different matrices in each decoding step (and that's why they're said to use multi-headed attention).&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-10-23-kv-caching/Q-K-V-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Simplified view of the Q/K/V matrices in a single self-attention head. The matrices go through a few more steps (softmax, regularization etc) which are not depicted here&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This process of computing the output vector for each token is called &lt;em&gt;scaled dot-product attention&lt;/em&gt; and, as we mentioned earlier, happens in every attention head of every decoding step. In summary, &lt;strong&gt;keys (K)&lt;/strong&gt; and &lt;strong&gt;values (V)&lt;/strong&gt; are the transformed representations of each preceding token that are used to compute attention, and they enable each token to gather information from the rest of the sequence by matching queries to keys and aggregating values.&lt;/p&gt;
&lt;p&gt;Let's pay close attention to these computations. We know that LLMs generate output one token at a time. This means that the LLM will recompute the K-V values for the tokens of the prompt over and over again for each new output token it generates. If you have already generated, say, 100 tokens of output, producing the 101st token requires recomputing a forward pass over all 100 tokens. A naive implementation would repeatedly recalculate a lot of the same intermediate results for the older tokens at every step of generation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-10-23-kv-caching/KV-caching-no-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Detail of the Q/K multiplication. As you can see, the content of the QK matrix is essentially the same at all steps, except for the last row. This means that as soon as we accumulate a few input tokens, most of the QK matrix will be nearly identical every time. Something very similar happens for the final QKV matrix.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;By the third generation step (three tokens in context), the model computes six attention scores (a 3√ó3 / 2 matrix); many of these correspond to interactions that were already computed in earlier steps. For example, the attention of token "I" with itself was computed in the first step, yet the naive approach computes it again when processing the sequence "I like" and "I like apples" and so on. In fact, by the time the sentence is complete, the majority of the query-key pairs being calculated are repeats of prior computations. This redundancy makes inference much slower as the sequence length grows: the model wastes time recalculating attention contributions for tokens that haven‚Äôt changed.&lt;/p&gt;
&lt;p&gt;Clearly we want to avoid recomputing things like the key and value vectors for past tokens at every step. That‚Äôs exactly what &lt;strong&gt;KV caching&lt;/strong&gt; achieves.&lt;/p&gt;
&lt;h2&gt;The KV Cache&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;KV caching&lt;/strong&gt; is an optimization that saves the key and value tensors from previous tokens so that the model doesn‚Äôt need to recompute them for each new token. The idea is straightforward: as the model generates tokens one by one, we store the keys and values produced at each layer for each token in a cache (which is just a reserved chunk of memory, typically in GPU RAM for speed). When the model is about to generate the next token, instead of recomputing all keys and values from scratch for the entire sequence, it retrieves the already-computed keys and values for the past tokens from this cache, and only computes the new token‚Äôs keys and values. &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-10-23-kv-caching/KV-caching-yes-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Computing the QK matrix by reusing the results of earlier passes makes the number of calculations needed at each step nearly linear, speeding up inference several times and preventing slowdowns related to the input size.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In essence, with KV caching the transformer's attention in each layer will take the new token‚Äôs query and concatenate it with the cached keys of prior tokens, then do the same for values, and move on immediately. The result is that each generation step‚Äôs workload is greatly reduced: the model focuses on what‚Äôs new instead of re-hashing the entire context every time.&lt;/p&gt;
&lt;h2&gt;Implementation&lt;/h2&gt;
&lt;p&gt;Modern libraries implement KV caching under the hood by carrying a ‚Äúpast key values‚Äù or similar object through successive generation calls. For example, the Hugging Face Transformers library‚Äôs &lt;code&gt;generate&lt;/code&gt; function uses a &lt;code&gt;use_cache&lt;/code&gt; flag that is True by default, meaning it will automatically store and reuse past keys/values between decoding steps. Conceptually, you can imagine that after the first forward pass on the prompt, the model keeps all the K and V tensors. When generating the next token, it feeds only the new token through each layer along with the cached K and V from previous tokens, to compute the next output efficiently.&lt;/p&gt;
&lt;p&gt;In summary, KV caching transforms the workload of each generation step. Without caching, each step &lt;em&gt;repeats&lt;/em&gt; the full attention computation over the entire context. With caching, each step adds only the computations for the new token and the necessary interactions with prior tokens. This makes the per-step cost roughly constant. The longer the generation goes on, the more time is saved relative to the naive approach. KV caching is thus a &lt;strong&gt;time-memory trade-off&lt;/strong&gt;: we trade some memory to store the cache in order to save a lot of compute time on each step.&lt;/p&gt;
&lt;h2&gt;Limitations&lt;/h2&gt;
&lt;p&gt;It‚Äôs important to note that KV caching only applies in &lt;em&gt;auto-regressive decoder&lt;/em&gt; models (where the output is generated sequentially). Models like BERT that process entire sequences in one go (and are not generative) do not use KV caching, since they don‚Äôt generate token-by-token or reuse past internal states. But for any generative LLM built on a decoder-only Transformer architecture, KV caching is a standard technique to speed up inference. &lt;/p&gt;
&lt;p&gt;It's worth noting that the KV cache needs to be managed just like every other type of cache. We're going to analyze some ways to handle this cache effectively in another post.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The takeaway is clear: &lt;strong&gt;always leverage KV caching for autoregressive LLM inference&lt;/strong&gt; (and practically all libraries do this for you) unless you have a very specific reason not to. It will make your LLM deployments run faster and more efficiently. &lt;/p&gt;
&lt;p&gt;KV caching exemplifies how understanding the internals of transformer models can lead to substantial engineering improvements. By recognizing that keys and values of the attention mechanism can be reused across time steps, we unlock a simple yet powerful optimization. This ensures that even as our LLMs get larger and our prompts get longer, we can keep inference running quickly, delivering the snappy responses users expect from AI-driven applications.&lt;/p&gt;
&lt;h2&gt;Learn more&lt;/h2&gt;
&lt;p&gt;Here are some useful resources I used to write this post:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer"&gt;The Illustrated Transformer&lt;/a&gt; by Jay Alammar&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jalammar.github.io/illustrated-gpt2/" target="_blank" rel="noopener noreferrer"&gt;The illustrated GPT-2&lt;/a&gt; by Jay Alammar&lt;/li&gt;
&lt;li&gt;&lt;a href="https://platform.openai.com/docs/guides/latency-optimization/3-use-fewer-input-tokens#use-fewer-input-tokens" target="_blank" rel="noopener noreferrer"&gt;Latency optimization tips&lt;/a&gt; by OpenAI&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@joaolages/kv-caching-explained-276520203249" target="_blank" rel="noopener noreferrer"&gt;KV Caching explained&lt;/a&gt; by Jo√£o Lages&lt;/li&gt;
&lt;li&gt;&lt;a href="https://neptune.ai/blog/transformers-key-value-caching" target="_blank" rel="noopener noreferrer"&gt;KV Caching&lt;/a&gt; by Neptune.ai&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.manning.com/books/build-a-large-language-model-from-scratch" target="_blank" rel="noopener noreferrer"&gt;Build a Large Language Model (from scratch)&lt;/a&gt; by Sebastian Raschka&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>What is prompt caching?</title><link>https://www.zansara.dev/posts/2025-10-17-prompt-caching/</link><pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/posts/2025-10-17-prompt-caching/</guid><description>&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;This is episode 2 of a series of shorter blog posts answering questions I received during the course of my work and reflect common misconceptions and doubts about various generative AI technologies. You can find the whole series here: &lt;a href="/series/practical-questions"&gt;Practical Questions&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;A common piece of advice to improve speed and reduce cost of inference in LLMs is to use prompt caching. However, it's often not clear what this means. What exactly is cached? When and why the improvements are really impactful? Understanding prompt caching starts with a deeper awareness of how computation and costs scale with large contexts.&lt;/p&gt;
&lt;h2&gt;LLMS are stateless&lt;/h2&gt;
&lt;p&gt;Each time an LLM processes input, it handles every token of the provided context. LLMs are stateless: this means that for every new message added to an existing chat, your application needs to submit the whole history which could include system prompts, documents, examples, and all the chat history. &lt;br /&gt;
The model recomputes all of those tokens each time. &lt;/p&gt;
&lt;p&gt;This is a massive inefficiency. For example, with an input cost around $1 per 1 million tokens, sending 100,000 tokens across 1,000 requests would cost approximately $100, while about 95% of those tokens remain unchanged across requests. In essence, a large portion of computation is wasted on repeatedly processing information that never changes: the message history.&lt;/p&gt;
&lt;h2&gt;Stateless vs stateful design&lt;/h2&gt;
&lt;p&gt;Naive API implementations that omit caching force the model to process the entire context anew each time. This "stateless" method is simpler to implement, but wastefully expensive. The system pays repeatedly to recompute static context, which could otherwise be reused.&lt;/p&gt;
&lt;p&gt;In contrast, with a stateful cache strategy, the system stores parts of the context and only processes new inputs (queries). Consider the following case:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the system prompt is 10,000 tokens long &lt;/li&gt;
&lt;li&gt;each user message is about 100 tokens&lt;/li&gt;
&lt;li&gt;each assistant response is about 1000 tokens&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In both cases, the first request processes 10,100 tokens (1 system prompt + 1 user message). On the second message, a stateless request (no caching) needs to process 11,200 tokens (1 system prompt + first user message + first assistant response + the next user message) while a stateful one can first load the cache and then process only 1100 new tokens (the assistant response + the new user message). That's an order of magnitude less tokens!&lt;br /&gt;
On top of that, as the chat continues, a stateful app will always need to only process the next new 1100 tokens, while the stateless version will process a chat history that grows by 1100 every time. For example, by the 10th request, with caching you need to process 1100 tokens, while without you need to deal with 20,000! (10,000 system prompt tokens + 9,000 assistant reply tokens + 1000 user message tokens).&lt;/p&gt;
&lt;p&gt;Here's a recap to highlight the difference:&lt;/p&gt;
&lt;div style="text-align: center;"&gt;

&lt;table style="width:100%; border: 2px solid black;"&gt;
&lt;tr&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;No Prompt Caching&lt;/th&gt;
    &lt;th&gt;With Prompt Caching&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;td&gt;1st request&lt;/td&gt;
    &lt;td&gt;10,100 tokens&lt;br&gt;&lt;small&gt;10,000tk sys + 100tk user&lt;/small&gt;&lt;/td&gt;
    &lt;td&gt;10,100 tokens&lt;br&gt;&lt;small&gt;10,000tk sys + 100tk user&lt;/small&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;td&gt;2nd request&lt;/td&gt;
    &lt;td&gt;11,200 tokens&lt;br&gt;&lt;small&gt;10,000tk sys + 1000tk llm + (100 * 2) tk user&lt;/small&gt;&lt;/td&gt;
    &lt;td&gt;1100 tokens&lt;br&gt;&lt;small&gt;1000tk llm + 100tk user&lt;/small&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;td&gt;...&lt;/td&gt;
    &lt;td&gt;...&lt;/td&gt;
    &lt;td&gt;...&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;td&gt;10th request&lt;/td&gt;
    &lt;td&gt;20,000 tokens&lt;br&gt;&lt;small&gt;10,000tk sys + (1000 * 9)tk llm + (100 * 10) tk user&lt;/small&gt;&lt;/td&gt;
    &lt;td&gt;1100 tokens&lt;br&gt;&lt;small&gt;1000tk llm + 100tk user&lt;/small&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;/div&gt;

&lt;p&gt;While cache warm-up is not free, it can make a huge difference in the latency of your responses and, if you're paying by the output token, reduce the costs by orders of magnitude.&lt;/p&gt;
&lt;h2&gt;Cache Hierarchies&lt;/h2&gt;
&lt;p&gt;Caching‚Äôs benefits come with architectural tradeoffs. Stateless designs are straightforward and predictably expensive: every token is always processed. Caching drastically reduces costs by reusing prior computation, but requires complexity in cache management, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cache invalidation: deciding how and when to refresh cached segments.&lt;/li&gt;
&lt;li&gt;Cache misses: when requested information isn‚Äôt in the cache, leading to full recomputation and latency spikes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because of these challenges, a single monolithic cache usually not enough to see many benefits. The most effective solution is a &lt;strong&gt;hierarchical cache strategy&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Effective prompt caching leverages multiple layers with varied lifetimes and hit rates:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;L1: System Prompt (e.g., 5,000 tokens)&lt;/strong&gt;: it rarely changes, so it has the best hit rate. In most chat you'll at least hit this cache.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;L2: System Prompt + Examples and Tools (e.g., +20,000 tokens)&lt;/strong&gt;: may change per task, so it can has a lower hit rate than the system prompt, but eventually it depends completely on your application type. Agentic apps that make heavy use of tools benefit the most from caching them, as they follow the system prompt and might not depend at all from the user query or the agent's decisions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;L3: System Prompt + Examples and Tools + Documents (e.g., +50,000 tokens)&lt;/strong&gt;: if you're working with documents, caching any initial retrieval can help too. These documents are likely to change per user and/or per session, so it has a moderate/low hit rate. However, the size of these chunks usually makes it worth it if you have some spare capacity or a small and static knowledge base to retrieve from.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A layered approach like balances freshness and reuse, optimizing both cost and performance.&lt;/p&gt;
&lt;h2&gt;Automatic prefix caching&lt;/h2&gt;
&lt;p&gt;If you're using a modern inference engine, prompt caching can also be done through &lt;strong&gt;automatic prefix caching&lt;/strong&gt;, where the engine itself takes the responsibility to identify and cache frequently used prefixes. Here you can find more details about the availability of this feature in &lt;a href="https://docs.vllm.ai/en/latest/design/prefix_caching.html" target="_blank" rel="noopener noreferrer"&gt;vLLM&lt;/a&gt;, &lt;a href="https://docs.sglang.ai/advanced_features/hicache_best_practices.html" target="_blank" rel="noopener noreferrer"&gt;SDLang&lt;/a&gt; and &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/8947" target="_blank" rel="noopener noreferrer"&gt;llama.cpp&lt;/a&gt;, but there are many other engines supporting it.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-10-17-prompt-caching/optimizations_table-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A feature comparison across inference engines from &lt;a href="https://arxiv.org/pdf/2505.01658" target="_blank" rel="noopener noreferrer"&gt;this May 2025 review&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Semantic caching&lt;/h2&gt;
&lt;p&gt;In extreme cases where cost, load or latency must be reduced to the maximum, semantic caching can also be employed. Semantic caching allows you to cache also the user queries and the assistant responses by keeping a registry of already processes user queries and performing a semantic search step between the new query and the cached ones. If a match is found, instead of invoking the LLM to generate a new answer, the cached reply is sent to the user immediately.&lt;/p&gt;
&lt;p&gt;Semantic caching however has several disadvantages that makes it worthwhile only in rare situations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Access control&lt;/strong&gt;. Caching must be done per user if each user has access to a different set of resources, to avoid accidental sharing of data and/or resources across users. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Very high similarity needed&lt;/strong&gt;: In order for the reply to be relevant, the semantic similarity between the two must be extremely high, or you risk that the answer returned to the user won't match their question. Semantic similarity tends to overlook details which are often very important to an accurate reply: for example, "What's the sum of these numbers: 1,2,3,4,5,6,7?" and  "What's the sum of these numbers: 1,2,3,4,5,6,7,8?" will have an extremely high similarity, but returning the response of the first to the second would not be a good idea.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Language management&lt;/strong&gt;: what to do when the exact same question is asked in two different languages? Semantic similarity may be perfect if your embedder is multilingual, but the user won't be pleased to receive a cached answer in a language different from their own.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Such constraints make cache misses extremely frequent, which defies the point of keeping a cache and simply adds complexity and latency to the system instead of reducing it. The similarity pitfalls introduces also nasty accuracy problems.&lt;/p&gt;
&lt;p&gt;In my personal experience, semantic caching is only useful for extremely high volume, low cost, public facing interfaces where accuracy is not critical. A perfect example could be a virtual assistant for anonymous customer support, or a helper bot for a software's documentation search. In any case, you usually need additional checks on the output in order to trust such a system.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Prompt caching is not just about cutting costs or speeding things up: it is a necessary architectural approach that addresses the quadratic computational cost inherent in large-context LLM processing. Without it, your backend will repeatedly recompute largely static information, wasting resources and imposing latency penalties that impact your user's experience. By adopting hierarchical, stateful caching and carefully designing prompts, you can reduce token processing costs and response speed by orders of magnitude, which is key for building sustainable, high-performance applications.&lt;/p&gt;
&lt;p&gt;In the next post we will talk in detail about &lt;a href="/posts/2025-10-23-kv-caching/"&gt;how prompt caching works&lt;/a&gt;. Be ready for a more technical deep dive into the architecture of LLMs!&lt;/p&gt;</description></item><item><title>Why using a reranker?</title><link>https://www.zansara.dev/posts/2025-10-09-rerankers/</link><pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/posts/2025-10-09-rerankers/</guid><description>&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;This is episode 1 of a series of shorter blog posts answering questions I received during the course of my work and reflect common misconceptions and doubts about various generative AI technologies. You can find the whole series here: &lt;a href="/series/practical-questions"&gt;Practical Questions&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Retrieval-Augmented Generation (RAG) systems are essential to connect large language models  with external knowledge sources. While in theory the retrieval step is enough to gather documents that are relevant to the user's request, it's often recommended to add an additional ranking step, the &lt;em&gt;reranking&lt;/em&gt;, to further filter the results.&lt;/p&gt;
&lt;p&gt;But why do we need rerankers? Isn‚Äôt semantic search good enough? The answer lies in understanding the limitations of traditional embedding-based retrieval.&lt;/p&gt;
&lt;h2&gt;Bi-encoders vs Cross-encoders&lt;/h2&gt;
&lt;p&gt;At the heart of modern, scalable semantic search systems lies the &lt;strong&gt;bi-encoder&lt;/strong&gt; model. This architecture creates independent vector representations for the query and the document; relevance is then computed through a similarity measure like the dot product or cosine similarity between those vectors.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-10-09-rerankers/bi-encoders-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;This design scales well: You can precompute document embeddings, store them in your vector DB, and compare any incoming query against millions of documents very efficiently. However, this convenience comes at a cost: &lt;strong&gt;the system never truly reads the document in the context of the query&lt;/strong&gt;. There‚Äôs no token-level interaction between the query and document embedding to judge whether the document actually answers the question or it simply happen to be talking about the same topic, and therefore semantically similar.&lt;/p&gt;
&lt;p&gt;For example, the query "How to protect my application from DDOS attacks?" may be semantically close to the statement "You should always take steps to protect your systems from DDOS attacks", but the statement does not contain the answer to the question. Without reranking, embedding-based retrieval systems often perform well at recall but poorly at precision.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cross-encoders&lt;/strong&gt; remedy the limitations of bi-encoders by encoding the query and document together, typically separated by a special token (like &lt;code&gt;[SEP]&lt;/code&gt;) using an encoder-only Transformer such as BERT. Then they include an additional fully connected layer that acts as a classifier, learning fine-grained token-level interactions that capture query/document word alignments, answer containment (i.e., does this passage actually answer the question?) and overall contextual relevance.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-10-09-rerankers/cross-encoders-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;This difference (from separate encodings to a joint representation) gives cross-encoders their power but also their cost. Since relevance depends on the specific query, you can‚Äôt precompute the document embeddings: in fact, the concept of "query embedding" and "document embeddings" disappears.  Every query-document pair requires a fresh forward pass through the whole model, which can be prohibitively expensive on a large corpus.&lt;/p&gt;
&lt;h2&gt;To each their place&lt;/h2&gt;
&lt;p&gt;No production system can afford to run interaction-rich models such as cross-encoders on millions of documents per query. Therefore, the two-stage retrieval pipeline remains the industry standard:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semantic Search (Bi-Encoder)&lt;/strong&gt; ‚Äì Quickly narrows a massive corpus (e.g., millions of document chunks) down to a small candidate set (e.g., top 100 chunks). Bi-encoders can be built with any embedding model: popular closed source embedders include OpenAI's, Voyage.ai, Cohere's, Gemini and more, while on the open-source front you can find BGE embedders, Mistral's models, Jina.ai, Gemma, IBM Granite, &lt;a href="https://huggingface.co/models?search=embedding" target="_blank" rel="noopener noreferrer"&gt;and more&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reranking (Cross-Encoder)&lt;/strong&gt; ‚Äì Evaluates those top 100 candidates more deeply by jointly encoding the query and document. A popular closed source choice for reranking models is Cohere's, while on the open source front you can find several Qwen-based rerankers, Jina.ai models, IBM's Granite rerankers, BGE rerankers, and &lt;a href="https://huggingface.co/models?search=reranker" target="_blank" rel="noopener noreferrer"&gt;many more&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-10-09-rerankers/two-tiered-system-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;h2&gt;Making Reranking Practical&lt;/h2&gt;
&lt;p&gt;Even in this two-tiered system, reranking may turn out to be too expensive for your latency constrains, but several engineering and modeling strategies have emerged to make it viable in production. Let‚Äôs break down a few of these methods.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Model Distillation&lt;/strong&gt;&lt;br /&gt;
    Distillation transfers the knowledge from a large, high-performing cross-encoder (often based on 12-layer BERT or similar) into a smaller student model (e.g., 6 layers, or even lighter). The process involves training the smaller model to mimic the scores or output logits of the larger one on large query‚Äìdocument pairs. While distillation inevitably loses some performance, careful tuning, domain-specific data, and intermediate-layer supervision can retain more than 90% of the original ranking quality at a fraction of the inference cost. You can learn more about model distillation &lt;a href="https://www.sbert.net/examples/sentence_transformer/training/distillation/README.html" target="_blank" rel="noopener noreferrer"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Listwise Reranking&lt;/strong&gt;&lt;br /&gt;
    Instead of scoring each query‚Äìdocument pair independently, listwise reranking generates scores for all top-k candidates in a single forward pass. This approach rearranges candidates into a batched tensor, leveraging GPU parallelism to process them together, reducing overhead from repeated encoder calls. Some implementations also use listwise loss functions (such as ListNet or LambdaMART-inspired objectives) to better preserve ranking order during training. To learn more about ML ranking, have a look at &lt;a href="https://towardsdatascience.com/ranking-basics-pointwise-pairwise-listwise-cd5318f86e1b/" target="_blank" rel="noopener noreferrer"&gt;this post&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Late Interaction Models (e.g., ColBERT)&lt;/strong&gt;&lt;br /&gt;
    Late interaction approaches store token-level embeddings of documents from fine-tuned contextual models. At query time, the system encodes the query tokens and performs efficient maximum similarity matching between query tokens and stored document tokens. By avoiding a full joint encoding across all tokens, these models approximate cross-encoder analysis but keep retrieval latency close to bi-encoder speeds. This approach can either substitute or complement cross-encoders by quickly reducing the candidates list returned from the vector database. To learn more about this approach, have a look at &lt;a href="https://medium.com/@aimichael/cross-encoders-colbert-and-llm-based-re-rankers-a-practical-guide-a23570d88548" target="_blank" rel="noopener noreferrer"&gt;this blog post&lt;/a&gt; or &lt;a href="https://arxiv.org/abs/2004.12832" target="_blank" rel="noopener noreferrer"&gt;the ColBERT paper&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Candidate Filtering and Adaptive k&lt;/strong&gt;&lt;br /&gt;
    Rather than always reranking a fixed top-k (like 100 documents), systems can use heuristics or intermediate classifiers to select fewer candidates when confidence in retrieval is high. This adaptive approach can cut reranking costs significantly while preserving precision in challenging cases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Approximate Cross-Attention Mechanisms&lt;/strong&gt;&lt;br /&gt;
    Instead of computing full self-attention across combined query and document tokens, some approaches reduce complexity by limiting cross-attention depth or dimensionality ‚Äî for example, attending only to the top N most informative tokens, or pruning low-importance attention heads. This can drastically lower token computations while maintaining critical interaction signals.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Caching for Frequent Queries&lt;/strong&gt;&lt;br /&gt;
    In platforms where certain queries or query patterns repeat, caching reranking results or partial computations can remove the need to rerun the full cross-encoder. Combined with normalization and paraphrase detection, such caches can return precise results instantly for repeated requests.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In production pipelines, these methods are often stacked: for example, using late interaction for most queries, distillation for cost control, and adaptive candidate selection to minimize unnecessary work. The overarching theme is balancing precision and latency, ensuring that rerankers deliver their interaction-driven relevance boost without overwhelming the system‚Äôs budget or responsiveness.&lt;/p&gt;</description></item><item><title>Trying to play "Guess Who" with an LLM</title><link>https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/</link><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/</guid><description>&lt;style&gt;
    p img {
        max-width: 500px;
    }

    p:has(&gt; img) {
        text-align:center!important;
    }

    pre {
        overflow: wrap;
    }
&lt;/style&gt;

&lt;p&gt;A few days ago I came to a realization. Modern LLMs can do a lot of things: they can &lt;a href="https://www.anthropic.com/news/claude-for-chrome" target="_blank" rel="noopener noreferrer"&gt;use a browser&lt;/a&gt; just like a human, they can (&lt;a href="https://dynomight.net/chess/" target="_blank" rel="noopener noreferrer"&gt;sometimes&lt;/a&gt;) &lt;a href="https://maxim-saplin.github.io/llm_chess/" target="_blank" rel="noopener noreferrer"&gt;play chess&lt;/a&gt;, and they seem to be so smart that they apparently can be trusted as personal assistants: they can read and reply to emails, organize trips, do shopping online on your behalf, and so on.&lt;/p&gt;
&lt;p&gt;If that's the case, I thought, it should be possible to also play some tabletop games with them!&lt;/p&gt;
&lt;p&gt;After all, many simple tabletop games don't require a lot of skill to play. You need to be able to read and understand the rules (very easy for an LLM), you need eyes to see the board (piece of cake for a multimodal LLM), and some ways to interact with the board (most LLM are able to call tools nowadays). So I figured it would be a nice idea to try and figure out which of these LLMs is the most fun to play with. Maybe the charming personality of GPT-4o? Or the clever Claude Opus 4?&lt;/p&gt;
&lt;p&gt;I did not expect any of the results I got.&lt;/p&gt;
&lt;h2&gt;Building the game&lt;/h2&gt;
&lt;p&gt;In order to be fair to dumber LLMs, I decided to start with a very simple tabletop game: &lt;a href="https://en.wikipedia.org/wiki/Guess_Who%3F" target="_blank" rel="noopener noreferrer"&gt;Guess Who&lt;/a&gt;. If you are not familiar with "Guess Who", here is a quick recap of the rules:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each player has a board full of characters.&lt;/li&gt;
&lt;li&gt;Each players draws an additional random character.&lt;/li&gt;
&lt;li&gt;Your goal is to guess which character the other player has received by asking yes/no questions, such as "Is your character male?" or "Does your character have black hair?" and so on&lt;/li&gt;
&lt;li&gt;The first player to guess the opponent character's name wins.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As you can see we're not talking of a complex game like Catan or a strategy game like chess, but a simple, fun tabletop game suitable for kids too. &lt;/p&gt;
&lt;p&gt;In order to build the game, as I am no frontend developer, I spent a few too many bucks on my favorite vibe-coding tool, &lt;a href="https://www.anthropic.com/claude-code" target="_blank" rel="noopener noreferrer"&gt;Claude Code&lt;/a&gt;, padded in a bit of &lt;a href="https://github.com/google-gemini/gemini-cli" target="_blank" rel="noopener noreferrer"&gt;Gemini CLI&lt;/a&gt; when I run out of credits, made a few tweaks by hand when asking the bots to do so felt overkill, and a few evenings later I had &lt;a href="/guess-who/"&gt;this nice Guess Who game&lt;/a&gt; live.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/game-ui.png" /&gt;&lt;/p&gt;
&lt;p&gt;Feel free to play a few round using your favorite LLM. The game supports OpenAI compatible endpoints, plus Anthropic's and Google's API. And if you don't trust me with your API key, go ahead and &lt;a href="https://github.com/ZanSara/guess-who" target="_blank" rel="noopener noreferrer"&gt;fork or clone the game&lt;/a&gt; (and maybe leave a ‚≠ê while you're at it ), host it where you like (it's a single HTML page with a bit of vanilla JS at the side) and have fun.&lt;/p&gt;
&lt;p&gt;Now for the spoilers.&lt;/p&gt;
&lt;h2&gt;Not as many LLMs&lt;/h2&gt;
&lt;p&gt;One of the first surprises was that, in practice, there aren't as many models that are capable of vision and tool calling at the same time. Apart from flagship models such as GPTs and Claude, OSS options were limited. Even GPT-OSS, unfortunately, does not support vision. I was especially surprised to learn that I could not play with any version of popular Chinese models such as Qwen or Deepseek, as they're either text only or unable to call tools.&lt;/p&gt;
&lt;p&gt;Either way, using a mix of proprietary hosting, &lt;a href="https://openrouter.ai/" target="_blank" rel="noopener noreferrer"&gt;OpenRouter&lt;/a&gt; and &lt;a href="https://www.together.ai/" target="_blank" rel="noopener noreferrer"&gt;Together.ai&lt;/a&gt;, I had plenty of models to try and ended up trying out 21:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Nova Pro v1&lt;/li&gt;
&lt;li&gt;Amazon Nova Lite v1&lt;/li&gt;
&lt;li&gt;Claude Opus 4.1&lt;/li&gt;
&lt;li&gt;Claude Opus 4.0&lt;/li&gt;
&lt;li&gt;Claude Sonnet 4.0&lt;/li&gt;
&lt;li&gt;Claude Sonnet 3.7&lt;/li&gt;
&lt;li&gt;Gemini 2.5 Pro&lt;/li&gt;
&lt;li&gt;Gemini 2.5 Flash&lt;/li&gt;
&lt;li&gt;Gemini 2.5 Flash Lite&lt;/li&gt;
&lt;li&gt;GML 4.5&lt;/li&gt;
&lt;li&gt;Grok 4&lt;/li&gt;
&lt;li&gt;GPT 5&lt;/li&gt;
&lt;li&gt;GPT 5 Nano&lt;/li&gt;
&lt;li&gt;GPT 5 Mini&lt;/li&gt;
&lt;li&gt;GPT 4o&lt;/li&gt;
&lt;li&gt;Llama 4 Maverick&lt;/li&gt;
&lt;li&gt;Llama 4 Scout&lt;/li&gt;
&lt;li&gt;Mistral Medium 3.1&lt;/li&gt;
&lt;li&gt;Mistral Small 3.2&lt;/li&gt;
&lt;li&gt;Sonoma Dusk Alpha&lt;/li&gt;
&lt;li&gt;Sonoma Sky Alpha&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It may sound like a lot of work, but as you'll see in a minute, for many of them it didn't take me long to form an opinion about their skill.&lt;/p&gt;
&lt;h2&gt;The prompts&lt;/h2&gt;
&lt;p&gt;Starting from the assumption that playing Guess Who should be within the cognitive abilities of most modern LLMs, I decided to settle for a simple system prompt, something that resembles the way I would explain the game to a fellow human.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You are an AI assistant playing "Guess Who" against the user. Here's how the game works:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You'll receive the board and your character image&lt;/li&gt;
&lt;li&gt;You must try to guess the user's character by asking yes/no questions&lt;/li&gt;
&lt;li&gt;You must answer the user's yes/no questions about your character&lt;/li&gt;
&lt;li&gt;One question per player per turn, no exceptions&lt;/li&gt;
&lt;li&gt;You can eliminate characters from your board based on the user's answers using the eliminateCharacter tool (this will only update the UI, so keep in mind who you're eliminating)&lt;/li&gt;
&lt;li&gt;The first player to correctly guess the opponent's character wins the game. When the user guesses your character or you guess theirs, call the endGame tool&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;After this system prompt, I send two more prompts:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Here is the board:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/full-board.png" /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;and here is your character:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/Amy.png" /&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately these two prompts need to be user prompts (not system prompts) because some LLMs (looking at you, Mistral!) do not support images in their system prompts.&lt;/p&gt;
&lt;p&gt;Last, when the user presses the Start button, one more system message is sent:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Generate a brief, friendly greeting message to start a Guess Who game. &lt;br /&gt;
Tell the user whether you received the images of your board and your character and ask them for their first question. &lt;br /&gt;
Keep it conversational and under 2 sentences.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The LLM also receives two tools to use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;eliminateCharacter&lt;/code&gt;, described as "Eliminate a character from your board when you learn they cannot be the user's character".&lt;/li&gt;
&lt;li&gt;&lt;code&gt;endGame&lt;/code&gt;, described as "When you or the user guess correctly, call this tool to end the game."&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Playing&lt;/h2&gt;
&lt;p&gt;With the game implemented and ready to go, I finally started playing a bit. I was especially curious how small models could deal with a game like this, so I began with GPT-5 Mini. Here is what happens:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/gpt-5-mini-example.png" /&gt;&lt;/p&gt;
&lt;p&gt;Ahah, GPT 5 Mini is far dumber than I thought! Let's try Gemini 2.5 Flash instead.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/gemini-2.5-flash-example.png" /&gt;&lt;/p&gt;
&lt;p&gt;Oh wow this is incredible. Ok, time to try a smarter model and have some actual fun. Claude Sonnet 4.0 will do for now.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/claude-sonnet-4-example.png" /&gt;&lt;/p&gt;
&lt;p&gt;At this point it started to become unbelievable. Did I fail to explain the game? Is something wrong with the prompts? It couldn't be, because some other models (such as the almighty GPT-4o) do what I expect instead:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/gpt-4o-example.png" /&gt;&lt;/p&gt;
&lt;p&gt;While others left me shocked:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/claude-opus-4.1-example.png" /&gt;&lt;/p&gt;
&lt;p&gt;How can a flagship model like &lt;em&gt;Claude Opus 4.1&lt;/em&gt; fail this way? I kept trying several other LLMs in disbelief, slowly coming to terms with the fact that most of them don't readily understand the concept of playing adversarial games, even simple ones as Guess Who.&lt;/p&gt;
&lt;h2&gt;A systematic review&lt;/h2&gt;
&lt;p&gt;At this point I felt the duty to document this problem across all the models that had enough capabilities (vision + tool calling) to play this game. If I ever want an LLM personal assistant to handle my private data and to act on my behalf, I'd better make sure they understand that they can't just hand out my credentials to the first kind thief that asks them.&lt;/p&gt;
&lt;p&gt;Here is a systematic review of the results, ordered roughly from worst to best. However, keep in mind that this is all based on a very small test sample, and although most models consistently fail the same way every time, there were some with a far more erratic behavior, looking very smart at times and incredibly dumb the next. &lt;/p&gt;
&lt;p&gt;First of all I list and disqualify all models that do not hide the identity of their character. Of the survivors, I ranked them by whether or not you can actually play with them in any capacity (many can't see well enough to tell the characters apart) and if the game is actually playable, how easy it is to break it.&lt;/p&gt;
&lt;h3&gt;Unplayable models&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Can't understand the instructions at all&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These models understood only part of the system prompt (if any), resulting in unpredictable answers.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Amazon Nova Lite v1&lt;/summary&gt;

&lt;p&gt;Possibly the most unpredictable model. Every run was a surprise. This is just a small sample to give you an idea.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/cant-play-at-all/amazon-nova-lite-v1.png" /&gt;&lt;/p&gt;

&lt;/details&gt;

&lt;p&gt;&lt;strong&gt;Reveals their charater unprompted in the first message&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Shockingly common issue among all tested models. They just volunteer the information unprompted. I assume they don't understand they're not supposed to help the user, or that this is an information they should hide.&lt;/p&gt;
&lt;p&gt;All these models have been tested several times to ensure this is their default behavior and not an exception. Some other models do occasionally fail this way (looking at you, Mistral Medium 3.1), but only rarely. Models listed here fail in this way very consistently.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Claude Opus 4.1&lt;/summary&gt;

&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/claude-opus-4-1.png" /&gt;&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Claude Opus 4.0&lt;/summary&gt;

&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/claude-opus-4.png" /&gt;&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Claude Sonnet 4.0&lt;/summary&gt;

&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/claude-sonnet-4.png" /&gt;&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Claude Sonnet 3.7&lt;/summary&gt;

&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/claude-sonnet-3-7.png" /&gt;&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Gemini 2.5 Flash&lt;/summary&gt;

&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/gemini-2.5-flash.png" /&gt;&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Gemini 2.5 Flash Lite&lt;/summary&gt;

&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/gemini-2.5-flash-lite.png" /&gt;&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;GPT 5 Mini&lt;/summary&gt;

&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/gpt-5-mini.png" /&gt;&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;GPT 5 Nano&lt;/summary&gt;

&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/gpt-5-nano.png" /&gt;&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Llama 4 Scout&lt;/summary&gt;

&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/llama-4-scout.png" /&gt;&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Sonoma Sky Alpha&lt;/summary&gt;

&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/sonoma-sky-alpha.png" /&gt;&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;GML 4.5&lt;/summary&gt;

&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/gml-4.5.png" /&gt;&lt;/p&gt;

&lt;/details&gt;

&lt;p&gt;&lt;strong&gt;Reveals their charater as soon as asked&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Some models did not volunteer the information but didn't exactly protect it either.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Amazon Nova Pro v1&lt;/summary&gt;

&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-when-asked/amazon-nova-pro-v1.png" /&gt;&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Llama 4 Maverick&lt;/summary&gt;

&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-when-asked/llama-4-maverick.png" /&gt;&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Mistral Small 3.2&lt;/summary&gt;

&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-when-asked/mistral-small-3.2.png" /&gt;&lt;/p&gt;

&lt;/details&gt;

&lt;h3&gt;Game looks playable but it's actually broken&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Low vision skills&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These models are smart enough to understand the basics of the game, but it's impossible to play with them due to their &lt;strong&gt;weak vision skills&lt;/strong&gt;. These models simply can't see well enough to delete the right character from the board or answer correctly all questions about their own. They will then hallucinate random answers and delete random characters from their boards, making the game unplayable.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Gemini 2.5 Pro&lt;/summary&gt;

&lt;p&gt;Gemini 2.5 Pro evidently has issues seeing both the board and the characters. Here it shows both flaws by deleting the wrong characters and lying about its character in a single response:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/cant-see-well/gemini-2.5-pro.png" /&gt;
&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/cant-see-well/gemini-2.5-pro-board.png" /&gt;&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;GPT-4o&lt;/summary&gt;

&lt;p&gt;GPT-4o also has issues seeing the board and the characters, but its blind spots less predictable than for Gemini 2.5 Pro, so it can occasionally manage to play for a while. It also frequently forgets to eliminate any characters from its board. GPT-4o also tends to get distracted, lose track of the turns, and so on.&lt;/p&gt;
&lt;p&gt;Here it deletes the wrong characters and loses track of the turns:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/cant-see-well/gpt-4o-wrong-removal.png" /&gt;
&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/cant-see-well/gpt-4o-wrong-removal-board.png" /&gt;&lt;/p&gt;
&lt;p&gt;and here it has trouble seeing its character:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/cant-see-well/gpt-4o-character.png" /&gt;&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Mistral Medium 3.1&lt;/summary&gt;

&lt;p&gt;Mistral Medium 3.1 has been hard to place. It seems that its biggest weakness is removing the correct characters from the board, although it does a much better job than Gemini 2.5 or GPT-4o. I've never seen it failing to describe its own character correctly, but it occasionally behaves in a very dumb way (on occasion it even revealed its character in the first message!). You may have flawless runs with this model or it might fail on the get-go.&lt;/p&gt;
&lt;p&gt;Here it deletes a couple of unrelated characters:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/cant-see-well/mistral-medium-3.1-wrong-removal.png" /&gt;
&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/cant-see-well/mistral-medium-3.1-wrong-removal-board.png" /&gt;&lt;/p&gt;

&lt;/details&gt;

&lt;p&gt;&lt;strong&gt;No tool calling&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is debatable whether the inability of a model to do tool calling should be considered a penalty: in theory LLMs remember everything perfectly, so they could choose what to ask next based on what they asked earlier and what characters still could match the opponent's. However, in practice no LLM could be trusted keeping track of the game this way, and I decided that the inability to invoke tools when instructed is a big enough flaw to disqualify them.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Sonoma Dusk Alpha&lt;/summary&gt;

&lt;p&gt;Assessing the vision skills of this model has been difficult due to its unwillingness to ever call the &lt;code&gt;eliminateCharacter&lt;/code&gt; tool. Sonoma Dusk Alpha doesn't seem to have issues seeing its character, but it's too weak to be considered playable: won't enforce turn taking, can be convinced I won the game without naming its character, and it's likely not really trying to narrow down on my character, it's just asking some questions.&lt;/p&gt;
&lt;p&gt;Here is an example gameplay.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/cant-see-well/sonoma-dusk-alpha.png" /&gt;&lt;/p&gt;

&lt;/details&gt;

&lt;h3&gt;Playable models&lt;/h3&gt;
&lt;p&gt;These models seems to understand the game, don't have issues seeing all the features of the characters, but they're still quite vulnerable to basic manipulation attempts. Typical issues are related to &lt;strong&gt;prompt hacking&lt;/strong&gt;, where the LLM simply does what I say rather than enforcing the game rules, and &lt;strong&gt;low tool handling ability&lt;/strong&gt;, where the LLM doesn't use the available tools when it should or uses them incorrectly.&lt;/p&gt;
&lt;p&gt;To test these skills, I checked whether the model will enforce turn taking when asking the question, and what happens when I claim to have won without naming the LLM's hidden character.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Grok 4&lt;/summary&gt;

&lt;p&gt;Grok 4 is a decent player but by far not a good one. It clearly sees the board and the character, it eliminates characters correctly most of the times, but fails to enforce turns.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/playable/grok-4.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/playable/grok-4-board.png" /&gt;&lt;/p&gt;
&lt;p&gt;Here an example of a game where a couple of mistakes were enough to prevent the model from winning (my character was Amy again).&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/playable/grok-4-failing-game-last-minute.png" /&gt;&lt;/p&gt;
&lt;p&gt;An award to this model for resisting my attempt to unilaterally declare victory without breaking the game! This is the only model that succeeded at this.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/playable/grok-4-resists-winning.png" /&gt;&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;GPT 5&lt;/summary&gt;

&lt;p&gt;GPT 5 is probably the best model to play with in terms of raw capabilities. It makes very occasional mistakes deleting characters but it's mostly on point. &lt;/p&gt;
&lt;p&gt;However it was really slow and annoying to get it to play at all. It generally can't seem to use tools and ask the next question at the same time, even if its response structure suggests it should be able to do it: this means that to play you must answer its question, wait for it to delete its character, and only then you can ask your own.&lt;/p&gt;
&lt;p&gt;It is also unbelievably slow compared to any other LLM I played with, which kills the fun.&lt;/p&gt;
&lt;p&gt;Here you can see GPT 5 enforcing turn-taking (plus a gratuitous pun?!):&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/playable/gpt-5-no-sense-of-humor.png" /&gt;&lt;/p&gt;
&lt;p&gt;When claiming that I won, GPT 5 almost manages to understand that it might be not the case, but still ruins the game. Unfortunately this is not a fluke, GPT 5 consistently reveals the character in this situation. It won't call the tool just yet, but once it reveals the character the game is over.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/misbehaves-when-I-say-I-won/gpt-5.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/misbehaves-when-I-say-I-won/gpt-5-again.png" /&gt;&lt;/p&gt;
&lt;p&gt;Here is an example of a game where GPT 5 actually wins:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/games/playable/gpt-5-full-winning-game.png" /&gt;&lt;/p&gt;
&lt;p&gt;In this case the &lt;code&gt;endGame&lt;/code&gt; tool was also invoked correctly.&lt;/p&gt;

&lt;/details&gt;

&lt;h2&gt;Can this be fixed?&lt;/h2&gt;
&lt;p&gt;My guess was that you can fix this behavior with a better system prompt. After this experiment I went back to the system prompt and described the game in far more detail.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You are an AI assistant playing "Guess Who" against the user. Here's how the game works.&lt;/p&gt;
&lt;p&gt;# Game Rules&lt;/p&gt;
&lt;p&gt;You will receive an image of the full Guess Who board showing all available characters. You will also receive an image of a specific character. This is YOUR character that the user must try to guess. REMEMBER: don't reveal who the character is! That's the point of the game!&lt;/p&gt;
&lt;p&gt;Your goal is to ask the user questions to identify THEIR hidden character while answering their questions about YOUR character. You need to ask the user yes/no questions about their character's appearance (e.g., "Does your character have glasses?", "Is your character male?"). When the user tells you something about THEIR character, you must eliminate characters that don't fit the description from your board using the eliminateCharacter tool. Keep in mind that this tool only updated the UI: you have to keep track of which characters are eliminated in your mind. Think carefully about which characters to eliminate and explain your reasoning out loud before calling the tool. Make sure to only eliminate characters that definitely do not match the user's description. If you make mistakes it will become impossible for you to win the game!&lt;/p&gt;
&lt;p&gt;When the user asks you questions about YOUR character, answer concisely and truthfully based on the character image you received. &lt;/p&gt;
&lt;p&gt;Each player can only ask ONE question and receive ONE answer - asking more than one question or asking another before your opponent had a chance to ask theirs is cheating! You must not cheat!&lt;/p&gt;
&lt;p&gt;The first player to correctly guess the opponent's character name wins the game, so try to guess when you're reasonably confident. A good time to guess is when your board only has one or two characters left. When you think you know the user's character, make your guess clearly (e.g., "Is your character [Name]?") This is how you can manage to win the game.&lt;/p&gt;
&lt;p&gt;When the user guesses correctly, call the endGame tool to finish the game. When the user tells you that you guessed their character, call the endGame tool to finish the game.&lt;/p&gt;
&lt;p&gt;Now you will receive YOUR board and YOUR character. Let's play!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can load this prompt &lt;a href="/guess-who/"&gt;in the game&lt;/a&gt; by checking the Advanced tab in the settings.&lt;/p&gt;
&lt;p&gt;This prompt helps a lot the models understand that they can't reveal the character's identity: however it's also not solving the problem entirely. For example this is what Claude Opus 4.1 does with this prompt:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-09-15-playing-guess-who-with-an-llm/claude-opus-4.1-spelled-out-prompt.png" /&gt;&lt;/p&gt;
&lt;p&gt;Guess what? There's only one character with gray hair and glasses on the board, and that's Emily... Should I review my system prompt again, make it even more detailed?&lt;/p&gt;
&lt;p&gt;At this point I gave up. Feel free to iterate on the prompt until you get one that works, and if you manage, I beg you to share it with me.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In the near future I plan to make a proper leaderboard for this simple game, to make an automated system to assess the model's skills and (hopefully) track their progress in this field.&lt;/p&gt;
&lt;p&gt;In the meantime, feel free to try your own favorite LLMs here and form your own opinion.&lt;/p&gt;
&lt;p&gt;However, let's be honest: if we need this level of effort to make Claude play such a simple game as Guess Who without messing up, how can we trust LLMs in general to handle our data and our money in the far more ambiguous and complex world out there? I suppose LLMs are not ready (yet) to be left unsupervised.&lt;/p&gt;
&lt;p class="fleuron"&gt;&lt;a href="/posts/2024-05-06-teranoptia/"&gt;SDE&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Can you really interrupt an LLM?</title><link>https://www.zansara.dev/posts/2025-06-02-can-you-really-interrupt-an-llm/</link><pubDate>Mon, 02 Jun 2025 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/posts/2025-06-02-can-you-really-interrupt-an-llm/</guid><description>&lt;p&gt;With the recent release of &lt;a href="https://support.anthropic.com/en/articles/11101966-using-voice-mode-on-claude-mobile-apps" target="_blank" rel="noopener noreferrer"&gt;Voice Mode&lt;/a&gt; for &lt;a href="https://www.anthropic.com/claude" target="_blank" rel="noopener noreferrer"&gt;Claude&lt;/a&gt;, it seems like Voice AI is a solved problem. Now that LLMs can speak natively, there's apparently no more need for any of the &lt;a href="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/"&gt;complex voice pipelines&lt;/a&gt; that used to be necessary last year: no need to do voice activity detection, no need to pipe data from the speech-to-text model to the LLM and then back to the text-to-speech engine at blazing speed in order to achieve a natural conversation flow. Modern LLMs can &lt;a href="https://vimeo.com/945587944" target="_blank" rel="noopener noreferrer"&gt;laugh and sing&lt;/a&gt;: what else could we need?&lt;/p&gt;
&lt;p&gt;It turns out, a lot is still missing. Here is an example:&lt;/p&gt;
&lt;div style="display: flex;"&gt;
  &lt;video style="margin:auto;" width="382" height="814" controls&gt;
    &lt;source src="/posts/2025-06-02-can-you-really-interrupt-an-llm/claude.mp4" type="video/mp4"&gt;
  &lt;/video&gt;
&lt;/div&gt;

&lt;p&gt;Is this an issue with Claude? Have a look at Gemini:&lt;/p&gt;
&lt;div style="display: flex;"&gt;
  &lt;video style="margin:auto;" width="384" height="796" controls&gt;
    &lt;source src="/posts/2025-06-02-can-you-really-interrupt-an-llm/gemini.mp4" type="video/mp4"&gt;
  &lt;/video&gt;
&lt;/div&gt;

&lt;p&gt;or even at the venerable GPT-4o, the most mature Voice AI out there:&lt;/p&gt;
&lt;div style="display: flex; align-content: center;"&gt;
  &lt;video style="margin:auto;" width="382" height="814" controls&gt;
    &lt;source src="/posts/2025-06-02-can-you-really-interrupt-an-llm/gpt-4o.mp4" type="video/mp4"&gt;
  &lt;/video&gt;
&lt;/div&gt;

&lt;p&gt;What's going on?&lt;/p&gt;
&lt;p&gt;This simple exercise highlights two core issues that are often overlooked when developing Voice AI agents. Let's see them.&lt;/p&gt;
&lt;h2&gt;Problem #1: LLMs don't perceive time&lt;/h2&gt;
&lt;p&gt;As algorithms trained to predict the most likely next word, LLMs don't have any concept of time. When dealing with text, this issue is not visible; however as soon as we cross over the domain of voice, their lack of understanding of time becomes a much bigger problem. LLMs still perceive the conversation as a series of tokens, with no concept of speed, pauses, or anything of that sort. They are often trained to control cadence, tone, to imitate pauses and adjust their talking speed, but they don't &lt;em&gt;perceive&lt;/em&gt; these features as we do: they are just additional properties of the output tokens.&lt;/p&gt;
&lt;p&gt;This means that an LLM will have a very hard time understanding requests that involve altering the timing of the response unless there is additional, external tooling to help them. "Please wait three second before replying", for example, is a meaningless query to an LLM that doesn't have a timer tool of some sort.&lt;/p&gt;
&lt;p&gt;For example, here is what GPT-4o (the LLM that handles time best) can do when asked to wait for a few seconds:&lt;/p&gt;
&lt;div style="display: flex; align-content: center;"&gt;
  &lt;video style="margin:auto;" width="382" height="814" controls&gt;
    &lt;source src="/posts/2025-06-02-can-you-really-interrupt-an-llm/wait-before-replying.mp4" type="video/mp4"&gt;
  &lt;/video&gt;
&lt;/div&gt;

&lt;h2&gt;Problem #2: Interruptions are not a native capability&lt;/h2&gt;
&lt;p&gt;Most Voice AIs out there feature the possibility to interrupt them. However, not having any innate concept of time, the ability to interrupt the model has to be implemented on the application end: and this is where it usually goes wrong.&lt;/p&gt;
&lt;p&gt;Voice LLMs are very fast: they generate the response in a fraction of the time needed to play it out. When you prompt an LLM, the model will start generate audio tokens and streaming them, but by the time the first one reaches the user, in most cases the majority of the response (if not the entirety of it) has already been generated and is queued in the audio buffer, waiting to be played.&lt;/p&gt;
&lt;p&gt;When a user interrupts the LLM, the app normally stops the playback as soon as possible and &lt;strong&gt;empties the audio buffer&lt;/strong&gt;, regardless of its content. &lt;/p&gt;
&lt;p&gt;However, unless the app notifies the LLM of this action, &lt;strong&gt;the LLM has no way to know that only part of the response was played to the user.&lt;/strong&gt; This is why most models believe they finished their countdown when in practice they were interrupted earlier.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-06-02-can-you-really-interrupt-an-llm/naive-interruption-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;h2&gt;Can it be solved?&lt;/h2&gt;
&lt;p&gt;If you paid close attention you may have noticed that GPT-4o, while it still stops at the wrong number, it does not believe it completed the countdown, but it understood that the counting was interrupted at some point before the end.&lt;/p&gt;
&lt;p&gt;This is possible because OpenAI's Realtime API provides the possibility to tell the model at which point it was interrupted. In the Realtime API documentation you can find this feature implemented with the event &lt;code&gt;conversation.item.truncate&lt;/code&gt; (see the &lt;a href="https://platform.openai.com/docs/api-reference/realtime-client-events/conversation/item/truncate" target="_blank" rel="noopener noreferrer"&gt;docs&lt;/a&gt;):&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;{
    &amp;quot;event_id&amp;quot;: &amp;quot;event_678&amp;quot;,
    &amp;quot;type&amp;quot;: &amp;quot;conversation.item.truncate&amp;quot;,
    &amp;quot;item_id&amp;quot;: &amp;quot;msg_002&amp;quot;,
    &amp;quot;content_index&amp;quot;: 0,
    &amp;quot;audio_end_ms&amp;quot;: 1500
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this event, the &lt;code&gt;audio_end_ms&lt;/code&gt; is what signals the model that the audio was interrupted at a certain time, before its natural end. This event in turn also trims the transcript to make the LLM know what the user heard and was was never played out. Precision however is not trivial to accomplish, because it's very easy for the application to register the interruption later than when it actually occurred and, like in the case of the ChatGPT app, convince the LLM that the interruption happened in the wrong point.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-06-02-can-you-really-interrupt-an-llm/gpt-4o-interruption-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;In the case of Gemini, there is a &lt;a href="https://ai.google.dev/gemini-api/docs/live#interruptions" target="_blank" rel="noopener noreferrer"&gt;"Handling Interruptions"&lt;/a&gt; section in its Live API documentation. However the feature seems incomplete, as they state:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Users can interrupt the model's output at any time. When Voice activity detection (VAD) detects an interruption, the ongoing generation is canceled and discarded. &lt;strong&gt;Only the information already sent to the client is retained in the session history&lt;/strong&gt;. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As we've seen, this is not sufficient to handle interruptions correctly. It's likely that this issue is not currently fixable.&lt;/p&gt;
&lt;p&gt;In the case of Claude we don't know yet if that's an inherent limitation or a bug in the app, because at the time of writing there is no Live/Realtime API available for Claude.&lt;/p&gt;
&lt;h2&gt;Wrapping up&lt;/h2&gt;
&lt;p&gt;Voice Mode for LLMs is a huge step forward for voice AI, but it's not a silver bullet. LLMs are first and foremost text prediction algorithms, and even when adapted to work with voice, some of their limitations persists. In order to have complete control, building a &lt;a href="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/"&gt;full pipeline for voice&lt;/a&gt; may still be your best bet if you have the infrastructure to achieve a low enough latency; otherwise, always make sure to test the behavior of your LLMs in these corner cases and stick to more well-tested models (in this case, OpenAI's) for better handling of time.&lt;/p&gt;
&lt;p class="fleuron"&gt;&lt;a href="/posts/2024-05-06-teranoptia/"&gt;SDH&lt;/a&gt;&lt;/p&gt;</description></item><item><title>A simple vibecoding exercise</title><link>https://www.zansara.dev/posts/2025-05-21-vibecoding/</link><pubDate>Wed, 21 May 2025 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/posts/2025-05-21-vibecoding/</guid><description>&lt;p&gt;Sometimes, after an entire day of coding, the last thing you want to do is to code some more. It would be so great if I could just sit down and enjoy some Youtube videos...&lt;/p&gt;
&lt;p&gt;Being abroad, most of the videos I watch are in a foreign language, and it helps immensely to have subtitles when I'm not in the mood for hard focus. However, Youtube subtitles are often terrible or missing entirely.&lt;/p&gt;
&lt;p&gt;Can the magic of modern Generative AI fix this problem?&lt;/p&gt;
&lt;p&gt;We've all heard of &lt;a href="https://x.com/karpathy/status/1886192184808149383" target="_blank" rel="noopener noreferrer"&gt;vibecoding&lt;/a&gt;: sitting in front of your IDE, telling an AI what you want the code to do and letting it loose to create &lt;em&gt;something&lt;/em&gt; that achieves that goal. In this case, the goal is rather simple: given a video file, generate subtitles for it using &lt;a href="https://deepgram.com/" target="_blank" rel="noopener noreferrer"&gt;Deepgram&lt;/a&gt;'s SDK (since it has a &lt;a href="https://deepgram.com/pricing" target="_blank" rel="noopener noreferrer"&gt;generous free tier&lt;/a&gt;). It seems such a simple task that even an LLM should be able to reach it with minimal or no assistance. Right?&lt;/p&gt;
&lt;h2&gt;The first shot: OpenAI&lt;/h2&gt;
&lt;p&gt;For this simple experiment I decided not to use a dedicated IDE or VSCode plugin, but to stick to text based tools. After all, I expected this task to be sorted with a single Python script made by OpenAI's famed &lt;a href="https://openai.com/index/introducing-o3-and-o4-mini/" target="_blank" rel="noopener noreferrer"&gt;&lt;code&gt;o4-mini-high&lt;/code&gt;&lt;/a&gt;, advertized as "Great at coding and visual reasoning".&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-05-21-vibecoding/openai-model-selector.png" /&gt;&lt;/p&gt;
&lt;p&gt;The prompt was very simple:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Write me a Python script that, given a video file, returns me an &lt;a href="https://en.wikipedia.org/wiki/SubRip" target="_blank" rel="noopener noreferrer"&gt;.srt&lt;/a&gt; subtitle file using Deepgram's API.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As expected, the model thought about it, did some web searches, and then cooked up a script that used &lt;code&gt;deepgram-sdk&lt;/code&gt; and &lt;code&gt;deepgram-captions&lt;/code&gt;. Looked good, but as soon as I tried to run it, issues came up. Deepgram's SDK complained about wrong formats, wrong SDK version, HTTP errors... Copy-pasting the errors back to &lt;code&gt;o4-mini-high&lt;/code&gt; was vain: the model seems to understand that the Deepgram API had a major upgrade since the model was trained, but fails to use the new version. After four or five attempts (including one full restart of the chat), I realized this was going nowhere and I looked for another option.&lt;/p&gt;
&lt;h2&gt;The backup option: Claude Code&lt;/h2&gt;
&lt;p&gt;I've heard many times that the best LLMs for vibecoding belong to the &lt;a href="https://www.anthropic.com/claude" target="_blank" rel="noopener noreferrer"&gt;Claude&lt;/a&gt; family. On top of that, there's a cool TUI utility called &lt;a href="https://www.anthropic.com/claude-code" target="_blank" rel="noopener noreferrer"&gt;Claude Code&lt;/a&gt; that allows you to vibecode from the terminal, no IDE required. It uses &lt;a href="https://www.anthropic.com/claude/sonnet" target="_blank" rel="noopener noreferrer"&gt;Claude 3.7 Sonnet&lt;/a&gt; under the hood, so the expectations are high.&lt;/p&gt;
&lt;p&gt;Time to give it a try.&lt;/p&gt;
&lt;p&gt;Installing the utility is matter of a single command (&lt;code&gt;npm install -g @anthropic-ai/claude-code&lt;/code&gt;) and a few emails to authenticate the utility into my Anthropic account. Once done we're ready to go.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2025-05-21-vibecoding/claude-code-intro.gif" /&gt;&lt;/p&gt;
&lt;p&gt;The prompt is the same:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Write me a Python script that, given a video file, returns me an .srt subtitle file using Deepgram's API.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Sure enough, Claude's first attempt also fails for the same reason as o4 did: their knowledge is outdated, and they both use the Deepgram's API in a way that's not compabible with its new v3 API. However, after a few attempts, Claude actually produces a script that &lt;em&gt;mostly&lt;/em&gt; works.&lt;/p&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://gist.github.com/ZanSara/4bab5db89376d595128e0688804d694c" target="_blank" rel="noopener noreferrer"&gt;Here&lt;/a&gt; is the output (I pasted the &lt;code&gt;README&lt;/code&gt; and the &lt;code&gt;requirements.txt&lt;/code&gt; at the top of the file for simplicity). I only needed to replace &lt;a href="https://developers.deepgram.com/docs/models-languages-overview#nova-2" target="_blank" rel="noopener noreferrer"&gt;&lt;code&gt;nova-2&lt;/code&gt;&lt;/a&gt; with &lt;a href="https://developers.deepgram.com/docs/models-languages-overview#nova-3" target="_blank" rel="noopener noreferrer"&gt;&lt;code&gt;nova-3&lt;/code&gt;&lt;/a&gt; to get the best possible transcription for Portuguese (other languages may get better transcription with &lt;code&gt;nova-2&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;To summarize:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Is it perfect? No.&lt;/strong&gt; I can easily spot a lot of improvements to the code just by looking at it. It's quite verbose, for example.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Was it cheap? No.&lt;/strong&gt; This script costed me a few dollars worth of tokens and about half a hour of trial and errors, about the hourly rate of a US software engineer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Is it enough for my purposes? Absolutely.&lt;/strong&gt; Now I am finally able to enjoy my videos with good quality subtitles without too much hassle.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Could somebody that can't program do this?&lt;/strong&gt; I'm not so sure. Given how simple this task was, I was a bit disappointed by how long it took and I am rather skeptical about the ability of today's LLMs to handle more complex requests without oversight - at least with the tools I used.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, looking at the big picture, the trend is clear. Three years ago, LLMs could just about write coherent sentences. Today, they can write decent helper scripts. Soon the may be able to implement your side projects from start to finish. &lt;/p&gt;
&lt;p&gt;Will it feel like a blessing or a curse? We'll soon find out.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Edit 22/05/2025: Claude 4 has been released the day after I published this post, so here is a video of myself reimplementing this same script with the new model ‚ú®&lt;/em&gt;&lt;/p&gt;
&lt;iframe src="https://drive.google.com/file/d/1cTo-VD8sFYYau900zIwFSCgkxLDt9iWO/preview" width="800" height="500" allow="autoplay"&gt;&lt;/iframe&gt;

&lt;p class="fleuron"&gt;&lt;a href="/posts/2024-05-06-teranoptia/"&gt;{z&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Using Llama Models in the EU</title><link>https://www.zansara.dev/posts/2025-05-16-llama-eu-ban/</link><pubDate>Fri, 16 May 2025 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/posts/2025-05-16-llama-eu-ban/</guid><description>&lt;p&gt;&lt;a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/" target="_blank" rel="noopener noreferrer"&gt;The Llama 4 family&lt;/a&gt; has been released over a month ago and I finally found some time to explore it. Or so I wished to do, until I realized one crucial issue with these models:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;They are banned in the EU.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Apparently Meta can‚Äôt be bothered to comply with EU regulations on AI, and therefore opted for a wide ban that should prevent such laws to apply to them. Of course, while this limitation is technically valid for each and every person and company domiciled in the EU, the problem arises primarily for companies that want to use Llama 4 to offer services and for researchers planning to work with these models, be it for evaluation, fine-tuning, distillation or other derivative work. Always keep in mind that I‚Äôm not a lawyer, so nothing of what I‚Äôm writing here constitutes as legal advice.&lt;/p&gt;
&lt;h2&gt;The terms&lt;/h2&gt;
&lt;p&gt;The interesting part of this ban can be found by reading the &lt;a href="https://github.com/meta-llama/llama-models/blob/main/models/llama4/USE_POLICY.md" target="_blank" rel="noopener noreferrer"&gt;terms&lt;/a&gt; of the Acceptable Usage Policy (AUP):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;With respect to any &lt;strong&gt;multimodal models&lt;/strong&gt; included in Llama 4, the rights granted under Section 1(a) of the Llama 4 Community License Agreement are not being granted to you if you are an individual domiciled in, or a company with a principal place of business in, the European Union. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As you can see, the restriction applies strictly to multimodal LLMs. Llama4 models are all multimodal, and that‚Äôs why the entire family of models is not accessible from the EU. However, if anyone releases a derivative model that is not multimodal, in theory the ban would not apply. I‚Äôm yet to see any such derivative model: if you know of any, let me know!&lt;/p&gt;
&lt;p&gt;Interestingly, the terms also state that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This restriction does not apply to end users of a product or service that incorporates any such multimodal models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So if you‚Äôre a company outside of the EU and provide services based on Llama4 to EU customers, you‚Äôre probably off the hook. Such interpretation seems to be confirmed by &lt;a href="https://www.llama.com/faq/" target="_blank" rel="noopener noreferrer"&gt;Meta‚Äôs FAQ&lt;/a&gt; about Llama models, which state:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Can a non-EU based company develop a product or service using the Llama multimodal models and distribute such product or service within the EU?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Yes, if you are a company with a principal place of business outside of the EU, you may distribute products or services that contain the Llama multimodal models in accordance with your standard global distribution business practices [...]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://www.llama.com/faq/" target="_blank" rel="noopener noreferrer"&gt;Meta‚Äôs FAQ&lt;/a&gt; are actually quite throughout, so if you have any doubt about your specific case you should head there and read more.&lt;/p&gt;
&lt;h2&gt;What about other Llamas?&lt;/h2&gt;
&lt;p&gt;This wide EU ban is not new: it was introduced with &lt;a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/" target="_blank" rel="noopener noreferrer"&gt;Llama 3.2 Vision&lt;/a&gt;, the first multimodal model released by Meta. The clause does not exist for any model older than Llama 3.2.&lt;/p&gt;
&lt;p&gt;To summarize, here is a list of which models can be used in the EU:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Llama 4: all banned because they're all multimodal (&lt;a href="https://github.com/meta-llama/llama-models/blob/main/models/llama4/USE_POLICY.md" target="_blank" rel="noopener noreferrer"&gt;terms&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Llama 3.3: allowed because it's not multimodal (&lt;a href="https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/USE_POLICY.md" target="_blank" rel="noopener noreferrer"&gt;terms&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Llama 3.2: text only models are allowed, vision models are not allowed (&lt;a href="https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/USE_POLICY.md" target="_blank" rel="noopener noreferrer"&gt;terms&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Llama 3.1 and earlier: allowed because there's no such clause (&lt;a href="https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/USE_POLICY.md" target="_blank" rel="noopener noreferrer"&gt;terms&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So for now this is the state of Llama licenses. My take is that with the implementation and rollout of the EU AI Act in 2025 and 2026, Meta will eventually adapt to make sure that the models are compliant with the way the Act is enforced in practice and relax, if not lift, the ban on newer models.&lt;/p&gt;
&lt;p&gt;Also, Llama4 has not been shining in the &lt;a href="https://lmarena.ai/?leaderboard" target="_blank" rel="noopener noreferrer"&gt;benchmarks&lt;/a&gt; (scroll down, I promise you it‚Äôs there)... we Europeans may not be missing much.&lt;/p&gt;
&lt;p class="fleuron"&gt;&lt;a href="/posts/2024-05-06-teranoptia/"&gt;AZ*&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Beyond the hype of reasoning models: debunking three common misunderstandings</title><link>https://www.zansara.dev/posts/2025-05-12-beyond-hype-reasoning-models/</link><pubDate>Mon, 12 May 2025 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/posts/2025-05-12-beyond-hype-reasoning-models/</guid><description>&lt;p&gt;With the release of OpenAI‚Äôs o1 and similar models such as DeepSeek R1, Gemini 2.0 Flash Thinking, Phi 4 Reasoning and more, a new type of LLMs entered the scene: the so-called reasoning models. With their unbelievable scores in the toughest benchmarks for machine intelligence, reasoning models immediately got the attention of most AI enthusiasts, sparking speculations about their capabilities and what those could mean for the industry.&lt;/p&gt;
&lt;p&gt;However, as often in the field of Generative AI, the hype makes it very difficult to understand at a glance what these models can really do. But before we jump into the details let‚Äôs clarify what we‚Äôre talking about.&lt;/p&gt;
&lt;h2&gt;What is a reasoning model?&lt;/h2&gt;
&lt;p&gt;Reasoning models are LLMs that are able to ‚Äúthink‚Äù. Instead of generating a reply immediately after the user‚Äôs prompt, like every other LLM, they first generate a series of ‚Äúreasoning tokens‚Äù, which is nothing more than the model thinking out loud, breaking down a complex problem into smaller steps, checking all its assumptions, asking itself whether it made any mistakes, double-checking its results, and so on. Once the model is satisfied by its conclusions, it starts generating actual response tokens that summarize the conclusions reached during the reasoning phase and presents those tokens to the user.&lt;/p&gt;
&lt;p&gt;In the case of some models such as OpenAI‚Äôs, the reasoning tokens are hidden from the user. In otner models, such as most open source ones, the reasoning output can be returned to the user as well. However, this trace is not optimized to be read by people, so it often looks odd and contrived even when it reaches the correct conclusions.&lt;/p&gt;
&lt;p&gt;Now that we understand better what a reasoning model is, let‚Äôs discuss a few common misunderstandings related to them.&lt;/p&gt;
&lt;h2&gt;Are reasoning models AGI?&lt;/h2&gt;
&lt;p&gt;AGI stands for Artificial General Intelligence, and it‚Äôs one of the most ill-defined terms in Generative AI. Several people have tried to offer a more precise definition of this term, out of which my favourite is the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;AGI is an AI that is better than any human at any economically valuable task.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Under this light it‚Äôs clear that no current LLM, not even the most advanced reasoning model, it yet at the level where it could replace any human at any task. They can surely offer very valuable help with their vast knowledge and their growing ability to reason, but they‚Äôre not yet at the point where they can take onto any job without further specialization and complex tooling around them.&lt;/p&gt;
&lt;h2&gt;Are reasoning models AI agents?&lt;/h2&gt;
&lt;p&gt;An AI agent is usually defined as any application that can use tools to achieve complex goals. Considering that reasoning models are usually able to use tools, it‚Äôs natural to think that they themselves may be considered AI agents.&lt;/p&gt;
&lt;p&gt;In practice, however, reasoning models on their own hardly qualify as agents. Many powerful agents systems do have an LLM at their core: they use it to understand the user‚Äôs request and plan the actions to take to achieve the goal they‚Äôre set to. Reasoning models are a perfect fit as the minds of agents like that, due to their advanced capabilities to break down problems into smaller, manageable parts and self-correct their strategy on the fly if something goes wrong. Taken in isolation, though, reasoning models can‚Äôt be called AI agents.&lt;/p&gt;
&lt;h2&gt;Are reasoning models glorified CoT prompts?&lt;/h2&gt;
&lt;p&gt;If you have worked with AI agents and other LLM systems designed to solve problems, you‚Äôve surely come across Chain of Thought prompting. In short, this technique involves adding in the system prompt of your LLM instructions to ‚Äúthink step by step‚Äù before replying. This makes the LLM think out loud before reaching a conclusion and, even in regular non-reasoning LLMs, improves significantly their problem solving skills.&lt;/p&gt;
&lt;p&gt;At a first glance, the output of a reasoning model may look precisely like the output of a CoT prompt, so some experts may think that their reasoning capabilities are the same. This is a mistake. Reasoning models are much more powerful than regular LLMs, even when these are equipped with a CoT prompt: this is because reasoning models pass through one additional step during their training where they learn to refine their ‚Äúthinking step by step‚Äù skills through supervised learning on prompts with verifiable output, such as mathematical problems. Reasoning models are not zero-shot resoners like regular LLMs: they‚Äôre fine-tuned for it.&lt;/p&gt;
&lt;h2&gt;Wrapping up&lt;/h2&gt;
&lt;p&gt;Reasoning models may not be the super-human intelligence some of us are waiting for, but they surely are a significant step forward toward LLMs with very strong reasoning abilities.&lt;/p&gt;
&lt;p&gt;If you want to learn more about what reasoning models can do, how they reason, when to use them and more, make sure to attend my talk &lt;a href="https://odsc.com/speakers/llms-that-think-demystifying-reasoning-models/" target="_blank" rel="noopener noreferrer"&gt;LLMs that Think: Demystifying Reasoning Models&lt;/a&gt; at this year‚Äôs virtual edition of &lt;a href="https://odsc.com/boston/" target="_blank" rel="noopener noreferrer"&gt;ODSC East&lt;/a&gt;. See you there!&lt;/p&gt;
&lt;p class="fleuron"&gt;&lt;a href="/posts/2024-05-06-teranoptia/"&gt;ifo&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Building Reliable Voice Bots with Open Source Tools - Part 2</title><link>https://www.zansara.dev/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/</link><pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/</guid><description>&lt;p&gt;&lt;em&gt;This is part two of the write-up of my talk at &lt;a href="/talks/2024-09-06-odsc-europe-voice-agents/"&gt;ODSC Europe 2024&lt;/a&gt; and &lt;a href="/talks/2024-10-29-odsc-west-voice-agents/"&gt;ODSC West 2024&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;In the last few years, the world of voice agents saw dramatic leaps forward in the state of the art of all its most basic components. Thanks mostly to OpenAI, bots are now able to understand human speech almost like a human would, they're able to speak back with completely naturally sounding voices, and are able to hold a free conversation that feels extremely natural.&lt;/p&gt;
&lt;p&gt;But building reliable and effective voice bots is far from a solved problem. These improved capabilities are raising the bar, and even users accustomed to the simpler capabilities of old bots now expect a whole new level of quality when it comes to interacting with them.&lt;/p&gt;
&lt;p&gt;In &lt;a href="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/"&gt;Part 1&lt;/a&gt; we've seen mostly &lt;strong&gt;the challenges&lt;/strong&gt; related to building such bot: we discussed the basic structure of most voice bots today, their shortcomings and the main issues that you may face on your journey to improve the quality of the conversation.&lt;/p&gt;
&lt;p&gt;In this post instead we will focus on &lt;strong&gt;the solutions&lt;/strong&gt; that are available today and we are going to build our own voice bot using &lt;a href="https://www.pipecat.ai" target="_blank" rel="noopener noreferrer"&gt;Pipecat&lt;/a&gt;, a recently released open-source library that makes building these bots a lot simpler.&lt;/p&gt;
&lt;h2&gt;Outline&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Start from &lt;a href="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/"&gt;Part 1&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#a-modern-voice-bot"&gt;A modern voice bot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#voice-activity-detection-vad"&gt;Voice Activity Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tools"&gt;Tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#llm-based-intent-detection"&gt;LLM-based intent detection&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#intent-detection"&gt;Intent detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#prompt-building"&gt;Prompt building&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reply-generation"&gt;Reply generation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#what-about-latency"&gt;What about latency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#the-code"&gt;The code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#looking-forward"&gt;Looking forward&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;A modern voice bot&lt;/h2&gt;
&lt;p&gt;At this point we have a &lt;a href="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/"&gt;comprehensive view&lt;/a&gt; of the issues that we need to solve to create a reliable, usable and natural-sounding voice agents. How can we actually build one?&lt;/p&gt;
&lt;p&gt;First of all, let's take a look at the structure we defined earlier and see how we can improve on it.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/structure-of-a-voice-bot-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;h3&gt;Voice Activity Detection (VAD)&lt;/h3&gt;
&lt;p&gt;One of the simplest improvements to this architecture is the addition of a robust Voice Activity Detection (VAD) model. VAD gives the bot the ability to hear interruptions from the user and react to them accordingly, helping to break the classic, rigid turn-based interactions of old-style bots.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/structure-of-a-voice-bot-vad-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;However, on its own VAD models are not enough. To make a bot truly interruptible we also need the rest of the pipeline to be aware of the possibility of an interruption and be ready to handle it: speech-to-text models need to start transcribing and the text-to-speech component needs to stop speaking as soon as the VAD picks up speech. &lt;/p&gt;
&lt;p&gt;The logic engine also needs to handle a half-spoken reply in a graceful way: it can't just assume that the whole reply was spoken out, and neither it can drop the whole reply as it never started. Most LLMs can handle this scenario by altering the last message in their conversation history, but implementing this workflow in practice is often not straightorward, because you need to keep track of how much of the reply was heard by the user, and when exactly this interruption happened.&lt;/p&gt;
&lt;p&gt;The quality of your VAD model matters a lot, as well as tuning its parameters appropriately. You don't want the bot to interrupt itself at every ambient sound it detects, but you also want the interruption to happen promptly, with a few hundreds of milliseconds of delay. Some of the best and most used models out there are &lt;a href="https://github.com/snakers4/silero-vad" target="_blank" rel="noopener noreferrer"&gt;Silero&lt;/a&gt;'s VAD models, or alternatively &lt;a href="https://picovoice.ai/" target="_blank" rel="noopener noreferrer"&gt;Picovoice&lt;/a&gt;'s &lt;a href="https://picovoice.ai/platform/cobra/" target="_blank" rel="noopener noreferrer"&gt;Cobra&lt;/a&gt; models.&lt;/p&gt;
&lt;h3&gt;Tools&lt;/h3&gt;
&lt;p&gt;Tools are often a major component of you bot's functionality. Modern and effective voice bots today are often able to take basic actions such as looking up data in a database or calling simple functions.&lt;/p&gt;
&lt;p&gt;Function calling is a feature of most of today's LLMs, so it's often a low-hanging fruit in terms of improvements to your bot. Simple actions like looking up the current time, or searching a knowledge base before replying (a technique called &lt;a href="/posts/2024-06-10-the-agent-compass/#agentic-rag"&gt;Agentic RAG&lt;/a&gt;), may make a huge difference in terms of the quality of its responses.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/structure-of-a-voice-bot-tools-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;h3&gt;LLM-based intent detection&lt;/h3&gt;
&lt;p&gt;Despite the &lt;a href="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/#logic-engine"&gt;distinction we made earlier&lt;/a&gt; between tree-based, intent-based and LLM-based bots, often the logic of voice bots is implemented in a blend of more than one style. Intent-based bots may contain small decision trees, as well as LLM prompts. Often these approaches deliver the best results by taking the best of each to compensate for the weaknesses of the others.&lt;/p&gt;
&lt;p&gt;One of the most effective approaches is to use intent detection to help control the flow of an LLM conversation. Let's see how.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/structure-of-a-voice-bot-intent-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;Suppose we're building a general purpose customer support bot.&lt;/p&gt;
&lt;p&gt;A bot like this needs to be able to handle a huge variety of requests: helping the user renew subscriptions, buy or return items, update them on the state of a shipping, telling the opening hours of the certified repair shop closer to their home, explaining the advantages of a promotion, and more. &lt;/p&gt;
&lt;p&gt;If we decide to implement this chatbot based on intents, we risk that in many cases users won't be able to find out how to achieve their goal, because many intent will look similar and there are many corner case requests that the original developers may not have foreseen.&lt;/p&gt;
&lt;p&gt;However, if we decide to implement this chatbot with an LLM, it becomes really hard to check its replies and make sure that the bot is not lying, because the amount of instructions its system prompt will end up containing is huge. The bot may also perform actions that it is not supposed to, like letting users return an item they have no warranty on anymore.&lt;/p&gt;
&lt;p&gt;There is an intermediate solution: &lt;strong&gt;first try to detect intent, then leverage the LLM&lt;/strong&gt;.&lt;/p&gt;
&lt;h4&gt;Intent detection&lt;/h4&gt;
&lt;p&gt;Step one is detecting the intention of the user. This step can be done with an LLM by sending it a message such as this:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Given the following conversation, select the intent
of the user:
1: Return an item
2: Help to use the product
3: Apply for a subscription
4: Get information about official repair centers
5: Find the nearest retail center near them
6: Learn about current promotion campaigns

Conversation:
assistant: Hello! How can I help you today?
user: Hello, can you tell me if there&amp;#39;s a repair shop for your product ABC open right now in Queens?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can see that at this stage we don't need to micromanage the model and we can stick to macro-categories safely. No need to specify "Find the opening hours of certified repair shops in New York", bur rather "Find information on certified repair shops" in general will suffice.&lt;/p&gt;
&lt;p&gt;This first steps narrows down drastically the scope of the conversation and, as a consequence, the amount of instructions that the LLM needs to handle to carry on the conversation effectively.&lt;/p&gt;
&lt;p&gt;So the next step is to retrieve these instructions&lt;/p&gt;
&lt;h4&gt;Prompt update&lt;/h4&gt;
&lt;p&gt;Once we know what the user's intention is, it's time to build the real prompt that will give us a reply for the user.&lt;/p&gt;
&lt;p&gt;With the general intent identified, we can equip the LLM strictly with the tools and information that it needs to proceed. If the user is asking about repair shops in their area, we can provide the LLM with a tool to search repair shops by zip code, a tool that would be useless if the user was asking about a shipment or a promotional campaign. Same for the background information: we don't need to tell the LLM that "you're a customer support bot", but we can narrow down its personality and background knowledge to make it focus a lot more on the task at hand, which is to help the user locating a suitable repair shop. And so on.&lt;/p&gt;
&lt;p&gt;This can be done by mapping each expected intent to a specific system prompt, pre-compiled to match the intent. At the prompt building stage we simply pick from our library of prompts and &lt;strong&gt;replace the system prompt&lt;/strong&gt; with the one that we just selected.&lt;/p&gt;
&lt;p&gt;For example, in our case the LLM selected intent n.4, "Get information about official repair centers". This intent may correspond to a prompt like the following:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;You‚Äôre a helpful assistant helping a user finding the best
repair center for them.

You can use the tool `find_repair_center` to get a list of
centers that match your query. Before calling the tool,
make sure to ask them for their zip code. If they asked about
a specific opening time, you can also use the `get_datetime`
tool to translate relative time (such as &amp;quot;now&amp;quot; or &amp;quot;tomorrow&amp;quot;)
into a specific date and time (like 2024-01-24 10:24:32)
Don&amp;#39;t forget about timezones. ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4&gt;Reply generation&lt;/h4&gt;
&lt;p&gt;With the new, narrower system prompt in place at the head of the conversation, we can finally prompt the LLM again to generate a reply for the user. The LLM, following the instructions of the updated prompt, has an easier time following its instructions (because they're simpler and more focused) and generated better quality answers for both the users and the developers.&lt;/p&gt;
&lt;p&gt;With a prompt like the above, the reply from the LLM is most likely going to be about the zipcode, something that normally an LLM would not attempt to ask for.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;assistant: Hello! How can I help you today?
user: Hello, can you tell me if there&amp;#39;s a repair shop for your product ABC open right now in Queens?

assistant: Sure, let me look it up! Can you please tell me your zipcode?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;What about latency?&lt;/h3&gt;
&lt;p&gt;With all these additional back-and-forth with the LLM, it's easy to find ourselves into a situation where latency gets out of hand. With only half a second of time to spare, making sure the system works as efficiently as possible is crucial.&lt;/p&gt;
&lt;p&gt;With today's models there are a few technical and non-technical ways to manage the latency of your bots and keep it under control.&lt;/p&gt;
&lt;h4&gt;Model colocation&lt;/h4&gt;
&lt;p&gt;Colocating models means that, instead of hosting each model on a different server or SaaS provider, you host all of them on the same machine or server rack, very close together.&lt;/p&gt;
&lt;p&gt;Colocation can be helpful to reduce or remove entirely the overhead of network requests, which often is the largest source of latency in your bots. Colocation is very powerful for bringing latency down, however it's not always feasible if you're using proprietary models that don't allow self-hosting.&lt;/p&gt;
&lt;p&gt;Keep in mind also that colocation can backfire if your hardware is not suitable for the needs of the models you're running. If you don't have GPUs available, or they don't fit all the models you need to colocate, your latency might increase dramatically.&lt;/p&gt;
&lt;h4&gt;I/O streaming&lt;/h4&gt;
&lt;p&gt;Modern LLMs and STT/TTS models are able to stream either their input or their output. The time it takes these models to generate the start of their output is often much faster than the time they take to generate the entire reply, so streaming the output of one into the input of the next will bring down the latency of the whole system by orders of magnitude.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Endpointing&lt;/strong&gt;, for example, is the technical term for the ability of a speech to text model to detect the end of a sentence and send it over to an LLM while it listens for the rest of the user's message. LLMs, while unable to take token-by-token inputs, can stream out their replies in this way. Text to speech then can detect dots and commas in the output stream to aggregate the tokens into sentences or phrases and start reading them out long before the last token is produced by the LLM.&lt;/p&gt;
&lt;p&gt;This is exactly what frameworks like Pipecat enable for all their models, and it's usually possible for all moderns LLMs.&lt;/p&gt;
&lt;h4&gt;Declaring the latency&lt;/h4&gt;
&lt;p&gt;If all technical solutions fails, one unconventional approach is to make the bot declare its own latency at the very start of the conversation. While it might sound silly, if a bot opens the chat saying &lt;code&gt;I might be a bit slow, so be patient with me&lt;/code&gt; users are automatically more keen to wait longer for the bot's response instead of pinging it continuously. While this does not make for the best user experience, being honest about your bot's capabilities is always appreciated.&lt;/p&gt;
&lt;p&gt;This technique, however, is not a band-aid for any sort of delay. Users won't manage to talk to a bot if each reply takes more than one or two seconds to come back to them, regardless of how patient they might be.&lt;/p&gt;
&lt;h4&gt;Buying time&lt;/h4&gt;
&lt;p&gt;Last but not least, occasionally the bot might have a spike in latency due to the usage of a slow tool. When your bot knows that its reply is going to take longer than usual, it's best, again, to warn the user by telling them what's going on. Having the bot say something like &lt;code&gt;Ok, let me look it up, it will take a few seconds&lt;/code&gt; is a huge user experience improvement you should not underestimate.&lt;/p&gt;
&lt;h2&gt;The code&lt;/h2&gt;
&lt;p&gt;Now that we've seen all the techniques that can make your bot effective, reliable and fast, it's time to actually implement one!&lt;/p&gt;
&lt;p&gt;One of the best frameworks out there to build open-source voice bots right now is &lt;a href="http://www.pipecat.ai" target="_blank" rel="noopener noreferrer"&gt;Pipecat&lt;/a&gt;, a small library maintained by &lt;a href="https://www.daily.co/" target="_blank" rel="noopener noreferrer"&gt;Daily.co&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://colab.research.google.com/drive/1CUX7JRYMU1MEJBZ6lWMg5EThPew19Zjs?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;Here&lt;/a&gt; you can find a commented Colab notebook to learn how &lt;a href="https://www.pipecat.ai" target="_blank" rel="noopener noreferrer"&gt;Pipecat&lt;/a&gt; can help you build a very basic voice bots, how to implement the intent-detection system we've outlined above, and try such a bot yourself. Watch out: you'll need a few API keys, but if you don't have a specific one, often the Pipecat documentation can help you find a replacement component for any alternative model provider you may have access to.&lt;/p&gt;
&lt;p&gt;Have fun!&lt;/p&gt;
&lt;div style="display: flex; align-content: center;"&gt;
  &lt;video style="margin:auto;" width="100%" height="100%" controls&gt;
    &lt;source src="/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/notebook-presentation-clip.mp4" type="video/mp4"&gt;
  &lt;/video&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;The Pipecat bot in action (from &lt;a href="/talks/2024-10-29-odsc-west-voice-agents/"&gt;my talk&lt;/a&gt; at ODSC West 2024, presenting &lt;a href="https://colab.research.google.com/drive/1CUX7JRYMU1MEJBZ6lWMg5EThPew19Zjs?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;this same notebook&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;
&lt;p class="fleuron"&gt;&lt;a href="/posts/2024-05-06-teranoptia/"&gt;W≈ΩH&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Building Reliable Voice Bots with Open Source Tools - Part 1</title><link>https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/</link><pubDate>Fri, 20 Sep 2024 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/</guid><description>&lt;p&gt;&lt;em&gt;This is part one of the write-up of my talk at &lt;a href="/talks/2024-09-06-odsc-europe-voice-agents/"&gt;ODSC Europe 2024&lt;/a&gt; and &lt;a href="/talks/2024-10-29-odsc-west-voice-agents/"&gt;ODSC West 2024&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;In the last few years, the world of voice agents saw dramatic leaps forward in the state of the art of all its most basic components. Thanks mostly to OpenAI, bots are now able to understand human speech almost like a human would, they're able to speak back with completely naturally sounding voices, and are able to hold a free conversation that feels extremely natural.&lt;/p&gt;
&lt;p&gt;But building voice bots is far from a solved problem. These improved capabilities are raising the bar, and even users accustomed to the simpler capabilities of old bots now expect a whole new level of quality when it comes to interacting with them.&lt;/p&gt;
&lt;p&gt;In this post we're going to focus mostly on &lt;strong&gt;the challenges&lt;/strong&gt;: we'll discuss the basic structure of most voice bots today, their shortcomings and the main issues that you may face on your journey to improve the quality of the conversation.&lt;/p&gt;
&lt;p&gt;In &lt;a href="/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/"&gt;Part 2&lt;/a&gt; we are going to focus on &lt;strong&gt;the solutions&lt;/strong&gt; that are available today, and we are going to build our own voice bot using &lt;a href="https://www.pipecat.ai" target="_blank" rel="noopener noreferrer"&gt;Pipecat&lt;/a&gt;, a recently released open-source library that makes building these bots a lot simpler.&lt;/p&gt;
&lt;h2&gt;Outline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#what-is-a-voice-agent"&gt;What is a voice agent?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#speech-to-text-stt"&gt;Speech-to-text (STT)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#text-to-speech-tts"&gt;Text-to-speech (TTS)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#logic-engine"&gt;Logic engine&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#tree-based"&gt;Tree-based&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#intent-based"&gt;Intent-based&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#llm-based"&gt;LLM-based&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#audio-to-audio"&gt;Audio-to-audio models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#new-challenges"&gt;New challenges&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#real-speech-is-not-turn-based"&gt;Real speech is not turn-based&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#real-conversation-flows-are-not-predictable"&gt;Real conversation flows are not predictable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#llms-bring-their-own-problems"&gt;LLMs bring their own problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#the-context-window"&gt;The context window&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#working-in-real-time"&gt;Working in real time&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Continues in &lt;a href="/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/"&gt;Part 2&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;What is a voice agent?&lt;/h2&gt;
&lt;p&gt;As the name says, voice agents are programs that are able to carry on a task and/or take actions and decisions on behalf of a user ("software agents") by using voice as their primary mean of communication (as opposed to the much more common text chat format). Voice agents are inherently harder to build than their text based counterparts: computers operate primarily with text, and the art of making machines understand human voices has been an elusive problem for decades.&lt;/p&gt;
&lt;p&gt;Today, the basic architecture of a modern voice agent can be decomposed into three main fundamental building blocks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a &lt;strong&gt;speech-to-text (STT)&lt;/strong&gt; component, tasked to translate an audio stream into readable text,&lt;/li&gt;
&lt;li&gt;the agent's &lt;strong&gt;logic engine&lt;/strong&gt;, which works entirely with text only,&lt;/li&gt;
&lt;li&gt;a &lt;strong&gt;text-to-speech (TTS)&lt;/strong&gt; component, which converts the bot's text responses back into an audio stream of synthetic speech.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/structure-of-a-voice-bot-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;Let's see the details of each.&lt;/p&gt;
&lt;h3&gt;Speech to text (STT)&lt;/h3&gt;
&lt;p&gt;Speech-to-text software is able to convert the audio stream of a person saying something and produce a transcription of what the person said. Speech-to-text engines have a &lt;a href="https://en.wikipedia.org/wiki/Speech_recognition#History" target="_blank" rel="noopener noreferrer"&gt;long history&lt;/a&gt;, but their limitations have always been quite severe: they used to require fine-tuning on each individual speaker, have a rather high word error rate (WER) and they mainly worked strictly with native speakers of major languages, failing hard on foreign and uncommon accents and native speakers of less mainstream languages. These issues limited the adoption of this technology for anything else than niche software and research applications.&lt;/p&gt;
&lt;p&gt;With the &lt;a href="https://openai.com/index/whisper/" target="_blank" rel="noopener noreferrer"&gt;first release of OpenAI's Whisper models&lt;/a&gt; in late 2022, the state of the art improved dramatically. Whisper enabled transcription (and even direct translation) of speech from many languages with an impressively low WER, finally comparable to the performance of a human, all with relatively low resources, higher then realtime speed, and no finetuning required. Not only, but the model was free to use, as OpenAI &lt;a href="https://huggingface.co/openai" target="_blank" rel="noopener noreferrer"&gt;open-sourced it&lt;/a&gt; together with a &lt;a href="https://github.com/openai/whisper" target="_blank" rel="noopener noreferrer"&gt;Python SDK&lt;/a&gt;, and the details of its architecture were &lt;a href="https://cdn.openai.com/papers/whisper.pdf" target="_blank" rel="noopener noreferrer"&gt;published&lt;/a&gt;, allowing the scientific community to improve on it.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/whisper-wer-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The WER (word error rate) of Whisper was extremely impressive at the time of its publication (see the full diagram &lt;a href="https://github.com/openai/whisper/assets/266841/f4619d66-1058-4005-8f67-a9d811b77c62" target="_blank" rel="noopener noreferrer"&gt;here&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Since then, speech-to-text models kept improving at a steady pace. Nowadays the Whisper's family of models sees some competition for the title of best STT model from  companies such as &lt;a href="https://deepgram.com/" target="_blank" rel="noopener noreferrer"&gt;Deepgram&lt;/a&gt;, but it's still one of the best options in terms of open-source models.&lt;/p&gt;
&lt;h3&gt;Text-to-speech (TTS)&lt;/h3&gt;
&lt;p&gt;Text-to-speech model perform the exact opposite task than speech-to-text models: their goal is to convert written text into an audio stream of synthetic speech. Text-to-speech has &lt;a href="https://en.wikipedia.org/wiki/Speech_synthesis#History" target="_blank" rel="noopener noreferrer"&gt;historically been an easier feat&lt;/a&gt; than speech-to-text, but it also recently saw drastic improvements in the quality of the synthetic voices, to the point that it could nearly be considered a solved problem in its most basic form.&lt;/p&gt;
&lt;p&gt;Today many companies (such as OpenAI, &lt;a href="https://cartesia.ai/sonic" target="_blank" rel="noopener noreferrer"&gt;Cartesia&lt;/a&gt;, &lt;a href="https://elevenlabs.io/" target="_blank" rel="noopener noreferrer"&gt;ElevenLabs&lt;/a&gt;, Azure and many others) offer TTS software with voices that sound nearly indistinguishable to a human. They also have the capability to clone a specific human voice with remarkably little training data (just a few seconds of speech) and to tune accents, inflections, tone and even emotion.&lt;/p&gt;
&lt;div&gt;
&lt;audio controls src="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/sonic-tts-sample.wav" style="width: 100%"&gt;&lt;/audio&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;&lt;a href="https://cartesia.ai/sonic" target="_blank" rel="noopener noreferrer"&gt;Cartesia's Sonic&lt;/a&gt; TTS example of a gaming NPC. Note how the model subtly reproduces the breathing in between sentences.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;TTS is still improving in quality by the day, but due to the incredibly high quality of the output competition now tends to focus on price and performance.&lt;/p&gt;
&lt;h3&gt;Logic engine&lt;/h3&gt;
&lt;p&gt;Advancements in the agent's ability to talk to users goes hand in hand with the progress of natural language understanding (NLU), another field with a &lt;a href="https://en.wikipedia.org/wiki/Natural_language_understanding#History" target="_blank" rel="noopener noreferrer"&gt;long and complicated history&lt;/a&gt;. Until recently, the bot's ability to understand the user's request has been severely limited and often available only for major languages.&lt;/p&gt;
&lt;p&gt;Based on the way their logic is implemented, today you may come across bots that rely on three different categories.&lt;/p&gt;
&lt;h4&gt;Tree-based&lt;/h4&gt;
&lt;p&gt;Tree-based (or rule-based) logic is one of the earliest method of implementing chatbot's logic, still very popular today for its simplicity. Tree-based bots don't really try to understand what the user is saying, but listen to the user looking for a keyword or key sentence that will trigger the next step. For example, a customer support chatbot may look for the keyword "refund" to give the user any information about how to perform a refund, or the name of a discount campaign to explain the user how to take advantage of that.&lt;/p&gt;
&lt;p&gt;Tree-based logic, while somewhat functional, doesn't really resemble a conversation and can become very frustrating to the user when the conversation tree was not designed with care, because it's difficult for the end user to understand which option or keyword they should use to achieve the desired outcome. It is also unsuitable to handle real questions and requests like a human would. &lt;/p&gt;
&lt;p&gt;One of its most effective usecases is as a first-line screening to triage incoming messages.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/tree-based-logic-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example of a very simple decision tree for a chatbot. While rather minimal, this bot already has several flaws: there's no way to correct the information you entered at a previous step, and it has no ability to recognize synonyms ("I want to buy an item" would trigger the fallback route.)&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;Intent-based&lt;/h4&gt;
&lt;p&gt;In intent-based bots, &lt;strong&gt;intents&lt;/strong&gt; are defined roughly as "actions the users may want to do". With respect to a strict, keyword-based tree structure, intent-based bots may switch from an intent to another much more easily (because they lack a strict tree-based routing) and may use advanced AI techniques to understand what the user is actually trying to accomplish and perform the required action.&lt;/p&gt;
&lt;p&gt;Advanced voice assistants such as Siri and Alexa use variations of this intent-based system. However, as their owners are usually familiar with, interacting with an intent-based bot doesn't always feel natural, especially when the available intents don't match the user's expectation and the bot ends up triggering an unexpected action. In the long run, this ends with users carefully second-guessing what words and sentence structures activate the response they need and eventually leads to a sort of "magical incantation" style of prompting the agent, where the user has to learn what is the "magical sentence" that the bot will recognize to perform a specific intent without misunderstandings.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/amazon-echo.webp" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Modern voice assistants like Alexa and Siri are often built on the concept of intent (image from Amazon).&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;LLM-based&lt;/h4&gt;
&lt;p&gt;The introduction of instruction-tuned GPT models like ChatGPT revolutionized the field of natural language understanding and, with it, the way bots can be built today. LLMs are naturally good at conversation and can formulate natural replies to any sort of question, making the conversation feel much more natural than with any technique that was ever available earlier.&lt;/p&gt;
&lt;p&gt;However, LLMs tend to be harder to control. Their very ability of generating naturally sounding responses for anything makes them behave in ways that are often unexpected to the developer of the chatbot: for example, users can get the LLM-based bot to promise them anything they ask for, or they can be convinced to say something incorrect or even occasionally lie.&lt;/p&gt;
&lt;p&gt;The problem of controlling the conversation, one that traditionally was always on the user's side, is now entirely on the shoulders of the developers and can easily backfire.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/chatgpt-takesies-backsies.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In a rather &lt;a href="https://x.com/ChrisJBakke/status/1736533308849443121" target="_blank" rel="noopener noreferrer"&gt;famous instance&lt;/a&gt;, a user managed to convince a Chevrolet dealership chatbot to promise selling him a Chevy Tahoe for a single dollar.&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Audio-to-audio models&lt;/h3&gt;
&lt;p&gt;On top of all these changes, OpenAI recently made a step further. They latest flagship model, &lt;a href="https://openai.com/index/hello-gpt-4o/" target="_blank" rel="noopener noreferrer"&gt;GPT 4o&lt;/a&gt;, was allegedly able to understand audio natively, taking away the need for a dedicated speech-to-text model, and to produce audio responses directly, making text-to-speech engines also redundant. &lt;/p&gt;
&lt;p&gt;For a long time these capabilities were heavily restricted to a limited number of partners, but as of the 1st of October 2024, they eventually made such capabilities generally available through their new &lt;a href="https://openai.com/index/introducing-the-realtime-api/" target="_blank" rel="noopener noreferrer"&gt;Realtime API&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;At a first glance, the release of such model seemed to shake the foundations of how we build voice bots today. However, at the time of writing, there are still a number of hurdles that prevents immediate adoption, the main one being &lt;strong&gt;price&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/realtime-api-pricing.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://openai.com/api/pricing/" target="_blank" rel="noopener noreferrer"&gt;Pricing of the Realtime API&lt;/a&gt; at the time of writing (October 2024)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The problem here is that voice bots built with the traditional stack can be more than sufficient to cover the most common usecases for a fraction of the price of GPT 4o, and while the latter can indeed handle cases that are impossible to address for a traditional voice bot, in most practical situations such capabilities are not necessary to achieve a smooth and effective interaction.&lt;/p&gt;
&lt;p&gt;However, GPT 4o is surely a step further on the evolutionary path of modern voice bots. With potential future price changes, a model like this could easily become a valid competitor to the architecture we're going to explore in the rest of the post, with its own pros and cons.&lt;/p&gt;
&lt;h2&gt;New challenges&lt;/h2&gt;
&lt;p&gt;Thanks to all these recent improvements, it would seem that making natural-sounding, smart bots is getting easier and easier. It is indeed much simpler to make a simple bot sound better, understand more and respond appropriately, but there's still a long way to go before users can interact with these new bots as they would with a human.&lt;/p&gt;
&lt;p&gt;The issue lays in the fact that &lt;strong&gt;users expectations grow&lt;/strong&gt; with the quality of the bot. It's not enough for the bot to have a voice that sounds human: users want to be able to interact with it in a way that it feels human too, which is far more rich and interactive than what the rigid tech of earlier chatbots allowed so far.&lt;/p&gt;
&lt;p&gt;What does this mean in practice? What are the expectations that users might have from our bots?&lt;/p&gt;
&lt;h3&gt;Real speech is not turn-based&lt;/h3&gt;
&lt;p&gt;Traditional bots can only handle turn-based conversations: the user talks, then the bot talks as well, then the user talks some more, and so on. A conversation with another human, however, has no such limitation: people may talk over each other, give audible feedback without interrupting, and more.&lt;/p&gt;
&lt;p&gt;Here are some examples of this richer interaction style:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interruptions&lt;/strong&gt;. Interruptions occur when a person is talking and another one starts talking at the same time. It is expected that the first person stops talking, at least for a few seconds, to understand what the interruption was about, while the second person continue to talk.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Back-channeling&lt;/strong&gt;. Back-channeling is the practice of saying "ok", "sure", "right" while the other person is explaining something, to give them feedback and letting them know we're paying attention to what is being said. The person that is talking is not supposed to stop: the aim of this sort of feedback is to let them know they are being heard.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Pinging&lt;/strong&gt;. This is the natural reaction a long silence, especially over a voice-only medium such as a phone call. When one of the two parties is supposed to speak but instead stays silent, the last one that talked might "ping" the silent party by asking "Are you there?", "Did you hear?", or even just "Hello?" to test whether they're being heard. This behavior is especially difficult to handle for voice agents that have a significant delay, because it may trigger an ugly vicious cycle of repetitions and delayed replies.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Buying time&lt;/strong&gt;. When one of the parties know that it will stay silent for a while, a natural reaction is to notify the other party in advance by saying something like "Hold on...", "Wait a second...", "Let me check..." and so on. This message has the benefit of preventing the "pinging" behavior we've seen before and can be very useful for voice bots that may need to carry on background work during the conversation, such as looking up information.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Audible clues&lt;/strong&gt;. Not everything can be transcribed by a speech-to-text model, but audio carries a lot of nuance that is often used by humans to communicate. A simple example is pitch: humans can often tell if they're talking to a child, a woman or a man by the pitch of their voice, but STT engines don't transcribe that information. So if a child picks up the phone, the model won't pick up the obvious audible clue and will likely assume it is talking to an adult. Similar considerations should be made for tone (to detect mood, sarcasm, etc) or other sounds like laughter, sobs, and more. &lt;strong&gt;Audio-to-audio models&lt;/strong&gt; such as GPT 4o don't have this intrinsic limitation, but while they surely can pick up these clues, their ability to use them effectively should not be taken for granted.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Real conversation flows are not predictable&lt;/h3&gt;
&lt;p&gt;Tree-based bots, and to some degree intent-based too, work on the implicit assumption that conversation flows are largely predictable. Once the user said something and the bot replied accordingly, they can only follow up with a fixed set of replies and nothing else.&lt;/p&gt;
&lt;p&gt;This is often a flawed assumption and the primary reason why talking to chatbots tends to be so frustrating.&lt;/p&gt;
&lt;p&gt;In reality, natural conversations are largely unpredictable. For example, they may feature:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sudden changes of topic&lt;/strong&gt;. Maybe user and bot were talking about making a refund, but then the user changes their mind and decides to ask for assistance finding a repair center for the product. Well designed intent-based bots can deal with that, but most bots are in practice unable to do so in a way that feels natural to the user.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unexpected, erratic phrasing&lt;/strong&gt;. This is common when users are nervous or in a bad mood for any reason. Erratic, convoluted phrasing, long sentences, rambling, are all very natural ways of expressing themselves, but such outbursts very often confuse bots completely.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Non native speakers&lt;/strong&gt;. Due to the nature la language learning, non native speakers may have trouble pronouncing words correctly, they may use highly unusual synonyms, or structure sentences in complicated ways. This is also difficult for bots to handle, because understanding the sentence is harder and transcription issues are far more likely.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Non sequitur&lt;/strong&gt;&lt;/em&gt;. &lt;em&gt;Non sequitur&lt;/em&gt; is an umbrella term for a sequence of sentences that bear no relation to each other in a conversation. A simple example is the user asking the bot "What's the capital of France" and the bot replies "It's raining now". When done by the bot, this is often due to a severe transcription issue or a very flawed conversation design. When done by the user, it's often a malicious intent to break the bot's logic, so it should be handled with some care.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;LLMs bring their own problems&lt;/h3&gt;
&lt;p&gt;It may seem that some of these issues, especially the ones related to conversation flow, could be easily solved with an LLM. These models, however, bring their own set of issues too:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hallucinations&lt;/strong&gt;. This is a technical term to say that LLMs can occasionally mis-remember information, or straight up lie. The problem is that they're also very confident about their statements, sometimes to the point of trying to gaslight their users. Hallucinations are a major problem for all LLMs: although it may seem to get more manageable with larger and smarter models, the problem only gets more subtle and harder to spot.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Misunderstandings&lt;/strong&gt;. While LLMs are great at understanding what the user is trying to say, they're not immune to misunderstandings. Unlike a human though, LLMs rarely suspect a misunderstanding and they rather make assumptions that ask for clarifications, resulting in surprising replies and behavior that are reminiscent of intent-based bots.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Lack of assertiveness&lt;/strong&gt;. LLMs are trained to listen to the user and do their best to be helpful. This means that LLMs are also not very good at taking the lead of the conversation when we would need them to, and are easily misled and distracted by a motivated user. Preventing your model to give your user's a literary analysis of their unpublished poetry may sound silly, but it's a lot harder than many suspect.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prompt hacking&lt;/strong&gt;. Often done with malicious intent by experienced users, prompt hacking is the practice of convincing an LLM to reveal its initial instructions, ignore them and perform actions they are explicitly forbidden from. This is especially dangerous and, while a lot of work has gone into this field, this is far from a solved problem.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;The context window&lt;/h3&gt;
&lt;p&gt;LLMs need to keep track of the whole conversation, or at least most of it, to be effective. However, they often have a limitation to the amount of text they can keep in mind at any given time: this limit is called &lt;strong&gt;context window&lt;/strong&gt; and for many models is still relatively low, at about 2000 tokens &lt;strong&gt;(between 1500-1800 words)&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;The problem is that this window also need to include all the instructions your bot needs for the conversation. This initial set of instructions is called &lt;strong&gt;system prompt&lt;/strong&gt;, and is slightly distinct from the other messages in the conversation to make the LLM understand that it's not part of it, but it's a set of instructions about how to handle the conversation.&lt;/p&gt;
&lt;p&gt;For example, a system prompt for a customer support bot may look like this:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;You&amp;#39;re a friendly customer support bot named VirtualAssistant. 
You are always kind to the customer and you must do your best 
to make them feel at ease and helped.

You may receive a set of different requests. If the users asks
you to do anything that is not in the list below, kindly refuse
to do so.

# Handle refunds

If the user asks you to handle a refund, perform these actions:
- Ask for their shipping code
- Ask for their last name
- Use the tool `get_shipping_info` to verify the shipping exists
...

# Handle subscriptions

If the user asks you to subscribe to a service, perform these actions:
- Ask what subscription are they interested in
- Ask if they have a promo code
- Ask for their username
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;and so on.&lt;/p&gt;
&lt;p&gt;Although very effective, system prompts have a tendency to become huge in terms of tokens. Adding information to it makes the LLM behave much more like you expect (although it's not infallible), hallucinate less, and can even shape its personality to some degree. But if the system prompt becomes too long (more than 1000 words), this means that the bot will only be able to exchange about 800 words worth of messages with the user before it starts to &lt;strong&gt;forget&lt;/strong&gt; either its instructions or the first messages of the conversation. For example, the bot will easily forget its own name and role, or it will forget the user's name and initial demands, which can make the conversation drift completely.&lt;/p&gt;
&lt;h3&gt;Working in real time&lt;/h3&gt;
&lt;p&gt;If all these issues weren't enough, there's also a fundamental issue related to voice interaction: &lt;strong&gt;latency&lt;/strong&gt;. Voice bots interact with their users in real time: this means that the whole pipeline of transcription, understanding, formulating a reply and synthesizing it back but be very fast.&lt;/p&gt;
&lt;p&gt;How fast? On average, people expect a reply from another person to arrive within &lt;strong&gt;300-500ms&lt;/strong&gt; to sound natural. They can normally wait for about 1-2 seconds. Any longer and they'll likely ping the bot, breaking the flow.&lt;/p&gt;
&lt;p&gt;This means that, even if we had some solutions to all of the above problems (and we do have some), these solutions needs to operate at blazing fast speed. Considering that LLM inference alone can take the better part of a second to even start being generated, latency is often one of the major issues that voice bots face when deployed at scale.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/ttft-inv.jpg"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Time to First Token (TTFT) stats for several LLM inference providers running Llama 2 70B chat. From &lt;a href="https://github.com/ray-project/llmperf-leaderboard" target="_blank" rel="noopener noreferrer"&gt;LLMPerf leaderboard&lt;/a&gt;. You can see how the time it takes for a reply to even start being produced is highly variable, going up to more than one second in some scenarios.&lt;/em&gt;  &lt;/p&gt;
&lt;h2&gt;To be continued...&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Interested? Check out &lt;a href="/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2"&gt;Part 2&lt;/a&gt;!&lt;/em&gt; &lt;/p&gt;
&lt;p class="fleuron"&gt;&lt;a href="/posts/2024-05-06-teranoptia/"&gt;F]&lt;/a&gt;&lt;/p&gt;</description></item><item><title>The Agent Compass</title><link>https://www.zansara.dev/posts/2024-06-10-the-agent-compass/</link><pubDate>Mon, 10 Jun 2024 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/posts/2024-06-10-the-agent-compass/</guid><description>&lt;p&gt;The concept of Agent is one of the vaguest out there in the post-ChatGPT landscape. The word has been used to identify systems that seem to have nothing in common with one another, from complex autonomous research systems down to a simple sequence of two predefined LLM calls. Even the distinction between Agents and techniques such as RAG and prompt engineering seems blurry at best.&lt;/p&gt;
&lt;p&gt;Let's try to shed some light on the topic by understanding just how much the term "AI Agent" covers and set some landmarks to better navigate the space.&lt;/p&gt;
&lt;h2&gt;Defining "Agent"&lt;/h2&gt;
&lt;p&gt;The problem starts with the definition of "agent". For example, &lt;a href="https://en.wikipedia.org/wiki/Software_agent" target="_blank" rel="noopener noreferrer"&gt;Wikipedia&lt;/a&gt; reports that a software agent is&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;a computer program that acts for a user or another program in a relationship of agency.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This definition is extremely high-level, to the point that it could be applied to systems ranging from ChatGPT to a thermostat. However, if we restrain our definition to "LLM-powered agents", then it starts to mean something: an Agent is an LLM-powered application that is given some &lt;strong&gt;agency&lt;/strong&gt;, which means that it can take actions to accomplish the goals set by its user. Here we see the difference between an agent and a simple chatbot, because a chatbot can only talk to a user. but don't have the agency to take any action on their behalf. Instead, an Agent is a system you can effectively delegate tasks to. &lt;/p&gt;
&lt;p&gt;In short, an LLM powered application can be called an Agent when&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;it can take decisions and choose to perform actions in order to achieve the goals set by the user.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Autonomous vs Conversational&lt;/h2&gt;
&lt;p&gt;On top of this definition there's an additional distinction to take into account, normally brought up by the terms &lt;strong&gt;autonomous&lt;/strong&gt; and &lt;strong&gt;conversational&lt;/strong&gt; agents.&lt;/p&gt;
&lt;p&gt;Autonomous Agents are applications that &lt;strong&gt;don't use conversation as a tool&lt;/strong&gt; to accomplish their goal. They can use several tools several times, but they won't produce an answer for the user until their goal is accomplished in full. These agents normally interact with a single user, the one that set their goal, and the whole result of their operations might be a simple notification that the task is done. The fact that they can understand language is rather a feature that lets them receive the user's task in natural language, understand it, and then to navigate the material they need to use (emails, webpages, etc).&lt;/p&gt;
&lt;p&gt;An example of an autonomous agent is a &lt;strong&gt;virtual personal assistant&lt;/strong&gt;: an app that can read through your emails and, for example, pays the bills for you when they're due. This is a system that the user sets up with a few credentials and then works autonomously, without the user's supervision, on the user's own behalf, possibly without bothering them at all.&lt;/p&gt;
&lt;p&gt;On the contrary, Conversational Agents &lt;strong&gt;use conversation as a tool&lt;/strong&gt;, often their primary one. This doesn't have to be a conversation with the person that set them off: it's usually a conversation with another party, that may or may not be aware that they're talking to an autonomous system. Naturally, they behave like agents only from the perspective of the user that assigned them the task, while in many cases they have very limited or no agency from the perspective of the users that holds the conversation with them.&lt;/p&gt;
&lt;p&gt;An example of a conversational agent is a &lt;strong&gt;virtual salesman&lt;/strong&gt;: an app that takes a list of potential clients and calls them one by one, trying to persuade them to buy. From the perspective of the clients receiving the call this bot is not an agent: it can perform no actions on their behalf, in fact it may not be able to perform actions at all other than talking to them. But from the perspective of the salesman the bots are agents, because they're calling people for them, saving a lot of their time.&lt;/p&gt;
&lt;p&gt;The distinction between these two categories is very blurry, and &lt;strong&gt;some systems may behave like both&lt;/strong&gt; depending on the circumnstances. For example, an autonomous agent might become a conversational one if it's configured to reschedule appointments for you by calling people, or to reply to your emails to automatically challenge parking fines, and so on. Alternatively, an LLM that asks you if it's appropriate to use a tool before using it is behaving a bit like a conversational agent, because it's using the chat to improve its odds of providing you a better result.&lt;/p&gt;
&lt;h2&gt;Degrees of agency&lt;/h2&gt;
&lt;p&gt;All the distinctions we made above are best understood as a continuous spectrum rather than hard categories. Various AI systems may have more or less agency and may be tuned towards a more "autonomous" or "conversational" behavior.&lt;/p&gt;
&lt;p&gt;In order to understand this difference in practice, let's try to categorize some well-known LLM techniques and apps to see how "agentic" they are. Having two axis to measure by, we can build a simple compass like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="a compass with two axis: no agency (left) to full agency (right) on the horizontal axis, and autonomous (bottom) to conversational (top) on the vertical axis." src="/posts/2024-06-10-the-agent-compass/empty-compass-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;div style="text-align:center;"&gt;&lt;i&gt;Our Agent compass&lt;/i&gt;&lt;/div&gt;

&lt;h3&gt;Bare LLMs&lt;/h3&gt;
&lt;p&gt;Many apps out there perform nothing more than direct calls to LLMs, such as ChatGPT's free app and other similarly simple assistants and chatbots. There are no more components to this system other than the model itself and their mode of operation is very straightforward: a user asks a question to an LLM, and the LLM replies directly.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram of the operation of a direct LLM call: a user asks a question to an LLM and the LLM replies directly." src="/posts/2024-06-10-the-agent-compass/direct-llm-call-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;This systems are not designed with the intent of accomplishing a goal, and neither can take any actions on the user's behalf. They focus on talking with a user in a reactive way and can do nothing else than talk back. An LLM on its own has &lt;strong&gt;no agency at all&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;At this level it also makes very little sense to distinguish between autonomous or conversational agent behavior, because the entire app shows no degrees of autonomy. So we can place them at the very center-left of the diagram.&lt;/p&gt;
&lt;p&gt;&lt;img alt="the updated compass" src="/posts/2024-06-10-the-agent-compass/direct-llm-call-compass-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;h3&gt;Basic RAG&lt;/h3&gt;
&lt;p&gt;Together with direct LLM calls and simple chatbots, basic RAG is also an example of an application that does not need any agency or goals to pursue in order to function. Simple RAG apps works in two stages: first the user question is sent to a retriever system, which fetches some additional data relevant to the question. Then, the question and the additional data is sent to the LLM to formulate an answer. &lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram of the operation of a RAG app: first the user question is sent to a retriever system, which fetches some additional data relevant to the question. Then, the question and the additional data is sent to the LLM to formulate an answer." src="/posts/2024-06-10-the-agent-compass/basic-rag-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;This means that simple RAG is not an agent: the LLM has no role in the retrieval step and simply reacts to the RAG prompt, doing little more than what a direct LLM call does. &lt;strong&gt;The LLM is given no agency&lt;/strong&gt;, takes no decisions in order to accomplish its goals, and has no tools it can decide to use, or actions it can decide to take. It's a fully pipelined, reactive system. However, we may rank basic RAG more on the autonomous side with respect to a direct LLM call, because there is one step that is done automonously (the retrieval).&lt;/p&gt;
&lt;p&gt;&lt;img alt="the updated compass" src="/posts/2024-06-10-the-agent-compass/basic-rag-compass-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;h3&gt;Agentic RAG&lt;/h3&gt;
&lt;p&gt;Agentic RAG is a slightly more advanced version of RAG that does not always perform the retrieval step. This helps the app produce better prompts for the LLM: for example, if the user is asking a question about trivia, retrieval is very important, while if they're quizzing the LLM with some mathematical problem, retrieval might confuse the LLM by giving it examples of solutions to different puzzles, and therefore make hallucinations more likely.&lt;/p&gt;
&lt;p&gt;This means that an agentic RAG app works as follows: when the user asks a question, before calling the retriever the app checks whether the retrieval step is necessary at all. Most of the time the preliminary check is done by an LLM as well, but in theory the same check coould be done by a properly trained classifier model. Once the check is done, if retrieval was necessary it is run, otherwise the app skips directly to the LLM, which then replies to the user.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram of the operation of an agentic RAG app: when the user asks a question, before calling the retriever the app checks whether the retrieval step is necessary at all. Once the check is done, if retrieval was necessary it is run, otherwise the app skips directly to the LLM, which then replies to the user." src="/posts/2024-06-10-the-agent-compass/agentic-rag-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;You can see immediately that there's a fundamental difference between this type of RAG and the basic pipelined form: the app needs to &lt;strong&gt;take a decision&lt;/strong&gt; in order to accomplish the goal of answering the user. The goal is very limited (giving a correct answer to the user), and the decision very simple (use or not use a single tool), but this little bit of agency given to the LLM makes us place an application like this definitely more towards the Agent side of the diagram.&lt;/p&gt;
&lt;p&gt;&lt;img alt="the updated compass" src="/posts/2024-06-10-the-agent-compass/agentic-rag-compass-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;We keep Agentic RAG towards the Autonomous side because in the vast majority of cases the decision to invoke the retriever is kept hidden from the user.&lt;/p&gt;
&lt;h3&gt;LLMs with function calling&lt;/h3&gt;
&lt;p&gt;Some LLM applications, such as ChatGPT with GPT4+ or Bing Chat, can make the LLM use some predefined tools: a web search, an image generator, and maybe a few more. The way they work is quite straightforward: when a user asks a question, the LLM first needs to decide whether it should use a tool to answer the question. If it decides that a tool is needed, it calls it, otherwise it skips directly to generating a reply, which is then sent back to the user.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram of the operation of an LLM with function calling: when a user asks a question, the LLM first needs to decide whether it should use a tool to answer the question. If it decides that a tool is needed, it calls it, otherwise it skips directly to generating a reply, which is then sent back to the user." src="/posts/2024-06-10-the-agent-compass/llm-with-function-calling-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;You can see how this diagram resemble agentic RAG's: before giving an answer to the user, the app needs to &lt;strong&gt;take a decision&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;With respect to Agentic RAG this decision is a lot more complex: it's not a simple yes/no decision, but it involves choosing which tool to use and also generate the input parameters for the selected tool that will provide the desired output. In many cases the tool's output will be given to the LLM to be re-elaborated (such as the output of a web search), while in some other it can go directly to the user (like in the case of image generators). This all implies that more agency is given to the system and, therefore, it can be placed more clearly towards the Agent end of the scale.&lt;/p&gt;
&lt;p&gt;&lt;img alt="the updated compass" src="/posts/2024-06-10-the-agent-compass/llm-with-function-calling-compass-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;We place LLMs with function calling in the middle between Conversational and Autonomous because the degree to which the user is aware of this decision can vary greatly between apps. For example, Bing Chat and ChatGPT normally notify the user that they're going to use a tool when they do, and the user can instruct them to use them or not, so they're slightly more conversational.&lt;/p&gt;
&lt;h3&gt;Self correcting RAG&lt;/h3&gt;
&lt;p&gt;Self-correcting RAG is a technique that improves on simple RAG by making the LLM double-check its replies before returning them to the user. It comes from an LLM evaluation technique called "LLM-as-a-judge", because an LLM is used to judge the output of a different LLM or RAG pipeline.&lt;/p&gt;
&lt;p&gt;Self-correcting RAG starts as simple RAG: when the user asks a question, the retriever is called and the results are sent to the LLM to extract an answer from. However, before returning the answer to the user, another LLM is asked to judge whether in their opinion, the answer is correct. If the second LLM agrees, the answer is sent to the user. If not, the second LLM generates a new question for the retriever and runs it again, or in other cases, it simply integrates its opinion in the prompt and runs the first LLM again.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram of the operation of self correcting RAG: when the user asks a question, the retriever is called and the results are sent to the LLM to extract an answer from. However, before returning the answer to the user, another LLM is asked to judge whether in their opinion, the answer is correct. If the second LLM agrees, the answer is sent to the user. If not, the second LLM generates a new question for the retriever and runs it again, or in other cases, it simply integrates its opinion in the prompt and runs the first LLM again." src="/posts/2024-06-10-the-agent-compass/self-correcting-rag-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;Self-correcting RAG can be seen as &lt;strong&gt;one more step towards agentic behavior&lt;/strong&gt; because it unlocks a new possibility for the application: &lt;strong&gt;the ability to try again&lt;/strong&gt;. A self-correcting RAG app has a chance to detect its own mistakes and has the agency to decide that it's better to try again, maybe with a slightly reworded question or different retrieval parameters, before answering the user. Given that this process is entirely autonomous, we'll place this technique quite towards the Autonomous end of the scale.&lt;/p&gt;
&lt;p&gt;&lt;img alt="the updated compass" src="/posts/2024-06-10-the-agent-compass/self-correcting-rag-compass-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;h3&gt;Chain-of-thought&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/2201.11903" target="_blank" rel="noopener noreferrer"&gt;Chain-of-thought&lt;/a&gt; is a family of prompting techniques that makes the LLM "reason out loud". It's very useful when the model needs to process a very complicated question, such as a mathematical problem or a layered question like "When was the eldest sistem of the current King of Sweden born?" Assuming that the LLM knows these facts, in order to not hallucinate it's best to ask the model to proceed "step-by-step" and find out, in order:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Who the current King of Sweden is,&lt;/li&gt;
&lt;li&gt;Whether he has an elder sister,&lt;/li&gt;
&lt;li&gt;If yes, who she is,&lt;/li&gt;
&lt;li&gt;The age of the person identified above.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The LLM might know the final fact in any case, but the probability of it giving the right answer increases noticeably if the LLM is prompted this way.&lt;/p&gt;
&lt;p&gt;Chain-of-thought prompts can also be seen as the LLM accomplishing the task of finding the correct answer in steps, which implies that there are two lines of thinking going on: on one side the LLM is answering the questions it's posing to itself, while on the other it's constantly re-assessing whether it has a final answer for the user. &lt;/p&gt;
&lt;p&gt;In the example above, the chain of thought might end at step 2 if the LLM realizes that the current King of Sweden has no elder sisters (he &lt;a href="https://en.wikipedia.org/wiki/Carl_XVI_Gustaf#Early_life" target="_blank" rel="noopener noreferrer"&gt;doesn't&lt;/a&gt;): the LLM needs to keep an eye of its own thought process and decide whether it needs to continue or not. &lt;/p&gt;
&lt;p&gt;We can summarize an app using chain-of-thought prompting like this: when a user asks a question, first of all the LLM reacts to the chain-of-thought prompt to lay out the sub-questions it needs to answer. Then it answers its own questions one by one, asking itself each time whether the final answer has already been found. When the LLM believes it has the final answer, it rewrites it for the user and returns it.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram of the operation of a chain-of-thought LLM app: when a user asks a question, first of all the LLM reacts to the chain-of-thought prompt to lay out the sub-questions it needs to answer. Then it answers its own questions one by one, asking itself each time whether the final answer has already been found. When the LLM believes it has the final answer, it rewrites it for the user and returns it " src="/posts/2024-06-10-the-agent-compass/chain-of-thought-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;This new prompting technique makes a big step towards full agency: the ability for the LLM to &lt;strong&gt;assess whether the goal has been achieved&lt;/strong&gt; before returning any answer to the user. While apps like Bing Chat iterate with the user and need their feedback to reach high-level goals, chain-of-thought gives the LLM the freedom to check its own answers before having the user judge them, which makes the loop much faster and can increase the output quality dramatically.&lt;/p&gt;
&lt;p&gt;This process is similar to what self-correcting RAG does, but has a wider scope, because the LLM does not only need to decide whether an answer is correct, it can also decide to continue reasoning in order to make it more complete, more detailed, to phrase it better, and so on.&lt;/p&gt;
&lt;p&gt;Another interesting trait of chain-of-thought apps is that they introduce the concept of &lt;strong&gt;inner monologue&lt;/strong&gt;. The inner monologue is a conversation that the LLM has with itself, a conversation buffer where it keeps adding messages as the reasoning develops. This monologue is not visible to the user, but helps the LLM deconstruct a complex reasoning line into a more manageable format, like a researcher that takes notes instead of keeping all their earlier reasoning inside their head all the times.&lt;/p&gt;
&lt;p&gt;Due to the wider scope of the decision-making that chain-of-thought apps are able to do, they also place in the middle of our compass They can be seen as slightly more autonomous than conversational due to the fact that they hide their internal monologue to the user.&lt;/p&gt;
&lt;p&gt;&lt;img alt="the updated compass" src="/posts/2024-06-10-the-agent-compass/chain-of-thought-compass-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;From here, the next step is straightforward: using tools.&lt;/p&gt;
&lt;h3&gt;Multi-hop RAG&lt;/h3&gt;
&lt;p&gt;Multi-hop RAG applications are nothing else than simple RAG apps that use chain-of-thought prompting and are free to invoke the retriever as many times as needed and only when needed.&lt;/p&gt;
&lt;p&gt;This is how it works. When the user makes a question, a chain of thought prompt is generated and sent to the LLM. The LLM assesses whether it knows the answer to the question and if not, asks itself whether a retrieval is necessary. If it decides that retrieval is necessary it calls it, otherwise it skips it and generates an answer directly. It then checks again whether the question is answered. Exiting the loop, the LLM produces a complete answer by re-reading its own inner monologue and returns this reply to the user.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram of the operation of multi-hop RAG: when the user makes a question, a chain of thought prompt is generated and sent to the LLM. The LLM assesses whether it knows the answer to the question and if not, asks itself whether a retrieval is necessary. If it decides that retrieval is necessary it calls it, otherwise it skips it and generates an answer directly. It then checks again whether the question is answered. Exiting the loop, the LLM produces a complete answer by re-reading its own inner monologue and returns this reply to the user." src="/posts/2024-06-10-the-agent-compass/multi-hop-rag-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;An app like this is getting quite close to a proper autonomous agent, because it can &lt;strong&gt;perform its own research autonomously&lt;/strong&gt;. The LLM calls are made in such a way that the system is able to assess whether it knows enough to answer or whether it should do more research by formulating more questions for the retriever and then reasoning over the new collected data.&lt;/p&gt;
&lt;p&gt;Multi-hop RAG is a very powerful technique that shows a lot of agency and autonomy, and therefore can be placed in the lower-right quadrant of out compass. However, it is still limited with respect to a "true" autonomous agent, because the only action it can take is to invoke the retriever.&lt;/p&gt;
&lt;p&gt;&lt;img alt="the updated compass" src="/posts/2024-06-10-the-agent-compass/multi-hop-rag-compass-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;h3&gt;ReAct Agents&lt;/h3&gt;
&lt;p&gt;Let's now move onto apps that can be defined proper "agents". One of the first flavor of agentic LLM apps, and still the most popular nowadays, is called "&lt;a href="https://arxiv.org/abs/2210.03629" target="_blank" rel="noopener noreferrer"&gt;ReAct&lt;/a&gt;" Agents, which stands for "Reason + Act". ReAct is a prompting technique that belongs to the chain-of-thought extended family: it makes the LLM reason step by step, decide whether to perform any action, and then observe the result of the actions it took before moving further.&lt;/p&gt;
&lt;p&gt;A ReAct agent works more or less like this: when user sets a goal, the app builds a ReAct prompt, which first of all asks the LLM whether the answer is already known. If the LLM says no, the prompt makes it select a tool. The tool returns some values which are added to the inner monologue of the application toghether with the invitation to re-assess whether the goal has been accomplished. The app loops over until the answer is found, and then the answer is returned to the user.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram of the operation of a ReAct Agent: when user sets a goal, the app builds a ReAct prompt, which first of all asks the LLM whether the answer is already known. If the LLM says no, the prompt makes it select a tool. The tool returns some values which are added to the inner monologue of the application toghether with the invitation to re-assess whether the goal has been accomplished. The app loops over until the answer is found, and then the answer is returned to the user." src="/posts/2024-06-10-the-agent-compass/react-agent-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, the structure is very similar to a multi-hop RAG, with an important difference: ReAct Agents normally have &lt;strong&gt;many tools to choose from&lt;/strong&gt; rather than a single retriever. This gives them the agency to take much more complex decisions and can be finally called "agents".&lt;/p&gt;
&lt;p&gt;&lt;img alt="the updated compass" src="/posts/2024-06-10-the-agent-compass/react-agent-compass-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;ReAct Agents are very autonomous in their tasks and rely on an inner monologue rather than a conversation with a user to achieve their goals. Therefore we place them very much on the Autonomous end of the spectrum.&lt;/p&gt;
&lt;h3&gt;Conversational Agents&lt;/h3&gt;
&lt;p&gt;Conversational Agents are a category of apps that can vary widely. As stated earlier, conversational agents focus on using the conversation itself as a tool to accomplish goals, so in order to understand them, one has to distinguish between the people that set the goal (let's call them &lt;em&gt;owners&lt;/em&gt;) and those who talk with the bot (the &lt;em&gt;users&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Once this distinction is made, this is how the most basic conversational agents normally work. First, the owner sets a goal. The application then starts a conversation with a user and, right after the first message, starts asking itself if the given goal was accomplished. It then keeps talking to the target user until it believes the goal was attained and, once done, it returns back to its owner to report the outcome.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram of the operation of a Conversational Agent: first, the owner sets a goal. The application then starts a conversation with a user and, right after the first message, starts asking itself if the given goal was accomplished. It then keeps talking to the target user until it believes the goal was attained and, once done, it returns back to its owner to report the outcome." src="/posts/2024-06-10-the-agent-compass/basic-conversational-agent-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;p&gt;Basic conversational agents are very agentic in the sense that they can take a task off the hands of their owners and keep working on them until the goal is achieved. However, &lt;strong&gt;they have varying degrees of agency&lt;/strong&gt; depending on how many tools they can use and how sophisticated is their ability to talk to their target users.&lt;/p&gt;
&lt;p&gt;For example, can the communication occurr over one single channel, be it email, chat, voice, or something else? Can the agent choose among different channels to reach the user? Can it perform side tasks to behalf of either party to work towards its task? There is a large variety of these agents available and no clear naming distinction between them, so depending on their abilities, their position on our compass might be very different. This is why we place them in the top center, spreading far out in both directions.&lt;/p&gt;
&lt;p&gt;&lt;img alt="the updated compass" src="/posts/2024-06-10-the-agent-compass/conversational-agent-compass-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;h3&gt;AI Crews&lt;/h3&gt;
&lt;p&gt;By far the most advanced agent implementation available right now is called AI Crew, such as the ones provided by &lt;a href="https://www.crewai.com/" target="_blank" rel="noopener noreferrer"&gt;CrewAI&lt;/a&gt;. These apps take the concept of autonomous agent to the next level by making several different agents work together.&lt;/p&gt;
&lt;p&gt;The way these apps works is very flexible. For example, let's imagine we are making an AI application that can build a fully working mobile game from a simple description. This is an extremely complex task that, in real life, requires several developers. To achieve the same with an AI Crew, the crew needs to contain several agents, each one with their own special skills, tools, and background knowledge. There could be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a Designer Agent, that has all the tools to generate artwork and assets; &lt;/li&gt;
&lt;li&gt;a Writer Agent that writes the story, the copy, the dialogues, and most of the text; &lt;/li&gt;
&lt;li&gt;a Frontend Developer Agent that designs and implements the user interface; &lt;/li&gt;
&lt;li&gt;a Game Developer Agent that writes the code for the game itself;&lt;/li&gt;
&lt;li&gt;a Manager Agent, that coordinates the work of all the other agents, keeps them on track and eventually reports the results of their work to the user.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These agents interact with each other just like a team of humans would: by exchanging messages in a chat format, asking each other to perform actions for them, until their manager decides that the overall goal they were set to has been accomplished, and reports to the user.&lt;/p&gt;
&lt;p&gt;AI Crews are very advanced and dynamic systems that are still actively researched and explored. One thing that's clear though is that they show the highest level of agency of any other LLM-based app, so we can place them right at the very bottom-right end of the scale.&lt;/p&gt;
&lt;p&gt;&lt;img alt="the updated compass" src="/posts/2024-06-10-the-agent-compass/ai-crews-compass-inv.png"  class="invertible" /&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;What we've seen here are just a few examples of LLM-powered applications and how close or far they are to the concept of a "real" AI agent. AI agents are still a very active area of research, and their effectiveness is getting more and more reasonable as LLMs become cheaper and more powerful. &lt;/p&gt;
&lt;p&gt;As matter of fact, with today's LLMs true AI agents are possible, but in many cases they're too brittle and expensive for real production use cases. Agentic systems today suffer from two main issues: they perform &lt;strong&gt;huge and frequent LLM calls&lt;/strong&gt; and they &lt;strong&gt;tolerate a very low error rate&lt;/strong&gt; in their decision making.&lt;/p&gt;
&lt;p&gt;Inner monologues can grow to an unbounded size during the agent's operation, making the context window size a potential limitation. A single bad decision can send a chain-of-thought reasoning train in a completely wrong direction and many LLM calls will be performed before the system realizes its mistake, if it does at all. However, as LLMs become faster, cheaper and smarter, the day when AI Agent will become reliable and cheap enough is nearer than many think.&lt;/p&gt;
&lt;p&gt;Let's be ready for it!&lt;/p&gt;
&lt;p class="fleuron"&gt;&lt;a href="/posts/2024-05-06-teranoptia/"&gt;SDH&lt;/a&gt;&lt;/p&gt;</description></item><item><title>Generating creatures with Teranoptia</title><link>https://www.zansara.dev/posts/2024-05-06-teranoptia/</link><pubDate>Mon, 06 May 2024 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/posts/2024-05-06-teranoptia/</guid><description>&lt;style&gt;
    @font-face {
        font-family: teranoptia;
        src: url("/posts/2024-05-06-teranoptia/teranoptia/fonts/Teranoptia-Furiae.ttf");
    }

    .teranoptia {
        font-size: 50px;
        font-family: teranoptia;
        hyphens: none!important;
        line-height: 70px;
    }

    .small {
        font-size:30px;
        line-height: 40px;
    }

    .glyphset {
        display: flex;
        flex-wrap: wrap;
    }
    .glyphset div {
        margin: 3px;
    }
    .glyphset div p {
        text-align: center;
    }

&lt;/style&gt;

&lt;p&gt;Having fun with fonts doesn't always mean obsessing over kerning and ligatures. Sometimes, writing text is not even the point!&lt;/p&gt;
&lt;p&gt;You don't believe it? Type something in here.&lt;/p&gt;
&lt;textarea id="test-generated-animal" class="teranoptia" style="width: 100%; line-height: 50pt;"&gt;&lt;/textarea&gt;

&lt;div style="display: flex; gap: 10px;"&gt;
    Characters to generate:
    &lt;input id="test-glyph-count" type="number" value=10 &gt;&lt;/input&gt;
    &lt;button onclick="generateTest(document.getElementById('test-glyph-count').value);"&gt;Generate!&lt;/button&gt;
&lt;/div&gt;

&lt;script&gt;
function makeBreakable(animal){
    // Line break trick - avoid hypens and allows wrapping
    const animalFragments = animal.split(/(?=[yvspmiea≈∫ACFILOSW≈πv])/g);
    animal = animalFragments.join("&lt;wbr&gt;");
    return animal;
}

function generateTest(value){
    var newAnimal = '';
    for (var i = 0; i &lt; value; i++) {
        newAnimal += randomFrom(validChars);
    }
    document.getElementById("test-generated-animal").value = newAnimal;
}

&lt;/script&gt;

&lt;p&gt;&lt;a href="https://www.tunera.xyz/fonts/teranoptia/" target="_blank" rel="noopener noreferrer"&gt;Teranoptia&lt;/a&gt; is a cool font that lets you build small creatures by mapping each letter (and a few other characters) to a piece of a creature like a head, a tail, a leg, a wing and so on. By typing words you can create strings of creatures. &lt;/p&gt;
&lt;p&gt;Here is the glyphset:&lt;/p&gt;
&lt;div class="glyphset"&gt;
    &lt;div&gt;&lt;p&gt;A&lt;/p&gt;&lt;p class="teranoptia"&gt;A&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;B&lt;/p&gt;&lt;p class="teranoptia"&gt;B&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;C&lt;/p&gt;&lt;p class="teranoptia"&gt;C&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;D&lt;/p&gt;&lt;p class="teranoptia"&gt;D&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;E&lt;/p&gt;&lt;p class="teranoptia"&gt;E&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;F&lt;/p&gt;&lt;p class="teranoptia"&gt;F&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;G&lt;/p&gt;&lt;p class="teranoptia"&gt;G&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;H&lt;/p&gt;&lt;p class="teranoptia"&gt;H&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;I&lt;/p&gt;&lt;p class="teranoptia"&gt;I&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;J&lt;/p&gt;&lt;p class="teranoptia"&gt;J&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;K&lt;/p&gt;&lt;p class="teranoptia"&gt;K&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;L&lt;/p&gt;&lt;p class="teranoptia"&gt;L&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;M&lt;/p&gt;&lt;p class="teranoptia"&gt;M&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;N&lt;/p&gt;&lt;p class="teranoptia"&gt;N&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;O&lt;/p&gt;&lt;p class="teranoptia"&gt;O&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;P&lt;/p&gt;&lt;p class="teranoptia"&gt;P&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;Q&lt;/p&gt;&lt;p class="teranoptia"&gt;Q&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;R&lt;/p&gt;&lt;p class="teranoptia"&gt;R&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;S&lt;/p&gt;&lt;p class="teranoptia"&gt;S&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;T&lt;/p&gt;&lt;p class="teranoptia"&gt;T&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;U&lt;/p&gt;&lt;p class="teranoptia"&gt;U&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;V&lt;/p&gt;&lt;p class="teranoptia"&gt;V&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;W&lt;/p&gt;&lt;p class="teranoptia"&gt;W&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;X&lt;/p&gt;&lt;p class="teranoptia"&gt;X&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;·∫ä&lt;/p&gt;&lt;p class="teranoptia"&gt;·∫ä&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;Y&lt;/p&gt;&lt;p class="teranoptia"&gt;Y&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;Z&lt;/p&gt;&lt;p class="teranoptia"&gt;Z&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;≈π&lt;/p&gt;&lt;p class="teranoptia"&gt;≈π&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;≈Ω&lt;/p&gt;&lt;p class="teranoptia"&gt;≈Ω&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;≈ª&lt;/p&gt;&lt;p class="teranoptia"&gt;≈ª&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;a&lt;/p&gt;&lt;p class="teranoptia"&gt;a&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;b&lt;/p&gt;&lt;p class="teranoptia"&gt;b&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;·∏Ö&lt;/p&gt;&lt;p class="teranoptia"&gt;·∏Ö&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;c&lt;/p&gt;&lt;p class="teranoptia"&gt;c&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;d&lt;/p&gt;&lt;p class="teranoptia"&gt;d&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;e&lt;/p&gt;&lt;p class="teranoptia"&gt;e&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;f&lt;/p&gt;&lt;p class="teranoptia"&gt;f&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;g&lt;/p&gt;&lt;p class="teranoptia"&gt;g&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;h&lt;/p&gt;&lt;p class="teranoptia"&gt;h&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;i&lt;/p&gt;&lt;p class="teranoptia"&gt;i&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;j&lt;/p&gt;&lt;p class="teranoptia"&gt;j&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;k&lt;/p&gt;&lt;p class="teranoptia"&gt;k&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;l&lt;/p&gt;&lt;p class="teranoptia"&gt;l&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;m&lt;/p&gt;&lt;p class="teranoptia"&gt;m&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;n&lt;/p&gt;&lt;p class="teranoptia"&gt;n&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;o&lt;/p&gt;&lt;p class="teranoptia"&gt;o&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;p&lt;/p&gt;&lt;p class="teranoptia"&gt;p&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;q&lt;/p&gt;&lt;p class="teranoptia"&gt;q&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;r&lt;/p&gt;&lt;p class="teranoptia"&gt;r&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;s&lt;/p&gt;&lt;p class="teranoptia"&gt;s&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;t&lt;/p&gt;&lt;p class="teranoptia"&gt;t&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;u&lt;/p&gt;&lt;p class="teranoptia"&gt;u&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;v&lt;/p&gt;&lt;p class="teranoptia"&gt;v&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;w&lt;/p&gt;&lt;p class="teranoptia"&gt;w&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;x&lt;/p&gt;&lt;p class="teranoptia"&gt;x&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;y&lt;/p&gt;&lt;p class="teranoptia"&gt;y&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;z&lt;/p&gt;&lt;p class="teranoptia"&gt;z&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;≈∫&lt;/p&gt;&lt;p class="teranoptia"&gt;≈∫&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;≈æ&lt;/p&gt;&lt;p class="teranoptia"&gt;≈æ&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;≈º&lt;/p&gt;&lt;p class="teranoptia"&gt;≈º&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;,&lt;/p&gt;&lt;p class="teranoptia"&gt;,&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;*&lt;/p&gt;&lt;p class="teranoptia"&gt;*&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;(&lt;/p&gt;&lt;p class="teranoptia"&gt;(&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;)&lt;/p&gt;&lt;p class="teranoptia"&gt;)&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;{&lt;/p&gt;&lt;p class="teranoptia"&gt;{&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;}&lt;/p&gt;&lt;p class="teranoptia"&gt;}&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;[&lt;/p&gt;&lt;p class="teranoptia"&gt;[&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;]&lt;/p&gt;&lt;p class="teranoptia"&gt;]&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;‚Äê&lt;/p&gt;&lt;p class="teranoptia"&gt;‚Äê&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;‚Äú&lt;/p&gt;&lt;p class="teranoptia"&gt;‚Äú&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;‚Äù&lt;/p&gt;&lt;p class="teranoptia"&gt;‚Äù&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;‚Äò&lt;/p&gt;&lt;p class="teranoptia"&gt;‚Äò&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;‚Äô&lt;/p&gt;&lt;p class="teranoptia"&gt;‚Äô&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;¬´&lt;/p&gt;&lt;p class="teranoptia"&gt;¬´&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;¬ª&lt;/p&gt;&lt;p class="teranoptia"&gt;¬ª&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;‚Äπ&lt;/p&gt;&lt;p class="teranoptia"&gt;‚Äπ&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;‚Ä∫&lt;/p&gt;&lt;p class="teranoptia"&gt;‚Ä∫&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;$&lt;/p&gt;&lt;p class="teranoptia"&gt;$&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;‚Ç¨&lt;/p&gt;&lt;p class="teranoptia"&gt;‚Ç¨&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;You'll notice that there's a lot you can do with it, from assembling simple creatures:&lt;/p&gt;
&lt;p class="teranoptia"&gt;vTN&lt;/p&gt;

&lt;p&gt;to more complex, multi-line designs:&lt;/p&gt;
&lt;p class="teranoptia"&gt;&lt;wbr&gt; {≈Ω}&lt;/p&gt;
&lt;p class="teranoptia"&gt;F] [Z&lt;/p&gt;

&lt;p&gt;Let's play with it a bit and see how we can put together a few "correct" looking creatures.&lt;/p&gt;
&lt;div class="notice info"&gt;
As you're about to notice, I'm no JavaScript developer. Don't expect high-quality JS in this post.
&lt;/div&gt;

&lt;h2&gt;Mirroring animals&lt;/h2&gt;
&lt;p&gt;To begin with, let's start with a simple function: animal mirroring. The glyphset includes a mirrored version of each non-symmetric glyph, but the mapping is rather arbitrary, so we are going to need a map.&lt;/p&gt;
&lt;p&gt;Here are the pairs: &lt;/p&gt;
&lt;p class="small teranoptia" style="letter-spacing: 5px;"&gt; By Ev Hs Kp Nm Ri Ve Za ≈ª≈∫ Az Cx Fu Ir Lo Ol Sh Wd ≈π≈º vE Dw Gt Jq Mn Pk Qj Tg Uf Xc ·∫ä·∏Ö Yb ≈Ω≈æ bY cX () [] {} &lt;/p&gt;

&lt;h3&gt;Animal mirror&lt;/h3&gt;
&lt;div style="display: flex; gap: 10px;"&gt;
    &lt;input id="original-animal" type="text" class="teranoptia" style="width: 50%; text-align:right;" oninput="mirrorAnimal(this.value);" value="WYZ*p¬ªgh"&gt;&lt;/input&gt;
    &lt;p id="mirrored-animal" class="teranoptia" style="line-height: 50pt;"&gt;ST¬ªK*abd&lt;/p&gt;
&lt;/div&gt;

&lt;script&gt;
const mirrorPairs = {"B": "y",  "y": "B", "E": "v",  "v": "E", "H": "s",  "s": "H", "K": "p",  "p": "K", "N": "m",  "m": "N", "R": "i",  "i": "R", "V": "e",  "e": "V", "Z": "a",  "a": "Z", "≈ª": "≈∫",  "≈∫": "≈ª", "A": "z",  "z": "A", "C": "x",  "x": "C", "F": "u",  "u": "F", "I": "r",  "r": "I", "L": "o",  "o": "L", "O": "l",  "l": "O", "S": "h",  "h": "S", "W": "d",  "d": "W", "≈π": "≈º",  "≈º": "≈π", "v": "E",  "E": "v", "D": "w",  "w": "D", "G": "t",  "t": "G", "J": "q",  "q": "J", "M": "n",  "n": "M", "P": "k",  "k": "P", "Q": "j",  "j": "Q", "T": "g",  "g": "T", "U": "f",  "f": "U", "X": "c",  "c": "X", "·∫ä": "·∏Ö",  "·∏Ö": "·∫ä", "Y": "b",  "b": "Y", "≈Ω": "≈æ",  "≈æ": "≈Ω", "b": "Y",  "Y": "b", "c": "X",  "X": "c", "(": ")",  ")": "(", "[": "]",  "]": "[", "{": "}", "}": "{"};

function mirrorAnimal(original){
    var mirror = '';
    for (i = original.length-1; i &gt;= 0; i--){
        newChar = mirrorPairs[original.charAt(i)];
        if (newChar){
            mirror += newChar;
        } else {
            mirror += original.charAt(i)
        }
        console.log(original, original.charAt(i), mirrorPairs[original.charAt(i)], mirror);
    }
    document.getElementById("mirrored-animal").innerHTML = mirror;
}
&lt;/script&gt;

&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;mirrorPairs&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;B&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;y&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;y&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;B&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;E&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;v&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;v&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;E&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;H&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;s&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;s&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;H&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;K&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;p&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;p&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;K&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;N&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;m&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;m&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;N&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;R&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;i&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;i&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;R&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;V&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;e&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;e&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;V&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Z&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Z&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;≈ª&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;≈∫&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;≈∫&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;≈ª&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;A&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;z&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;z&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;A&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;C&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;x&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;x&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;C&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;F&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;u&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;u&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;F&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;I&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;r&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;r&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;I&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;L&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;o&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;o&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;L&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;O&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;l&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;l&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;O&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;S&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;h&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;h&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;S&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;W&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;d&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;d&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;W&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;≈π&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;≈º&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;≈º&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;≈π&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;v&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;E&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;E&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;v&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;D&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;w&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;w&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;D&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;G&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;t&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;t&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;G&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;J&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;q&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;q&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;J&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;M&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;n&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;M&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;P&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;k&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;k&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;P&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Q&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;j&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;j&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Q&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;T&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;g&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;g&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;T&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;U&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;f&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;f&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;U&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;X&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;c&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;c&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;X&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;·∫ä&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;·∏Ö&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;·∏Ö&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;·∫ä&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Y&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;b&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;b&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Y&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;≈Ω&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;≈æ&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;≈æ&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;≈Ω&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;b&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Y&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Y&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;b&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;c&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;X&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;X&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;c&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;(&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;)&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;(&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;[&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;]&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;]&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;[&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;{&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;}&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;}&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;{&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;};&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;mirrorAnimal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;original&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;mirror&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;original&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;length&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;newChar&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;mirrorPairs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;original&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;charAt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)];&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;newChar&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="nx"&gt;mirror&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;newChar&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="nx"&gt;mirror&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;original&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;charAt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;mirror&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Random animal generation&lt;/h2&gt;
&lt;p&gt;While it's fun to build complicated animals this way, you'll notice something: it's pretty hard to make them come out right by simply typing something. Most of the time you need quite careful planning. In addition there's almost no meaningful (English) word that corresponds to a well-defined creature. Very often the characters don't match, creating a sequence of "chopped" creatures.&lt;/p&gt;
&lt;p&gt;For example, "Hello" becomes:&lt;/p&gt;
&lt;p class="teranoptia"&gt;Hello&lt;/p&gt;

&lt;p&gt;This is a problem if we want to make a parametric or random creature generator, because most of the random strings won't look good. &lt;/p&gt;
&lt;h3&gt;Naive random generator&lt;/h3&gt;
&lt;div style="display: flex; gap: 10px;"&gt;
    Characters to generate:
    &lt;input id="naive-glyph-count" type="number" value=10&gt;&lt;/input&gt;
    &lt;button onclick="generateNaive(document.getElementById('naive-glyph-count').value);"&gt;Generate!&lt;/button&gt;
&lt;/div&gt;

&lt;p id="naive-generated-animal" class="teranoptia" style="line-height: 50pt;"&gt;n]Zgame≈∫)‚Äê&lt;/p&gt;

&lt;script&gt;
const validChars = "ABCDEFGHIJKLMNOPQRSTUVWX·∫äYZ≈π≈Ω≈ªab·∏Öcdefghijklmnopqrstuvwxyz≈∫≈æ≈º,*(){}[]‚Äê‚Äú‚Äù¬´¬ª$"; //‚Äò‚Äô‚Äπ‚Ä∫‚Ç¨

function randomFrom(list){
    return list[Math.floor(Math.random() * list.length)];
}

function generateNaive(value){
    var newAnimal = '';
    for (var i = 0; i &lt; value; i++) {
        newAnimal += randomFrom(validChars);
    }

    // Line break trick - helps with wrapping
    const animalFragments = newAnimal.split('');
    newAnimal = animalFragments.join("&lt;wbr&gt;");

    document.getElementById("naive-generated-animal").innerHTML = newAnimal;
}
generateNaive(document.getElementById('naive-glyph-count').value);

&lt;/script&gt;

&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;validChars&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ABCDEFGHIJKLMNOPQRSTUVWX·∫äYZ≈π≈Ω≈ªab·∏Öcdefghijklmnopqrstuvwxyz≈∫≈æ≈º,*(){}[]‚Äê‚Äú‚Äù¬´¬ª$&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;// ‚Äò‚Äô‚Äπ‚Ä∫‚Ç¨ excluded because they&amp;#39;re mostly vertical&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;randomFrom&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;list&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;list&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;floor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;list&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;length&lt;/span&gt;&lt;span class="p"&gt;)];&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;generateNaive&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;newAnimal&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;newAnimal&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;randomFrom&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;validChars&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;newAnimal&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Can we do better than this?&lt;/p&gt;
&lt;h2&gt;Generating "good" animals&lt;/h2&gt;
&lt;p&gt;There are many ways to define "good" or "well-formed" creatures. One of the first rules we can introduce is that we don't want chopped body parts to float alone. &lt;/p&gt;
&lt;p&gt;Translating it into a rule we can implement: a character that is "open" on the right must be followed by a character that is open on the left, and a character that is &lt;em&gt;not&lt;/em&gt; open on the right must be followed by another character that is &lt;em&gt;not&lt;/em&gt; open on the left.&lt;/p&gt;
&lt;p&gt;For example, &lt;span class="small teranoptia"&gt;A&lt;/span&gt; may be followed by &lt;span class="small teranoptia"&gt;B&lt;/span&gt; to make &lt;span class="small teranoptia"&gt;AB&lt;/span&gt;, but &lt;span class="small teranoptia"&gt;A&lt;/span&gt; cannot be followed by &lt;span class="small teranoptia"&gt;C&lt;/span&gt; to make &lt;span class="small teranoptia"&gt;AC&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;In the same way, &lt;span class="small teranoptia"&gt;Z&lt;/span&gt; may be followed by &lt;span class="small teranoptia"&gt;A&lt;/span&gt; to make &lt;span class="small teranoptia"&gt;ZA&lt;/span&gt;, but &lt;span class="small teranoptia"&gt;Z&lt;/span&gt; cannot be followed by &lt;span class="small teranoptia"&gt;≈º&lt;/span&gt; to make &lt;span class="small teranoptia"&gt;Z≈º&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;This way we will get rid of all those "chopped" monsters that make up most of the randomly generated string.&lt;/p&gt;
&lt;p&gt;To summarize, the rules we have to implement are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Any character that is open on the right must be followed by another character that is open on the left.&lt;/li&gt;
&lt;li&gt;Any character that is closed on the right must be followed by another character that is closed on the left.&lt;/li&gt;
&lt;li&gt;The first character must not be open on the left.&lt;/li&gt;
&lt;li&gt;The last character must not be open on the right.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Non-chopped animals generator&lt;/h3&gt;
&lt;div style="display: flex; gap: 10px;"&gt;
    Characters to generate:
    &lt;input id="nochop-glyph-count" type="number" value=10&gt;&lt;/input&gt;
    &lt;button onclick="generateNoChop(document.getElementById('nochop-glyph-count').value);"&gt;Generate!&lt;/button&gt;
&lt;/div&gt;

&lt;p id="nochop-generated-animal" class="teranoptia" style="line-height: 50pt;"&gt;suSHebQ¬´EIl&lt;/p&gt;

&lt;script&gt;
const charsOpenOnTheRightOnly = "yvspmiea≈∫ACFILOSW≈π({[";
const charsOpenOnTheLeftOnly =  "BEHKNRVZ≈ªzxurolhd≈º)]}";
const charsOpenOnBothSides = "DGJMPQTUX·∫äY≈Ωbcwtqnkjgfc·∏Öb≈æYX¬´¬ª";
const charsOpenOnNoSides = ",*-‚Äú‚Äù";

const charsOpenOnTheRight = charsOpenOnTheRightOnly + charsOpenOnBothSides;
const charsOpenOnTheLeft = charsOpenOnTheLeftOnly + charsOpenOnBothSides;
const validInitialChars = charsOpenOnTheRightOnly + charsOpenOnNoSides;

function generateNoChop(value){
    document.getElementById("nochop-generated-animal").innerHTML = "";
    var newAnimal = '' + randomFrom(validInitialChars);
    for (var i = 0; i &lt; value-1; i++) {
        if (charsOpenOnTheRight.indexOf(newAnimal[i]) &gt; -1){
            newAnimal += randomFrom(charsOpenOnTheLeft);

        } else if (charsOpenOnTheLeftOnly.indexOf(newAnimal[i]) &gt; -1){
            newAnimal += randomFrom(charsOpenOnTheRightOnly);

        } else if (charsOpenOnNoSides.indexOf(newAnimal[i]) &gt; -1){
            newAnimal += randomFrom(validInitialChars);
        }
    }
    // Final character
    if (charsOpenOnTheRight.indexOf(newAnimal[i]) &gt; -1){
        newAnimal += randomFrom(charsOpenOnTheLeftOnly);
    } else {
        newAnimal += randomFrom(charsOpenOnNoSides);
    }
    document.getElementById("nochop-generated-animal").innerHTML = makeBreakable(newAnimal);
}
generateNoChop(document.getElementById("nochop-glyph-count").value);

&lt;/script&gt;

&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;charsOpenOnTheRightOnly&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;yvspmiea≈∫ACFILOSW≈π({[&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;charsOpenOnTheLeftOnly&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;BEHKNRVZ≈ªzxurolhd≈º)]}&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;charsOpenOnBothSides&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;DGJMPQTUX·∫äY≈Ωbcwtqnkjgfc·∏Öb≈æYX¬´¬ª&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;charsOpenOnNoSides&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;,*-‚Äú‚Äù&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;charsOpenOnTheRight&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;charsOpenOnTheRightOnly&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;charsOpenOnBothSides&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;charsOpenOnTheLeft&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;charsOpenOnTheLeftOnly&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;charsOpenOnBothSides&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;validInitialChars&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;charsOpenOnTheRightOnly&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;charsOpenOnNoSides&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;generateNoChop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;newAnimal&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;randomFrom&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;validInitialChars&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;charsOpenOnTheRight&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;indexOf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;newAnimal&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="nx"&gt;newAnimal&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;randomFrom&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;charsOpenOnTheLeft&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;charsOpenOnTheLeftOnly&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;indexOf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;newAnimal&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="nx"&gt;newAnimal&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;randomFrom&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;charsOpenOnTheRightOnly&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;charsOpenOnNoSides&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;indexOf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;newAnimal&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="nx"&gt;newAnimal&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;randomFrom&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;validInitialChars&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="c1"&gt;// Final character&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;charsOpenOnTheRight&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;indexOf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;newAnimal&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;newAnimal&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;randomFrom&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;charsOpenOnTheLeftOnly&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;newAnimal&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;randomFrom&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;charsOpenOnNoSides&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;newAnimal&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The resulting animals are already quite better!&lt;/p&gt;
&lt;p&gt;There are still a few things we may want to fix. For example, some animals end up being just a pair of heads (such as &lt;span class="small teranoptia"&gt;sN&lt;/span&gt;); others instead have their bodies oriented in the wrong direction (like &lt;span class="small teranoptia"&gt;IgV&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Let's try to get rid of those too.&lt;/p&gt;
&lt;p&gt;The trick here is to separate the characters into two groups: elements that are "facing left", elements that are "facing right", and symmetric ones. At this point, it's convenient to call them "heads", "bodies" and "tails" to make the code more understandable, like the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Right heads: &lt;span class="small teranoptia"&gt;BEHKNRVZ≈ª&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Left heads: &lt;span class="small teranoptia"&gt;yvspmiea≈∫&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Right tails: &lt;span class="small teranoptia"&gt;ACFILOSW≈πv&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Left tails: &lt;span class="small teranoptia"&gt;zxurolhd≈ºE&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Right bodies: &lt;span class="small teranoptia" style="letter-spacing: 5px;"&gt;DGJMPQTU·∫ä≈Ω&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Left bodies: &lt;span class="small teranoptia" style="letter-spacing: 5px;"&gt;wtqnkjgf·∏Ö≈æ&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Entering hole: &lt;span class="small teranoptia" style="letter-spacing: 5px;"&gt;)]}&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Exiting hole: &lt;span class="small teranoptia" style="letter-spacing: 5px;"&gt;([{&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bounce &amp;amp; symmetric bodies: &lt;span class="small teranoptia" style="letter-spacing: 5px;"&gt;¬´¬ª$bcXY&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Singletons: &lt;span class="small teranoptia" style="letter-spacing: 5px;"&gt;,*-&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let's put this all together!&lt;/p&gt;
&lt;h3&gt;Oriented animals generator&lt;/h3&gt;
&lt;div style="display: flex; gap: 10px;"&gt;
    Characters to generate:
    &lt;input id="oriented-glyph-count" type="number" value=10&gt;&lt;/input&gt;
    &lt;button onclick="generateOriented(document.getElementById('oriented-glyph-count').value);"&gt;Generate!&lt;/button&gt;
&lt;/div&gt;

&lt;p id="oriented-generated-animal" class="teranoptia" style="line-height: 50pt;"&gt;suSHebQ¬´EIl&lt;/p&gt;

&lt;script&gt;

&lt;p&gt;const rightAnimalHeads = "BEHKNRVZ≈ª";
const leftAnimalHeads = "yvspmiea≈∫";
const rightAnimalTails = "ACFILOSW≈πv";
const leftAnimalTails = "zxurolhd≈ºE";
const rightAnimalBodies = "DGJMPQTU·∫ä≈Ω";
const leftAnimalBodies = "wtqnkjgf·∏Ö≈æ";
const singletons = ",*‚Äê";
const exitingHole = "([{";
const enteringHole = ")]}";
const bounce = "¬´¬ª$bcXY";&lt;/p&gt;
&lt;p&gt;const validStarts = leftAnimalHeads + rightAnimalTails + exitingHole;
const validSuccessors = {
    [exitingHole + bounce]: rightAnimalHeads + rightAnimalBodies + leftAnimalBodies + leftAnimalTails + enteringHole + bounce,
    [enteringHole]: rightAnimalTails + leftAnimalHeads + exitingHole + singletons,
    [rightAnimalHeads + leftAnimalTails + singletons]: rightAnimalTails + leftAnimalHeads + exitingHole + singletons,
    [leftAnimalHeads]: leftAnimalBodies + leftAnimalBodies + leftAnimalBodies + leftAnimalTails + enteringHole + bounce,
    [rightAnimalTails]: rightAnimalBodies + rightAnimalBodies + rightAnimalBodies + rightAnimalHeads + enteringHole + bounce,
    [rightAnimalBodies]: rightAnimalBodies + rightAnimalBodies + rightAnimalBodies + rightAnimalHeads + enteringHole + bounce,
    [leftAnimalBodies]: leftAnimalBodies + leftAnimalBodies + leftAnimalBodies + leftAnimalTails + enteringHole + bounce,
};
const validEnds = {
    [exitingHole + bounce]: leftAnimalTails + rightAnimalHeads + enteringHole,
    [rightAnimalHeads + leftAnimalTails + enteringHole]: singletons,
    [leftAnimalHeads]: leftAnimalTails + enteringHole,
    [rightAnimalTails]: rightAnimalHeads + enteringHole,
    [rightAnimalBodies]: rightAnimalHeads,
    [leftAnimalBodies]: leftAnimalTails,
};&lt;/p&gt;
&lt;p&gt;function generateOriented(value){&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;var newAnimal = &amp;#39;&amp;#39; + randomFrom(validStarts);
for (var i = 0; i &amp;lt; value-1; i++) {
    last_char = newAnimal[i-1];
    for (const [predecessor, successor] of Object.entries(validSuccessors)) {
        if (predecessor.indexOf(last_char) &amp;gt; -1){
            newAnimal += randomFrom(successor);
            break;
        }
    }
}
last_char = newAnimal[i-1];
for (const [predecessor, successor] of Object.entries(validEnds)) {
    if (predecessor.indexOf(last_char) &amp;gt; -1){
        newAnimal += randomFrom(successor);
        break;
    }
}
document.getElementById(&amp;quot;oriented-generated-animal&amp;quot;).innerHTML = makeBreakable(newAnimal);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;}
generateOriented(document.getElementById("oriented-glyph-count").value);&lt;/p&gt;

&lt;/script&gt;

&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;rightAnimalHeads&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;BEHKNRVZ≈ª&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;leftAnimalHeads&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;yvspmiea≈∫&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;rightAnimalTails&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ACFILOSW≈πv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;leftAnimalTails&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;zxurolhd≈ºE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;rightAnimalBodies&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;DGJMPQTU·∫ä≈Ω&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;leftAnimalBodies&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;wtqnkjgf·∏Ö≈æ&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;singletons&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;,*‚Äê&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;exitingHole&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;([{&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;enteringHole&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;)]}&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;bounce&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;¬´¬ª$bcXY&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;validStarts&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;leftAnimalHeads&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;rightAnimalTails&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;exitingHole&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;validSuccessors&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;exitingHole&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;bounce&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;rightAnimalHeads&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;rightAnimalBodies&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;leftAnimalBodies&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;leftAnimalTails&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;enteringHole&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;bounce&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;enteringHole&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;rightAnimalTails&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;leftAnimalHeads&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;exitingHole&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;singletons&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;rightAnimalHeads&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;leftAnimalTails&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;singletons&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;rightAnimalTails&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;leftAnimalHeads&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;exitingHole&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;singletons&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;leftAnimalHeads&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;leftAnimalBodies&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;leftAnimalBodies&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;leftAnimalBodies&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;leftAnimalTails&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;enteringHole&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;bounce&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;rightAnimalTails&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;rightAnimalBodies&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;rightAnimalBodies&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;rightAnimalBodies&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;rightAnimalHeads&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;enteringHole&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;bounce&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;rightAnimalBodies&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;rightAnimalBodies&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;rightAnimalBodies&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;rightAnimalBodies&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;rightAnimalHeads&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;enteringHole&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;bounce&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;leftAnimalBodies&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;leftAnimalBodies&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;leftAnimalBodies&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;leftAnimalBodies&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;leftAnimalTails&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;enteringHole&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;bounce&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;};&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;validEnds&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;exitingHole&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;bounce&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;leftAnimalTails&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;rightAnimalHeads&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;enteringHole&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;rightAnimalHeads&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;leftAnimalTails&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;enteringHole&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;singletons&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;leftAnimalHeads&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;leftAnimalTails&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;enteringHole&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;rightAnimalTails&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;rightAnimalHeads&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;enteringHole&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;rightAnimalBodies&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;rightAnimalHeads&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;leftAnimalBodies&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;leftAnimalTails&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;};&lt;/span&gt;

&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;generateOriented&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;newAnimal&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;randomFrom&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;validStarts&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;last_char&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;newAnimal&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;predecessor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;successor&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;Object&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;entries&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;validSuccessors&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;predecessor&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;indexOf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;last_char&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="nx"&gt;newAnimal&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;randomFrom&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;successor&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;last_char&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;newAnimal&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;const&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;predecessor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;successor&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;Object&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;entries&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;validEnds&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;predecessor&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;indexOf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;last_char&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="nx"&gt;newAnimal&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;randomFrom&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;successor&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;newAnimal&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;A regular grammar&lt;/h2&gt;
&lt;p&gt;Let's move up a level now.&lt;/p&gt;
&lt;p&gt;What we've defined up to this point is a set of rules that, given a string, determine what characters are allowed next. This is called a &lt;a href="https://en.wikipedia.org/wiki/Formal_grammar" target="_blank" rel="noopener noreferrer"&gt;&lt;strong&gt;formal grammar&lt;/strong&gt;&lt;/a&gt; in Computer Science.&lt;/p&gt;
&lt;p&gt;A grammar is defined primarily by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;an &lt;strong&gt;alphabet&lt;/strong&gt; of symbols (our Teranoptia font).&lt;/li&gt;
&lt;li&gt;a set of &lt;strong&gt;starting characters&lt;/strong&gt;: all the characters that can be used at the start of the string (such as &lt;span class="small teranoptia"&gt;a&lt;/span&gt; or &lt;span class="small teranoptia"&gt;*&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;a set of &lt;strong&gt;terminating character&lt;/strong&gt;: all the characters that can be used to terminate the string (such as &lt;span class="small teranoptia"&gt;d&lt;/span&gt; or &lt;span class="small teranoptia"&gt;-&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;a set of &lt;strong&gt;production rules&lt;/strong&gt;: the rules needed to generate valid strings in that grammar. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In our case, we're looking for a grammar that defines "well formed" animals. For example, our production rules might look like this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;S (the start of the string) ‚Üí a (&lt;span class="small teranoptia"&gt;a&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;a (&lt;span class="small teranoptia"&gt;a&lt;/span&gt;) ‚Üí ad (&lt;span class="small teranoptia"&gt;ad&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;a (&lt;span class="small teranoptia"&gt;a&lt;/span&gt;) ‚Üí ab (&lt;span class="small teranoptia"&gt;ab&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;b (&lt;span class="small teranoptia"&gt;b&lt;/span&gt;) ‚Üí bb (&lt;span class="small teranoptia"&gt;bb&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;b (&lt;span class="small teranoptia"&gt;b&lt;/span&gt;) ‚Üí bd (&lt;span class="small teranoptia"&gt;bd&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;d (&lt;span class="small teranoptia"&gt;d&lt;/span&gt;) ‚Üí E (the end of the string)&lt;/li&gt;
&lt;li&gt;, (&lt;span class="small teranoptia"&gt;,&lt;/span&gt;) ‚Üí E (the end of the string)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and so on. Each combination would have its own rule.&lt;/p&gt;
&lt;p&gt;There are three main types of grammars according to Chomsky's hierarchy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Regular grammars&lt;/strong&gt;: in all rules, the left-hand side is only a single nonterminal symbol and right-hand side may be the empty string, or a single terminal symbol, or a single terminal symbol followed by a nonterminal symbol, but nothing else. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Context-free grammars&lt;/strong&gt;: in all rules, the left-hand side of each production rule consists of only a single nonterminal symbol, while the right-hand side may contain any number of terminal and non-terminal symbols.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Context-sensitive grammars&lt;/strong&gt;: rules can contain many terminal and non-terminal characters on both sides.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In our case, all the production rules look very much like the examples we defined above: one character on the left-hand side, at most two on the right-hand side. This means we're dealing with a regular grammar. And this is good news, because it means that this language can be encoded into a &lt;strong&gt;regular expression&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;Building the regex&lt;/h2&gt;
&lt;p&gt;Regular expressions are a very powerful tool, one that needs to be used with care. They're best used for string validation: given an arbitrary string, they are going to check whether it respects the grammar, i.e. whether the string it could have been generated by applying the rules above.&lt;/p&gt;
&lt;p&gt;Having a regex for our Teranoptia animals will allow us to search for valid animals in long lists of stroings, for example an English dictionary. Such a search would have been prohibitively expensive without a regular expression: using one, while still quite costly, is orders of magnitude more efficient.&lt;/p&gt;
&lt;p&gt;In order to build this complex regex, let's start with a very limited example: a regex that matches left-facing snakes.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;^(a(b|c|X|Y)*d)+$
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This regex is fairly straightforward: the string must start with a (&lt;span class="small teranoptia"&gt;a&lt;/span&gt;), can contain any number of b (&lt;span class="small teranoptia"&gt;b&lt;/span&gt;), c (&lt;span class="small teranoptia"&gt;c&lt;/span&gt;), X (&lt;span class="small teranoptia"&gt;X&lt;/span&gt;) and Y (&lt;span class="small teranoptia"&gt;Y&lt;/span&gt;), and must end with d (&lt;span class="small teranoptia"&gt;d&lt;/span&gt;). While we're at it, let's add a + to the end, meaning that this pattern can repeat multiple times: the string will simply contain many snakes.&lt;/p&gt;
&lt;h3&gt;Left-facing snakes regex&lt;/h3&gt;
&lt;div style="display: flex; gap: 10px;"&gt;
    &lt;input id="left-facing-snakes-input" type="string" class="teranoptia" value="abd" oninput="validateLeftFacingSnake();" style="width: 100%;"&gt;&lt;/input&gt;
    &lt;p id="left-facing-snakes-result"&gt;Valid&lt;/p&gt;
&lt;/div&gt;

&lt;script&gt;
var leftFacingSnake = new RegExp("^(a(b|c|X|Y)*d)+$");

function validateLeftFacingSnake(){
    const candidate = document.getElementById('left-facing-snakes-input').value;
    if (leftFacingSnake.test(candidate)){
        document.getElementById('left-facing-snakes-input').style.color = "green";
        document.getElementById('left-facing-snakes-result').innerHTML = "Valid!";
    } else {
        document.getElementById('left-facing-snakes-input').style.color = "red";
        document.getElementById('left-facing-snakes-result').innerHTML = "NOT valid!";
    }
}
validateLeftFacingSnake()
&lt;/script&gt;

&lt;p&gt;What would it take to extend it to snakes that face either side? Luckily, snake bodies are symmetrical, so we can take advantage of that and write:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;^((a|W)(b|c|X|Y)*(d|Z))+$
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Naive snakes&lt;/h3&gt;
&lt;div style="display: flex; gap: 10px;"&gt;
    &lt;input id="naive-snakes-input" type="string" class="teranoptia" value="abdWXZ" oninput="validateNaiveSnake();" style="width: 100%;"&gt;&lt;/input&gt;
    &lt;p id="naive-snakes-result"&gt;Valid&lt;/p&gt;
&lt;/div&gt;

&lt;script&gt;
var naiveSnake = new RegExp("^((a|W)(b|c|X|Y)*(d|Z))+$");

function validateNaiveSnake(){
    const candidate = document.getElementById('naive-snakes-input').value;
    if (naiveSnake.test(candidate)){
        document.getElementById('naive-snakes-input').style.color = "green";
        document.getElementById('naive-snakes-result').innerHTML = "Valid!";
    } else {
        document.getElementById('naive-snakes-input').style.color = "red";
        document.getElementById('naive-snakes-result').innerHTML = "NOT valid!";
    }
}
validateNaiveSnake();
&lt;/script&gt;

&lt;p&gt;That looks super-promising until we realize that there's a problem: this "snake" &lt;span class="small teranoptia"&gt;aZ&lt;/span&gt; also matches the regex. To generate well-formed animals we need to keep heads and tails separate. In the regex, it would look like:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;^(
    (a)(b|c|X|Y)*(d) |
    (W)(b|c|X|Y)*(Z)
)+$
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Correct snakes&lt;/h3&gt;
&lt;div style="display: flex; gap: 10px;"&gt;
    &lt;input id="correct-snakes-input" type="string" class="teranoptia" value="abdWXZ" oninput="validateCorrectSnake();" style="width: 100%;"&gt;&lt;/input&gt;
    &lt;p id="correct-snakes-result"&gt;Valid&lt;/p&gt;
&lt;/div&gt;

&lt;script&gt;
var correctSnake = new RegExp("^(((a)(b|c|X|Y)*(d))|((W)(b|c|X|Y)*(Z)))+$");

function validateCorrectSnake(){
    const candidate = document.getElementById('correct-snakes-input').value;
    if (correctSnake.test(candidate)){
        document.getElementById('correct-snakes-input').style.color = "green";
        document.getElementById('correct-snakes-result').innerHTML = "Valid!";
    } else {
        document.getElementById('correct-snakes-input').style.color = "red";
        document.getElementById('correct-snakes-result').innerHTML = "NOT valid!";
    }
}
validateCorrectSnake()
&lt;/script&gt;

&lt;p&gt;Once here, building the rest of the regex is simply matter of adding the correct characters to each group. We're gonna trade some extra characters for an easier structure by duplicating the symmetric characters when needed.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;^(
    // Left-facing animals
    (
        y|v|s|p|m|i|e|a|≈∫|(|[|{   // Left heads &amp;amp; exiting holes
    )(
        w|t|q|n|k|j|g|f|·∏Ö|≈æ|X|Y|b|c|$|¬´|¬ª  // Left &amp;amp; symmetric bodies
    )*(
        z|x|u|r|o|l|h|d|≈º|E|)|]|}  // Left tails &amp;amp; entering holes
    ) |

    // Right facing animals
    (
        A|C|F|I|L|O|S|W|≈π|v|(|[|{   // right tails &amp;amp; exiting holes
    )(
        D|G|J|M|P|Q|T|U|·∫ä|≈Ω|b|c|X|Y|$|¬´|¬ª  // right &amp;amp; symmetric bodies  
    )*(
        B|E|H|K|N|R|V|Z|≈ª|)|]|}   // right heads &amp;amp; entering holes
    ) |

    // Singletons
    (,|-|*)
)+$
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Well-formed animals regex&lt;/h3&gt;
&lt;div style="display: flex; gap: 10px;"&gt;
    &lt;input id="correct-animal-input" type="string" class="teranoptia" value="abu*W¬´XZ" oninput="validateCorrectAnimal();" style="width: 100%;"&gt;&lt;/input&gt;
    &lt;p id="correct-animal-result"&gt;Valid&lt;/p&gt;
&lt;/div&gt;

&lt;script&gt;
var correctAnimal = new RegExp("^((y|v|s|p|m|i|e|a|≈∫|\\(|\\[|\\{)(w|t|q|n|k|j|g|f|·∏Ö|≈æ|b|c|X|Y|\\$|¬´|¬ª)*(z|x|u|r|o|l|h|d|≈º|E|\\)|\\]|\\})|(A|C|F|I|L|O|S|W|≈π|v|\\(|\\[|\\{)(D|G|J|M|P|Q|T|U|·∫ä|≈Ω|b|c|X|Y|\\$|¬´|¬ª)*(B|E|H|K|N|R|V|Z|≈ª|\\)|\\]|\\})|(-|\\*|,))+$");

function validateCorrectAnimal(){
    const candidate = document.getElementById('correct-animal-input').value;
    if (correctAnimal.test(candidate)){
        document.getElementById('correct-animal-input').style.color = "green";
        document.getElementById('correct-animal-result').innerHTML = "Valid!";
    } else {
        document.getElementById('correct-animal-input').style.color = "red";
        document.getElementById('correct-animal-result').innerHTML = "NOT valid!";
    }
}
validateCorrectAnimal();
&lt;/script&gt;

&lt;p&gt;If you play with the above regex, you'll notice a slight discrepancy with what our well-formed animal generator creates. The generator can create "double-headed" monsters where a symmetric body part is inserted, like &lt;span class="small teranoptia"&gt;a¬´Z&lt;/span&gt;. However, the regex does not allow it. Extending it to account for these scenarios would make it even more unreadable, so this is left as an exercise for the reader.&lt;/p&gt;
&lt;h2&gt;Searching for "monstrous" words&lt;/h2&gt;
&lt;p&gt;Let's put the regex to use! There must be some English words that match the regex, right?&lt;/p&gt;
&lt;p&gt;Google helpfully compiled a text file with the most frequent 10.000 English words by frequency. Let's load it up and match every line with our brand-new regex. Unfortunately Teranoptia is case-sensitive and uses quite a few odd letters and special characters, so it's unlikely we're going to find many interesting creatures. Still worth an attempt.&lt;/p&gt;
&lt;h3&gt;Monster search&lt;/h3&gt;
&lt;div style="display: flex; gap: 10px;"&gt;
    &lt;input id="file-url" type="url" value="https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english.txt" style="width: 100%;"&gt;&lt;/input&gt;
    &lt;button onclick="searchFile();"&gt;Search&lt;/button&gt;
&lt;/div&gt;
&lt;p id="search-result"&gt;&lt;/p&gt;
&lt;div id="words-found"&gt;&lt;/div&gt;

&lt;script&gt;
var correctAnimal = new RegExp("^((y|v|s|p|m|i|e|a|≈∫|\\(|\\[|\\{)(w|t|q|n|k|j|g|f|·∏Ö|≈æ|b|c|X|Y|\\$|¬´|¬ª)*(z|x|u|r|o|l|h|d|≈º|E|\\)|\\]|\\})|(A|C|F|I|L|O|S|W|≈π|v|\\(|\\[|\\{)(D|G|J|M|P|Q|T|U|·∫ä|≈Ω|b|c|X|Y|\\$|¬´|¬ª)*(B|E|H|K|N|R|V|Z|≈ª|\\)|\\]|\\})|(-|\\*|,))+$");

function searchFile(){
    document.getElementById('search-result').innerHTML = "Loading...";

    fetch(document.getElementById('file-url').value)
    .then((response) =&gt; {
        if (!response.ok) {
            throw new Error(`HTTP error: ${response.status}`);
        }
        return response.text();
    })
    .then((text) =&gt; {
        lines = text.split('\n');
        counter = 0;

        for (i = 0; i &lt; lines.length; i++){
            var candidate = lines[i];
            document.getElementById('search-result').innerHTML = "Checking " + candidate;
            if (correctAnimal.test(candidate)){
                document.getElementById('words-found').innerHTML += "&lt;p&gt;"+candidate+"&lt;span class='teranoptia'&gt; "+candidate+"&lt;/span&gt;&lt;/p&gt;";
                counter++;
            }
        }
        document.getElementById('search-result').innerHTML = "Done! Found "+ counter +" animals over "+lines.length+" words tested.";        
    })
    .catch((error) =&gt; {
        document.getElementById('search-result').innerHTML = "Failed to fetch file :(";
    });
}
&lt;/script&gt;

&lt;p&gt;Go ahead and put your own vocabulary file to see if your language contains more animals!&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post I've just put together a few exercises for fun, but these tools can be great for teaching purposes: the output is very easy to validate visually, and the grammar involved, while not trivial, is not as complex as natural language or as dry as numerical sequences. If you need something to keep your students engaged, this might be a simple trick to help them visualize the concepts better.&lt;/p&gt;
&lt;p&gt;On my side, I think I'm going to use these neat little monsters as weird &lt;a href="https://en.wikipedia.org/wiki/Fleuron_(typography)" target="_blank" rel="noopener noreferrer"&gt;fleurons&lt;/a&gt; :)&lt;/p&gt;
&lt;p class="fleuron"&gt;&lt;a href="/posts/2024-05-06-teranoptia/"&gt;su&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Download Teranoptia at this link: https://www.tunera.xyz/fonts/teranoptia/&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>