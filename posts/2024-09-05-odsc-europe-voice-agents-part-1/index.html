<!DOCTYPE html>
<html lang="en">
<head>
  <title>Building Reliable Voice Bots with Open Source Tools - Part 1 · Sara Zan</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="color-scheme" content="light dark">
  <meta name="author" content="Sara Zan">
  <meta name="description" content="Sara Zan's Blog">
  <meta name="keywords" content="blog,developer,personal,python,llm,nlp,swe,software-engineering,open-source,ai,genai">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Building Reliable Voice Bots with Open Source Tools - Part 1 · Sara Zan">
  <meta name="twitter:description" content="Sara Zan's Blog">
  <meta property="og:url" content="https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/">
  <meta property="og:site_name" content="Sara Zan">
  <meta property="og:title" content="Building Reliable Voice Bots with Open Source Tools - Part 1">
  <meta property="og:description" content="Sara Zan's Blog">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
  <meta name="msvalidate.01" content="CD2BB9B57B16AF914327870432D856C1" />
  <meta name="yandex-verification" content="a886d3d5d2b57cb5" />
    <meta name="image" content="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/cover.png">
  <meta name="og:image" content="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/cover.png">
  <meta name="twitter:image" content="https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/cover.png">
  <link rel="canonical" href="https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/">
  <link rel="stylesheet" href="/css/style.css" media="screen">
  <link rel="icon" type="image/svg+xml" href="/assets/avatar/avatar.svg" sizes="any">
  <link rel="icon" type="image/png" href="/assets/avatar/avatar.png" sizes="32x32">
  <link rel="apple-touch-icon" href="/assets/avatar/avatar.png">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+HK:wght@200..900&family=Noto+Serif+Hebrew:wght@100..900&family=Noto+Naskh+Arabic:wght@400..700&family=Noto+Serif+JP&family=Noto+Serif+KR&family=Noto+Serif+SC&family=Noto+Serif+TC&family=Noto+Serif+Thai:wght@100..900&family=Noto+Serif:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
  <script data-goatcounter="https://zansaradev.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
</head>

<body>

  <!-- Theme toggle -->
  <input type="checkbox" id="theme-toggle" hidden>
  <label for="theme-toggle">
    <svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
      <circle cx="12" cy="12" r="10" fill="currentColor" opacity="0.3"/>
      <path d="M12 2 A10 10 0 0 1 12 22 Z" fill="currentColor"/>
    </svg>
  </label>

  <!-- Load theme immediately to avoid flash -->
  <script>
    (function() {
      const themeToggle = document.getElementById('theme-toggle');
      const savedTheme = localStorage.getItem('theme');
      if (savedTheme === 'dark') {
        themeToggle.checked = true;
      }
    })();
  </script>

  <main>

    <nav style="padding: 20px 0 10px 0; display: flex; flex-direction: column; align-items: center; gap: 10px; border-bottom: 1px solid var(--border);">
  <a href="/" style="color: var(--text); text-decoration: none; font-size: 25px; margin: 10px 0;">
    <img src="/assets/avatar/avatar.svg" style="width: 1em; height: 1em; margin-right: 5px; margin-bottom: -2px;">
    Sara Zan's Blog
  </a>
  <div style="display: flex; flex-flow: wrap; gap: 0; justify-content: center;">
    <a href="/about" style="color: var(--text); text-decoration: none; margin: 0 10px;">About</a>
<a href="/posts/" style="color: var(--text); text-decoration: none; margin: 0 10px;">Posts</a>
<a href="/projects/" style="color: var(--text); text-decoration: none; margin: 0 10px;">Projects</a>
<a href="/publications/" style="color: var(--text); text-decoration: none; margin: 0 10px;">Publications</a>
<a href="/talks/" style="color: var(--text); text-decoration: none; margin: 0 10px;">Talks</a>
  </div>
</nav>


    <section>
  <article>
    <header>
      <h1 style="text-align: left;">Building Reliable Voice Bots with Open Source Tools - Part 1</h1>
      
<span style="color: var(--muted-text);">A deep look at the main challenges of building performant and cost effective voice bots.</span>
<br>
      <time style="font-style: italic; line-height: 0.8; font-size: medium; color: var(--muted-text);" datetime="2024-09-20T00:00:00Z">by <a href="/">Sara Zan</a>, September 20, 2024</time>
    </header>

    <img style="width:100%; margin: 20px 0 0 0;" src="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/cover.png" alt="Featured image" />

    <p><em>This is part one of the write-up of my talk at <a href="/talks/2024-09-06-odsc-europe-voice-agents/">ODSC Europe 2024</a> and <a href="/talks/2024-10-29-odsc-west-voice-agents/">ODSC West 2024</a>.</em></p>
<hr />
<p>In the last few years, the world of voice agents saw dramatic leaps forward in the state of the art of all its most basic components. Thanks mostly to OpenAI, bots are now able to understand human speech almost like a human would, they're able to speak back with completely naturally sounding voices, and are able to hold a free conversation that feels extremely natural.</p>
<p>But building voice bots is far from a solved problem. These improved capabilities are raising the bar, and even users accustomed to the simpler capabilities of old bots now expect a whole new level of quality when it comes to interacting with them.</p>
<p>In this post we're going to focus mostly on <strong>the challenges</strong>: we'll discuss the basic structure of most voice bots today, their shortcomings and the main issues that you may face on your journey to improve the quality of the conversation.</p>
<p>In <a href="/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/">Part 2</a> we are going to focus on <strong>the solutions</strong> that are available today, and we are going to build our own voice bot using <a href="https://www.pipecat.ai" target="_blank" rel="noopener noreferrer">Pipecat</a>, a recently released open-source library that makes building these bots a lot simpler.</p>
<h2>Outline</h2>
<ul>
<li><a href="#what-is-a-voice-agent">What is a voice agent?</a></li>
<li><a href="#speech-to-text-stt">Speech-to-text (STT)</a></li>
<li><a href="#text-to-speech-tts">Text-to-speech (TTS)</a></li>
<li><a href="#logic-engine">Logic engine</a><ul>
<li><a href="#tree-based">Tree-based</a></li>
<li><a href="#intent-based">Intent-based</a></li>
<li><a href="#llm-based">LLM-based</a></li>
</ul>
</li>
<li><a href="#audio-to-audio">Audio-to-audio models</a></li>
<li><a href="#new-challenges">New challenges</a></li>
<li><a href="#real-speech-is-not-turn-based">Real speech is not turn-based</a></li>
<li><a href="#real-conversation-flows-are-not-predictable">Real conversation flows are not predictable</a></li>
<li><a href="#llms-bring-their-own-problems">LLMs bring their own problems</a></li>
<li><a href="#the-context-window">The context window</a></li>
<li><a href="#working-in-real-time">Working in real time</a></li>
</ul>
<p><em>Continues in <a href="/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/">Part 2</a>.</em></p>
<h2>What is a voice agent?</h2>
<p>As the name says, voice agents are programs that are able to carry on a task and/or take actions and decisions on behalf of a user ("software agents") by using voice as their primary mean of communication (as opposed to the much more common text chat format). Voice agents are inherently harder to build than their text based counterparts: computers operate primarily with text, and the art of making machines understand human voices has been an elusive problem for decades.</p>
<p>Today, the basic architecture of a modern voice agent can be decomposed into three main fundamental building blocks:</p>
<ul>
<li>a <strong>speech-to-text (STT)</strong> component, tasked to translate an audio stream into readable text,</li>
<li>the agent's <strong>logic engine</strong>, which works entirely with text only,</li>
<li>a <strong>text-to-speech (TTS)</strong> component, which converts the bot's text responses back into an audio stream of synthetic speech.</li>
</ul>
<p><img alt="" src="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/structure-of-a-voice-bot-inv.png"  class="invertible" /></p>
<p>Let's see the details of each.</p>
<h3>Speech to text (STT)</h3>
<p>Speech-to-text software is able to convert the audio stream of a person saying something and produce a transcription of what the person said. Speech-to-text engines have a <a href="https://en.wikipedia.org/wiki/Speech_recognition#History" target="_blank" rel="noopener noreferrer">long history</a>, but their limitations have always been quite severe: they used to require fine-tuning on each individual speaker, have a rather high word error rate (WER) and they mainly worked strictly with native speakers of major languages, failing hard on foreign and uncommon accents and native speakers of less mainstream languages. These issues limited the adoption of this technology for anything else than niche software and research applications.</p>
<p>With the <a href="https://openai.com/index/whisper/" target="_blank" rel="noopener noreferrer">first release of OpenAI's Whisper models</a> in late 2022, the state of the art improved dramatically. Whisper enabled transcription (and even direct translation) of speech from many languages with an impressively low WER, finally comparable to the performance of a human, all with relatively low resources, higher then realtime speed, and no finetuning required. Not only, but the model was free to use, as OpenAI <a href="https://huggingface.co/openai" target="_blank" rel="noopener noreferrer">open-sourced it</a> together with a <a href="https://github.com/openai/whisper" target="_blank" rel="noopener noreferrer">Python SDK</a>, and the details of its architecture were <a href="https://cdn.openai.com/papers/whisper.pdf" target="_blank" rel="noopener noreferrer">published</a>, allowing the scientific community to improve on it.</p>
<p><img alt="" src="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/whisper-wer-inv.png"  class="invertible" /></p>
<p><em>The WER (word error rate) of Whisper was extremely impressive at the time of its publication (see the full diagram <a href="https://github.com/openai/whisper/assets/266841/f4619d66-1058-4005-8f67-a9d811b77c62" target="_blank" rel="noopener noreferrer">here</a>).</em></p>
<p>Since then, speech-to-text models kept improving at a steady pace. Nowadays the Whisper's family of models sees some competition for the title of best STT model from  companies such as <a href="https://deepgram.com/" target="_blank" rel="noopener noreferrer">Deepgram</a>, but it's still one of the best options in terms of open-source models.</p>
<h3>Text-to-speech (TTS)</h3>
<p>Text-to-speech model perform the exact opposite task than speech-to-text models: their goal is to convert written text into an audio stream of synthetic speech. Text-to-speech has <a href="https://en.wikipedia.org/wiki/Speech_synthesis#History" target="_blank" rel="noopener noreferrer">historically been an easier feat</a> than speech-to-text, but it also recently saw drastic improvements in the quality of the synthetic voices, to the point that it could nearly be considered a solved problem in its most basic form.</p>
<p>Today many companies (such as OpenAI, <a href="https://cartesia.ai/sonic" target="_blank" rel="noopener noreferrer">Cartesia</a>, <a href="https://elevenlabs.io/" target="_blank" rel="noopener noreferrer">ElevenLabs</a>, Azure and many others) offer TTS software with voices that sound nearly indistinguishable to a human. They also have the capability to clone a specific human voice with remarkably little training data (just a few seconds of speech) and to tune accents, inflections, tone and even emotion.</p>
<div>
<audio controls src="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/sonic-tts-sample.wav" style="width: 100%"></audio>
</div>

<p><em><a href="https://cartesia.ai/sonic" target="_blank" rel="noopener noreferrer">Cartesia's Sonic</a> TTS example of a gaming NPC. Note how the model subtly reproduces the breathing in between sentences.</em></p>
<p>TTS is still improving in quality by the day, but due to the incredibly high quality of the output competition now tends to focus on price and performance.</p>
<h3>Logic engine</h3>
<p>Advancements in the agent's ability to talk to users goes hand in hand with the progress of natural language understanding (NLU), another field with a <a href="https://en.wikipedia.org/wiki/Natural_language_understanding#History" target="_blank" rel="noopener noreferrer">long and complicated history</a>. Until recently, the bot's ability to understand the user's request has been severely limited and often available only for major languages.</p>
<p>Based on the way their logic is implemented, today you may come across bots that rely on three different categories.</p>
<h4>Tree-based</h4>
<p>Tree-based (or rule-based) logic is one of the earliest method of implementing chatbot's logic, still very popular today for its simplicity. Tree-based bots don't really try to understand what the user is saying, but listen to the user looking for a keyword or key sentence that will trigger the next step. For example, a customer support chatbot may look for the keyword "refund" to give the user any information about how to perform a refund, or the name of a discount campaign to explain the user how to take advantage of that.</p>
<p>Tree-based logic, while somewhat functional, doesn't really resemble a conversation and can become very frustrating to the user when the conversation tree was not designed with care, because it's difficult for the end user to understand which option or keyword they should use to achieve the desired outcome. It is also unsuitable to handle real questions and requests like a human would. </p>
<p>One of its most effective usecases is as a first-line screening to triage incoming messages.</p>
<p><img alt="" src="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/tree-based-logic-inv.png"  class="invertible" /></p>
<p><em>Example of a very simple decision tree for a chatbot. While rather minimal, this bot already has several flaws: there's no way to correct the information you entered at a previous step, and it has no ability to recognize synonyms ("I want to buy an item" would trigger the fallback route.)</em></p>
<h4>Intent-based</h4>
<p>In intent-based bots, <strong>intents</strong> are defined roughly as "actions the users may want to do". With respect to a strict, keyword-based tree structure, intent-based bots may switch from an intent to another much more easily (because they lack a strict tree-based routing) and may use advanced AI techniques to understand what the user is actually trying to accomplish and perform the required action.</p>
<p>Advanced voice assistants such as Siri and Alexa use variations of this intent-based system. However, as their owners are usually familiar with, interacting with an intent-based bot doesn't always feel natural, especially when the available intents don't match the user's expectation and the bot ends up triggering an unexpected action. In the long run, this ends with users carefully second-guessing what words and sentence structures activate the response they need and eventually leads to a sort of "magical incantation" style of prompting the agent, where the user has to learn what is the "magical sentence" that the bot will recognize to perform a specific intent without misunderstandings.</p>
<p><img alt="" src="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/amazon-echo.webp" /></p>
<p><em>Modern voice assistants like Alexa and Siri are often built on the concept of intent (image from Amazon).</em></p>
<h4>LLM-based</h4>
<p>The introduction of instruction-tuned GPT models like ChatGPT revolutionized the field of natural language understanding and, with it, the way bots can be built today. LLMs are naturally good at conversation and can formulate natural replies to any sort of question, making the conversation feel much more natural than with any technique that was ever available earlier.</p>
<p>However, LLMs tend to be harder to control. Their very ability of generating naturally sounding responses for anything makes them behave in ways that are often unexpected to the developer of the chatbot: for example, users can get the LLM-based bot to promise them anything they ask for, or they can be convinced to say something incorrect or even occasionally lie.</p>
<p>The problem of controlling the conversation, one that traditionally was always on the user's side, is now entirely on the shoulders of the developers and can easily backfire.</p>
<p><img alt="" src="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/chatgpt-takesies-backsies.png" /></p>
<p><em>In a rather <a href="https://x.com/ChrisJBakke/status/1736533308849443121" target="_blank" rel="noopener noreferrer">famous instance</a>, a user managed to convince a Chevrolet dealership chatbot to promise selling him a Chevy Tahoe for a single dollar.</em></p>
<h3>Audio-to-audio models</h3>
<p>On top of all these changes, OpenAI recently made a step further. They latest flagship model, <a href="https://openai.com/index/hello-gpt-4o/" target="_blank" rel="noopener noreferrer">GPT 4o</a>, was allegedly able to understand audio natively, taking away the need for a dedicated speech-to-text model, and to produce audio responses directly, making text-to-speech engines also redundant. </p>
<p>For a long time these capabilities were heavily restricted to a limited number of partners, but as of the 1st of October 2024, they eventually made such capabilities generally available through their new <a href="https://openai.com/index/introducing-the-realtime-api/" target="_blank" rel="noopener noreferrer">Realtime API</a>.</p>
<p>At a first glance, the release of such model seemed to shake the foundations of how we build voice bots today. However, at the time of writing, there are still a number of hurdles that prevents immediate adoption, the main one being <strong>price</strong>.</p>
<p><img alt="" src="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/realtime-api-pricing.png" /></p>
<p><em><a href="https://openai.com/api/pricing/" target="_blank" rel="noopener noreferrer">Pricing of the Realtime API</a> at the time of writing (October 2024)</em></p>
<p>The problem here is that voice bots built with the traditional stack can be more than sufficient to cover the most common usecases for a fraction of the price of GPT 4o, and while the latter can indeed handle cases that are impossible to address for a traditional voice bot, in most practical situations such capabilities are not necessary to achieve a smooth and effective interaction.</p>
<p>However, GPT 4o is surely a step further on the evolutionary path of modern voice bots. With potential future price changes, a model like this could easily become a valid competitor to the architecture we're going to explore in the rest of the post, with its own pros and cons.</p>
<h2>New challenges</h2>
<p>Thanks to all these recent improvements, it would seem that making natural-sounding, smart bots is getting easier and easier. It is indeed much simpler to make a simple bot sound better, understand more and respond appropriately, but there's still a long way to go before users can interact with these new bots as they would with a human.</p>
<p>The issue lays in the fact that <strong>users expectations grow</strong> with the quality of the bot. It's not enough for the bot to have a voice that sounds human: users want to be able to interact with it in a way that it feels human too, which is far more rich and interactive than what the rigid tech of earlier chatbots allowed so far.</p>
<p>What does this mean in practice? What are the expectations that users might have from our bots?</p>
<h3>Real speech is not turn-based</h3>
<p>Traditional bots can only handle turn-based conversations: the user talks, then the bot talks as well, then the user talks some more, and so on. A conversation with another human, however, has no such limitation: people may talk over each other, give audible feedback without interrupting, and more.</p>
<p>Here are some examples of this richer interaction style:</p>
<ul>
<li>
<p><strong>Interruptions</strong>. Interruptions occur when a person is talking and another one starts talking at the same time. It is expected that the first person stops talking, at least for a few seconds, to understand what the interruption was about, while the second person continue to talk.</p>
</li>
<li>
<p><strong>Back-channeling</strong>. Back-channeling is the practice of saying "ok", "sure", "right" while the other person is explaining something, to give them feedback and letting them know we're paying attention to what is being said. The person that is talking is not supposed to stop: the aim of this sort of feedback is to let them know they are being heard.</p>
</li>
<li>
<p><strong>Pinging</strong>. This is the natural reaction a long silence, especially over a voice-only medium such as a phone call. When one of the two parties is supposed to speak but instead stays silent, the last one that talked might "ping" the silent party by asking "Are you there?", "Did you hear?", or even just "Hello?" to test whether they're being heard. This behavior is especially difficult to handle for voice agents that have a significant delay, because it may trigger an ugly vicious cycle of repetitions and delayed replies.</p>
</li>
<li>
<p><strong>Buying time</strong>. When one of the parties know that it will stay silent for a while, a natural reaction is to notify the other party in advance by saying something like "Hold on...", "Wait a second...", "Let me check..." and so on. This message has the benefit of preventing the "pinging" behavior we've seen before and can be very useful for voice bots that may need to carry on background work during the conversation, such as looking up information.</p>
</li>
<li>
<p><strong>Audible clues</strong>. Not everything can be transcribed by a speech-to-text model, but audio carries a lot of nuance that is often used by humans to communicate. A simple example is pitch: humans can often tell if they're talking to a child, a woman or a man by the pitch of their voice, but STT engines don't transcribe that information. So if a child picks up the phone, the model won't pick up the obvious audible clue and will likely assume it is talking to an adult. Similar considerations should be made for tone (to detect mood, sarcasm, etc) or other sounds like laughter, sobs, and more. <strong>Audio-to-audio models</strong> such as GPT 4o don't have this intrinsic limitation, but while they surely can pick up these clues, their ability to use them effectively should not be taken for granted.</p>
</li>
</ul>
<h3>Real conversation flows are not predictable</h3>
<p>Tree-based bots, and to some degree intent-based too, work on the implicit assumption that conversation flows are largely predictable. Once the user said something and the bot replied accordingly, they can only follow up with a fixed set of replies and nothing else.</p>
<p>This is often a flawed assumption and the primary reason why talking to chatbots tends to be so frustrating.</p>
<p>In reality, natural conversations are largely unpredictable. For example, they may feature:</p>
<ul>
<li>
<p><strong>Sudden changes of topic</strong>. Maybe user and bot were talking about making a refund, but then the user changes their mind and decides to ask for assistance finding a repair center for the product. Well designed intent-based bots can deal with that, but most bots are in practice unable to do so in a way that feels natural to the user.</p>
</li>
<li>
<p><strong>Unexpected, erratic phrasing</strong>. This is common when users are nervous or in a bad mood for any reason. Erratic, convoluted phrasing, long sentences, rambling, are all very natural ways of expressing themselves, but such outbursts very often confuse bots completely.</p>
</li>
<li>
<p><strong>Non native speakers</strong>. Due to the nature la language learning, non native speakers may have trouble pronouncing words correctly, they may use highly unusual synonyms, or structure sentences in complicated ways. This is also difficult for bots to handle, because understanding the sentence is harder and transcription issues are far more likely.</p>
</li>
<li>
<p><em><strong>Non sequitur</strong></em>. <em>Non sequitur</em> is an umbrella term for a sequence of sentences that bear no relation to each other in a conversation. A simple example is the user asking the bot "What's the capital of France" and the bot replies "It's raining now". When done by the bot, this is often due to a severe transcription issue or a very flawed conversation design. When done by the user, it's often a malicious intent to break the bot's logic, so it should be handled with some care.</p>
</li>
</ul>
<h3>LLMs bring their own problems</h3>
<p>It may seem that some of these issues, especially the ones related to conversation flow, could be easily solved with an LLM. These models, however, bring their own set of issues too:</p>
<ul>
<li>
<p><strong>Hallucinations</strong>. This is a technical term to say that LLMs can occasionally mis-remember information, or straight up lie. The problem is that they're also very confident about their statements, sometimes to the point of trying to gaslight their users. Hallucinations are a major problem for all LLMs: although it may seem to get more manageable with larger and smarter models, the problem only gets more subtle and harder to spot.</p>
</li>
<li>
<p><strong>Misunderstandings</strong>. While LLMs are great at understanding what the user is trying to say, they're not immune to misunderstandings. Unlike a human though, LLMs rarely suspect a misunderstanding and they rather make assumptions that ask for clarifications, resulting in surprising replies and behavior that are reminiscent of intent-based bots.</p>
</li>
<li>
<p><strong>Lack of assertiveness</strong>. LLMs are trained to listen to the user and do their best to be helpful. This means that LLMs are also not very good at taking the lead of the conversation when we would need them to, and are easily misled and distracted by a motivated user. Preventing your model to give your user's a literary analysis of their unpublished poetry may sound silly, but it's a lot harder than many suspect.</p>
</li>
<li>
<p><strong>Prompt hacking</strong>. Often done with malicious intent by experienced users, prompt hacking is the practice of convincing an LLM to reveal its initial instructions, ignore them and perform actions they are explicitly forbidden from. This is especially dangerous and, while a lot of work has gone into this field, this is far from a solved problem.</p>
</li>
</ul>
<h3>The context window</h3>
<p>LLMs need to keep track of the whole conversation, or at least most of it, to be effective. However, they often have a limitation to the amount of text they can keep in mind at any given time: this limit is called <strong>context window</strong> and for many models is still relatively low, at about 2000 tokens <strong>(between 1500-1800 words)</strong>. </p>
<p>The problem is that this window also need to include all the instructions your bot needs for the conversation. This initial set of instructions is called <strong>system prompt</strong>, and is slightly distinct from the other messages in the conversation to make the LLM understand that it's not part of it, but it's a set of instructions about how to handle the conversation.</p>
<p>For example, a system prompt for a customer support bot may look like this:</p>
<div class="codehilite"><pre><span></span><code>You&#39;re a friendly customer support bot named VirtualAssistant. 
You are always kind to the customer and you must do your best 
to make them feel at ease and helped.

You may receive a set of different requests. If the users asks
you to do anything that is not in the list below, kindly refuse
to do so.

# Handle refunds

If the user asks you to handle a refund, perform these actions:
- Ask for their shipping code
- Ask for their last name
- Use the tool `get_shipping_info` to verify the shipping exists
...

# Handle subscriptions

If the user asks you to subscribe to a service, perform these actions:
- Ask what subscription are they interested in
- Ask if they have a promo code
- Ask for their username
...
</code></pre></div>

<p>and so on.</p>
<p>Although very effective, system prompts have a tendency to become huge in terms of tokens. Adding information to it makes the LLM behave much more like you expect (although it's not infallible), hallucinate less, and can even shape its personality to some degree. But if the system prompt becomes too long (more than 1000 words), this means that the bot will only be able to exchange about 800 words worth of messages with the user before it starts to <strong>forget</strong> either its instructions or the first messages of the conversation. For example, the bot will easily forget its own name and role, or it will forget the user's name and initial demands, which can make the conversation drift completely.</p>
<h3>Working in real time</h3>
<p>If all these issues weren't enough, there's also a fundamental issue related to voice interaction: <strong>latency</strong>. Voice bots interact with their users in real time: this means that the whole pipeline of transcription, understanding, formulating a reply and synthesizing it back but be very fast.</p>
<p>How fast? On average, people expect a reply from another person to arrive within <strong>300-500ms</strong> to sound natural. They can normally wait for about 1-2 seconds. Any longer and they'll likely ping the bot, breaking the flow.</p>
<p>This means that, even if we had some solutions to all of the above problems (and we do have some), these solutions needs to operate at blazing fast speed. Considering that LLM inference alone can take the better part of a second to even start being generated, latency is often one of the major issues that voice bots face when deployed at scale.</p>
<p><img alt="" src="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/ttft-inv.jpg"  class="invertible" /></p>
<p><em>Time to First Token (TTFT) stats for several LLM inference providers running Llama 2 70B chat. From <a href="https://github.com/ray-project/llmperf-leaderboard" target="_blank" rel="noopener noreferrer">LLMPerf leaderboard</a>. You can see how the time it takes for a reply to even start being produced is highly variable, going up to more than one second in some scenarios.</em>  </p>
<h2>To be continued...</h2>
<p><em>Interested? Check out <a href="/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2">Part 2</a>!</em> </p>
<p class="fleuron"><a href="/posts/2024-05-06-teranoptia/">F]</a></p>

  </article>
</section>


    <footer>
  <section>
    ©
    2023 -
    2026 by &MediumSpace; <a href="/"><img src="/assets/avatar/avatar.svg" style="width: 1em; height: 1em; margin-right: 5px;"> Sara Zan</a>
  </section>
</footer>


  </main>

  

  <!-- Theme toggle persistence -->
  <script>
    (function() {
      const themeToggle = document.getElementById('theme-toggle');

      // Load saved theme preference
      const savedTheme = localStorage.getItem('theme');
      if (savedTheme === 'dark') {
        themeToggle.checked = true;
      }

      // Save theme preference on change
      themeToggle.addEventListener('change', function() {
        if (this.checked) {
          localStorage.setItem('theme', 'dark');
        } else {
          localStorage.setItem('theme', 'light');
        }
      });
    })();
  </script>

</body>
</html>
