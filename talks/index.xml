<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Talks on Sara Zan</title><link>https://www.zansara.dev/talks/</link><description>Recent content in talks on Sara Zan</description><generator>Custom Script</generator><language>en</language><lastBuildDate>Tue, 19 May 2026 00:00:00 +0000</lastBuildDate><item><title>[UPCOMING] AI Impact Summit - Smarter systems, leaner models: reducing compute costs without sacrificing quality</title><link>https://events.economist.com/ai-compute/programme/#day1+cat-10+smarter-systems-leaner-models-reducing-compute-costs-without-sacrificing-quality</link><pubDate>Tue, 19 May 2026 00:00:00 +0000</pubDate><guid>https://events.economist.com/ai-compute/programme/#day1+cat-10+smarter-systems-leaner-models-reducing-compute-costs-without-sacrificing-quality</guid><description>&lt;p&gt;...&lt;/p&gt;</description></item><item><title>ODSC AI East - From RAG to AI Agent</title><link>https://schedule.odsc.ai/</link><pubDate>Wed, 29 Apr 2026 00:00:00 +0000</pubDate><guid>https://schedule.odsc.ai/</guid><description>&lt;p&gt;...&lt;/p&gt;</description></item><item><title>Agentic AI Summit - From RAG to AI Agent</title><link>https://www.zansara.dev/talks/2026-01-21-agentic-ai-summit-from-rag-to-ai-agents/</link><pubDate>Wed, 21 Jan 2026 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/talks/2026-01-21-agentic-ai-summit-from-rag-to-ai-agents/</guid><description>&lt;p&gt;&lt;a href="https://www.summit.ai/#w-dropdown-toggle-30" target="_blank" rel="noopener noreferrer"&gt;Announcement&lt;/a&gt;, &lt;a href="https://colab.research.google.com/drive/1YVN5GrmZMM7qpI-dbeV-HIYig9XoSk6W?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;interactive notebook&lt;/a&gt;, &lt;a href="https://colab.research.google.com/drive/1W3OsRSGxntPRnMFweY-LwU_qVWXsp-RW?usp=drive_link" target="_blank" rel="noopener noreferrer"&gt;presentation&lt;/a&gt;. All resources can also be found in &lt;a href="https://drive.google.com/drive/folders/1swGeBLjWWu6mueFFwlvr11761RgMLOBq?usp=drive_link" target="_blank" rel="noopener noreferrer"&gt;my archive&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;At the &lt;a href="https://summit.ai/" target="_blank" rel="noopener noreferrer"&gt;Agentic AI Summit 2026&lt;/a&gt; I show how you can take your RAG pipeline and, step by step, convert it into an AI agent. &lt;/p&gt;
&lt;p&gt;The workshop is based on an earlier &lt;a href="https://www.zansara.dev/posts/2026-01-07-from-rag-to-ai-agent/"&gt;blog article&lt;/a&gt; and goes through &lt;a href="https://colab.research.google.com/drive/1YVN5GrmZMM7qpI-dbeV-HIYig9XoSk6W?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;this Colab notebook&lt;/a&gt; where in under a hour we build from scratch a chatbot, a RAG pipeline, and then we progressively upgrade it to an AI Agent. Every step of the way is hand-on and you can take the notebook and adapt it to your stack for a more realistic example.&lt;/p&gt;</description></item><item><title>Embrace:AI // 2025.06 - Reasoning LLMs &amp; Multimodal Architecture</title><link>https://www.zansara.dev/talks/2025-11-06-embrace-ai-meetup-reasoning-models/</link><pubDate>Thu, 06 Nov 2025 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/talks/2025-11-06-embrace-ai-meetup-reasoning-models/</guid><description>&lt;p&gt;&lt;a href="https://www.meetup.com/embrace-ai/events/311629934/" target="_blank" rel="noopener noreferrer"&gt;Announcement&lt;/a&gt;, &lt;a href="https://drive.google.com/file/d/1QoUVlA915-7UJqu9DkxKQUc3deJhsB8t/view?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;slides&lt;/a&gt;. All resources can also be found in &lt;a href="https://docs.google.com/presentation/d/1RzJOwSwaLcNFkkPuvpR9e9pRzf0mnz23l8H29X9UDSA/edit?usp=drive_link" target="_blank" rel="noopener noreferrer"&gt;my archive&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;At &lt;a href="https://www.meetup.com/embrace-ai/" target="_blank" rel="noopener noreferrer"&gt;Embrace.ai's November Meetup&lt;/a&gt;, part of the &lt;a href="https://lisbonaiweek.com/" target="_blank" rel="noopener noreferrer"&gt;Lisbon AI Week&lt;/a&gt; I talked about reasoning models: what they are, what they aren't, how they work and when to use them. Is GPT-5 AGI? Is it an AI Agent? Or is it just a glorified chain-of-thought prompt under the hood? &lt;/p&gt;
&lt;p&gt;To answer these questions, I classified LLMs into a small "taxonomy" based on the post-training steps they go through, in order to highlight how reasoning models differ qualitatively from their predecessors just like instruction-tuned models were not simple text-completion models with a better prompt. &lt;/p&gt;
&lt;p&gt;I also covered the effect of increasing the reasoning effort of the model, clarifying when it's useful and when it can lead to overthinking.&lt;/p&gt;</description></item><item><title>ODSC West: LLMs that think - Demystifying Reasoning Models</title><link>https://www.zansara.dev/talks/2025-10-29-odsc-west-reasoning-models/</link><pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/talks/2025-10-29-odsc-west-reasoning-models/</guid><description>&lt;p&gt;&lt;a href="https://odsc.ai/speakers-portfolio/llms-that-think-demystifying-reasoning-models/" target="_blank" rel="noopener noreferrer"&gt;Announcement&lt;/a&gt;, &lt;a href="/posts/2025-05-12-beyond-hype-reasoning-models/"&gt;teaser article&lt;/a&gt;, &lt;a href="https://drive.google.com/file/d/1x-DobRa7ZrUncTe1kUVHc6BGFzxiTELc/view?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;slides&lt;/a&gt;. All resources can also be found in &lt;a href="https://drive.google.com/drive/folders/1-UbjdFxvg5NtCUxUFGg47H4EPkeXLuy_?usp=drive_link" target="_blank" rel="noopener noreferrer"&gt;my archive&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;At &lt;a href="https://odsc.ai/west/schedule/" target="_blank" rel="noopener noreferrer"&gt;ODSC West 2025&lt;/a&gt; I talked about reasoning models: what they are, what they aren't, how they work and when to use them. Is GPT-5 AGI? Is it an AI Agent? Or is it just a glorified chain-of-thought prompt under the hood? &lt;/p&gt;
&lt;p&gt;To answer these questions, I classified LLMs into a small "taxonomy" based on the post-training steps they go through, in order to highlight how reasoning models differ qualitatively from their predecessors just like instruction-tuned models were not simple text-completion models with a better prompt. &lt;/p&gt;
&lt;p&gt;I also covered the effect of increasing the reasoning effort of the model, clarifying when it's useful and when it can lead to overthinking.&lt;/p&gt;</description></item><item><title>ODSC East: LLMs that think - Demystifying Reasoning Models</title><link>https://www.zansara.dev/talks/2025-05-14-odsc-east-reasoning-models/</link><pubDate>Wed, 14 May 2025 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/talks/2025-05-14-odsc-east-reasoning-models/</guid><description>&lt;p&gt;&lt;a href="https://odsc.com/speakers/llms-that-think-demystifying-reasoning-models/" target="_blank" rel="noopener noreferrer"&gt;Announcement&lt;/a&gt;, &lt;a href="/posts/2025-05-12-beyond-hype-reasoning-models/"&gt;teaser article&lt;/a&gt;, &lt;a href="https://drive.google.com/file/d/1Gmxx2G-H0aozBZtACCvGIqJNgybXB716/view?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;slides&lt;/a&gt;. All resources can also be found in &lt;a href="https://drive.google.com/drive/folders/1Iy_mJr7MYdrbb-W-g1U38gkPBjrV8dGu?usp=drive_link" target="_blank" rel="noopener noreferrer"&gt;my archive&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;At &lt;a href="https://odsc.com/boston/" target="_blank" rel="noopener noreferrer"&gt;ODSC East 2025&lt;/a&gt; I talked about reasoning models: what they are, what they aren't, how they work and when to use them. Is OpenAI's o4 AGI? Is it an AI Agent? Or is it just a glorified chain-of-thought prompt under the hood? &lt;/p&gt;
&lt;p&gt;To answer these questions, I classified LLMs into a small "taxonomy" based on the post-training steps they go through, in order to highlight how reasoning models differ qualitatively from their predecessors just like instruction-tuned models were not simple text-completion models with a better prompt. &lt;/p&gt;
&lt;p&gt;I also covered the effect of increasing the reasoning effort of the model, clarifying when it's useful and when it can lead to overthinking.&lt;/p&gt;</description></item><item><title>PyData Global: Building LLM Voice Bots with Open Source Tools</title><link>https://www.zansara.dev/talks/2024-12-03-pydata-global-voice-bots/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/talks/2024-12-03-pydata-global-voice-bots/</guid><description>&lt;p&gt;&lt;a href="https://global2024.pydata.org/cfp/talk/T3YDBP/" target="_blank" rel="noopener noreferrer"&gt;Announcement&lt;/a&gt;, &lt;a href="https://drive.google.com/file/d/1rXb4-m-BWwhAqDCDBXpzw6nJ9OELOpSl/view?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;slides&lt;/a&gt;, &lt;a href="https://drive.google.com/file/d/1bja0O8LG7790UIU7HpAYXat-BYXeUbK-/view?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;demo video&lt;/a&gt; and full video (&lt;a href="https://www.youtube.com/watch?v=Td5dFdG0wE4" target="_blank" rel="noopener noreferrer"&gt;Youtube&lt;/a&gt;, &lt;a href="https://drive.google.com/file/d/1HTEEs-Zr8mZoJA8a7AuiJf61soGvQNPI/view?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;backup&lt;/a&gt;). All resources can also be found in &lt;a href="https://drive.google.com/drive/folders/1ZPkne2QxOnSXfchv08CWkAZG3duXxd4Z?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;my archive&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;iframe src="https://drive.google.com/file/d/1HTEEs-Zr8mZoJA8a7AuiJf61soGvQNPI/preview" width="800" height="500" allow="autoplay"&gt;&lt;/iframe&gt;
&lt;hr /&gt;
&lt;h2&gt;Demo&lt;/h2&gt;
&lt;p&gt;During the talk I showed a recording of a demo built with &lt;a href="https://github.com/intentional-ai/intentional" target="_blank" rel="noopener noreferrer"&gt;Intentional&lt;/a&gt;, a new library to prompt voice bots in a way that takes inspiration from classic intent-based chatbots. Here are the instructions needed to run this same demo on your own machine and play with it.&lt;/p&gt;
&lt;div class="notice info"&gt;

&lt;p&gt;Intentional is still in its very first stages of development and highly unstable!&lt;/p&gt;
&lt;p&gt;I am looking for contributors to help this project come to life, so if you would like to help there are many ways to do so: leave a star on &lt;a href="https://github.com/intentional-ai/intentional" target="_blank" rel="noopener noreferrer"&gt;the repo&lt;/a&gt;, &lt;a href="https://intentional-ai.github.io/intentional/docs/home/" target="_blank" rel="noopener noreferrer"&gt;test the library&lt;/a&gt; on your machine, &lt;a href="https://github.com/intentional-ai/intentional/issues/new" target="_blank" rel="noopener noreferrer"&gt;open an issue&lt;/a&gt;, reach out to me to leave feedback (you can find my contact &lt;a href="/"&gt;on the homepage&lt;/a&gt;), &lt;a href="https://github.com/intentional-ai/intentional" target="_blank" rel="noopener noreferrer"&gt;spread the word&lt;/a&gt; about Intentional, or even &lt;a href="https://intentional-ai.github.io/intentional/CONTRIBUTING/" target="_blank" rel="noopener noreferrer"&gt;contribute to the project&lt;/a&gt; with a PR.&lt;/p&gt;
&lt;!-- Place this tag in your head or just before your close body tag. --&gt;
&lt;script async defer src="https://buttons.github.io/buttons.js"&gt;&lt;/script&gt;
&lt;p&gt;&lt;a class="github-button" href="https://github.com/intentional-ai" data-color-scheme="no-preference: light; light: light; dark: dark;" data-size="large" data-show-count="true" aria-label="Follow @intentional-ai on GitHub" target="_blank" rel="noopener noreferrer"&gt;Follow @intentional-ai&lt;/a&gt;
&lt;a class="github-button" href="https://github.com/intentional-ai/intentional/subscription" data-color-scheme="no-preference: light; light: light; dark: dark;" data-size="large" data-show-count="true" aria-label="Watch intentional-ai/intentional on GitHub" target="_blank" rel="noopener noreferrer"&gt;Watch&lt;/a&gt;
&lt;a class="github-button" href="https://github.com/intentional-ai/intentional" data-color-scheme="no-preference: light; light: light; dark: dark;" data-size="large" data-show-count="true" aria-label="Star intentional-ai/intentional on GitHub" target="_blank" rel="noopener noreferrer"&gt;Star&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;First, install Intentional and the plugin for &lt;a href="https://textual.textualize.io/" target="_blank" rel="noopener noreferrer"&gt;Textual&lt;/a&gt;'s UI:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip&lt;span class="w"&gt; &lt;/span&gt;install&lt;span class="w"&gt; &lt;/span&gt;intentional&lt;span class="w"&gt; &lt;/span&gt;intentional-textual-ui
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="notice info"&gt;

&lt;p&gt;This demo was only tested on Linux (Ubuntu). You will need &lt;code&gt;portaudio&lt;/code&gt; in order for Intentional to handle audio from your microphone, so if you face errors during the installation, try: &lt;code&gt;sudo apt install portaudio19-dev&lt;/code&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Next, you'll need the configuration file where the conversation tree is defined. Intentional bots are, at their core, entirely defined by this configuration file (with the partial exception of tools, as you can see &lt;a href="https://intentional-ai.github.io/intentional/docs/home/" target="_blank" rel="noopener noreferrer"&gt;in the documentation&lt;/a&gt;). Download the demo configuration file &lt;a href="https://drive.google.com/file/d/1dkvxpCH6uny8ew3wrsgh7SPZdqvKuTyd/view?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;from this link&lt;/a&gt; and save it as &lt;code&gt;demo.yml&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now, make your OpenAI API key available to the CLI:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;OPENAI_API_KEY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&amp;lt;your&lt;span class="w"&gt; &lt;/span&gt;api&lt;span class="w"&gt; &lt;/span&gt;key&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now you're ready to run the bot. Intentional comes with a simple CLI that can directly run bots from a config file and draw their conversation graph. To run the bot and talk to it as shown in the video, run:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;intentional&lt;span class="w"&gt; &lt;/span&gt;demo.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You should see a UI such as this coming up (with the chat history empty):&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/talks/2024-12-03-pydata-global-voice-bots-demo-ui.png" /&gt;&lt;/p&gt;
&lt;p&gt;Just start speaking and the bot will quickly reply to you. &lt;/p&gt;
&lt;div class="notice info"&gt;
Occasionally the transcriptions don't work well. The bot is generating such transcriptions exclusively for your convenience, so even if they're mangled, in most cases you can be confident that the model actually heard you well.
&lt;/div&gt;

&lt;p&gt;To see the conversation graph, run this command:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;intentional&lt;span class="w"&gt; &lt;/span&gt;demo.yml&lt;span class="w"&gt; &lt;/span&gt;--draw
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The PNG file will be created next to &lt;code&gt;demo.yml&lt;/code&gt; and will be called &lt;code&gt;demo.png&lt;/code&gt;.&lt;/p&gt;</description></item><item><title>ODSC West: Building Reliable Voice Agents with Open Source tools</title><link>https://www.zansara.dev/talks/2024-10-29-odsc-west-voice-agents/</link><pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/talks/2024-10-29-odsc-west-voice-agents/</guid><description>&lt;p&gt;&lt;a href="https://odsc.com/speakers/building-reliable-voice-agents-with-open-source-tools-2/" target="_blank" rel="noopener noreferrer"&gt;Announcement&lt;/a&gt;, &lt;a href="https://drive.google.com/file/d/1H_8PKQY_kFIzwxakZc-RJU8Rtwnjajhl/view?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;slides&lt;/a&gt;, &lt;a href="https://colab.research.google.com/drive/1CUX7JRYMU1MEJBZ6lWMg5EThPew19Zjs?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;notebook&lt;/a&gt;. All resources can also be found in &lt;a href="https://drive.google.com/drive/folders/1baO1Gv55CIjLT-KPWtSm2z4IOBdXkHjn?usp=drive_link" target="_blank" rel="noopener noreferrer"&gt;my archive&lt;/a&gt;. Did you miss the talk? Check out the write-up's &lt;a href="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/"&gt;part 1&lt;/a&gt; and &lt;a href="/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/"&gt;part 2&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;iframe src="https://drive.google.com/file/d/1uQTJFmHTinvUvCMHRq9rrbaClNw9I9i2/preview" width="800" height="500" allow="autoplay"&gt;&lt;/iframe&gt;
&lt;hr /&gt;
&lt;p&gt;At &lt;a href="https://odsc.com/california/" target="_blank" rel="noopener noreferrer"&gt;ODSC West 2024&lt;/a&gt; I talked about building modern and reliable voice bots using Pipecat, a recently released open source tool. I gave an overview of the general structure of voice bots, of the improvements their underlying tech recently saw, from the first Whisper release to GPT 4o Realtime, and the new challenges that developers face when implementing one of these systems.&lt;/p&gt;
&lt;p&gt;The main highlight of the talk is the &lt;a href="https://colab.research.google.com/drive/1CUX7JRYMU1MEJBZ6lWMg5EThPew19Zjs?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;notebook&lt;/a&gt; where I implement first a simple Pipecat bot from scratch, and then I give an overview of how to blend intent detection and system prompt switching to improve our control of how LLM bots interact with users.&lt;/p&gt;</description></item><item><title>SNAIL Opening Day: Should I use an LLM Framework? (Private Event)</title><link>https://www.zansara.dev/talks/2024-10-01-snail-opening-day-should-i-use-an-llm-framework/</link><pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/talks/2024-10-01-snail-opening-day-should-i-use-an-llm-framework/</guid><description>&lt;p&gt;&lt;a href="https://drive.google.com/file/d/1GQJ1qEY2hXQ6EBF-rtqzJqZzidfS7HfI/view?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;Slides&lt;/a&gt;, &lt;a href="https://colab.research.google.com/drive/11aOq-43wEWhSlxtkdXEAwPEarC0IQ3eN?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;notebook&lt;/a&gt;, &lt;a href="https://huggingface.co/datasets/ZanSara/seven-wonders" target="_blank" rel="noopener noreferrer"&gt;RAG dataset&lt;/a&gt; and &lt;a href="https://huggingface.co/datasets/ZanSara/seven-wonders-eval" target="_blank" rel="noopener noreferrer"&gt;evaluation dataset&lt;/a&gt;. All resources can also be found in &lt;a href="https://drive.google.com/drive/folders/1anl3adpxgbwq5nsFn8QXuofIWXX0jRKo?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;my archive&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;iframe src="https://drive.google.com/file/d/1AORVusaHVBqNvJ5OtctyB5TWQZSadoqT/preview" width="800" height="500" allow="autoplay"&gt;&lt;/iframe&gt;

&lt;p&gt;Find the transcript &lt;a href="https://drive.google.com/file/d/1wwnTFmGOANVmxUaVd1PC3cfztzIfSCEa/view?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;For the &lt;a href="https://group.springernature.com/gp/group" target="_blank" rel="noopener noreferrer"&gt;Springer Nature&lt;/a&gt; AI Lab Opening Day I talk about LLM frameworks: what they are, when they can be useful, and how to choose and compare one framework to the other.&lt;/p&gt;
&lt;p&gt;After an overview of six application frameworks (&lt;a href="https://www.langchain.com/" target="_blank" rel="noopener noreferrer"&gt;LangChain&lt;/a&gt;, &lt;a href="https://www.llamaindex.ai/" target="_blank" rel="noopener noreferrer"&gt;LlamaIndex&lt;/a&gt;, &lt;a href="https://haystack.deepset.ai/" target="_blank" rel="noopener noreferrer"&gt;Haystack&lt;/a&gt;, &lt;a href="https://neuml.github.io/txtai/" target="_blank" rel="noopener noreferrer"&gt;txtai&lt;/a&gt;, &lt;a href="https://dspy-docs.vercel.app/" target="_blank" rel="noopener noreferrer"&gt;DSPy&lt;/a&gt; and &lt;a href="https://www.crewai.com/" target="_blank" rel="noopener noreferrer"&gt;CrewAI&lt;/a&gt;), we run a notebook where we used &lt;a href="https://docs.ragas.io/en/latest/" target="_blank" rel="noopener noreferrer"&gt;RAGAS&lt;/a&gt; to compare four small RAG applications and see which one performs better.&lt;/p&gt;</description></item><item><title>AMTA 2024 Virtual Tutorial Day: Controlling LLM Translations of Invariant Elements with RAG</title><link>https://www.zansara.dev/talks/2024-09-18-amta-2024-controlling-invariants-rag/</link><pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/talks/2024-09-18-amta-2024-controlling-invariants-rag/</guid><description>&lt;p&gt;&lt;a href="https://amtaweb.org/virtual-tutorial-day-program/" target="_blank" rel="noopener noreferrer"&gt;Announcement&lt;/a&gt;, &lt;a href="https://colab.research.google.com/drive/1VMgK3DcVny_zTtAG_V3QSSdfSFBWAgmb?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;notebook&lt;/a&gt; and &lt;a href="https://docs.google.com/spreadsheets/d/1A1zk-u-RTSqBfE8LksZxihnp7KxWO7YK/edit?usp=sharing&amp;amp;ouid=102297935451395786183&amp;amp;rtpof=true&amp;amp;sd=true" target="_blank" rel="noopener noreferrer"&gt;glossary&lt;/a&gt;. All resources can also be found in &lt;a href="https://drive.google.com/drive/folders/1Tdq92P_E_77sErGjz7jSPfJ-or9UZXvn?usp=drive_link" target="_blank" rel="noopener noreferrer"&gt;my archive&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;iframe src="https://drive.google.com/file/d/1BvcNbsAGWp25EDpiQ5ljYos3_eneo3wu/preview" width="800" height="500" allow="autoplay"&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;em&gt;Note: this was a tutorial session co-presented with &lt;a href="https://www.linkedin.com/in/christian-lang-8942b0145/" target="_blank" rel="noopener noreferrer"&gt;Christian Lang&lt;/a&gt; and &lt;a href="https://www.linkedin.com/in/brunobitter/" target="_blank" rel="noopener noreferrer"&gt;Bruno Bitter&lt;/a&gt;. My section starts at 01:08.&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;At the &lt;a href="https://amtaweb.org/virtual-tutorial-day-program/" target="_blank" rel="noopener noreferrer"&gt;AMTA 2024 Virtual Tutorial Day&lt;/a&gt; I talked about controlling invariant translation elements with RAG. During the talk several speakers intervened on the topic, each bringing a different perspective of it. &lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/georgkirchner/" target="_blank" rel="noopener noreferrer"&gt;Georg Kirchner&lt;/a&gt; introduced the concept of invariant translation elements, such as brand names, UI elements, and corporate slogans. &lt;a href="https://www.linkedin.com/in/christian-lang-8942b0145/" target="_blank" rel="noopener noreferrer"&gt;Christian Lang&lt;/a&gt; gave a comprehensive overview of the challenges of handling invariant translation elements with existing tools and how LLMs can help at various stages of the translation, covering several approaches, including RAG. Building on his overview, I showed how to implement a simple RAG system to handle these invariants properly using &lt;a href="https://haystack.deepset.ai/?utm_campaign=amta-2024" target="_blank" rel="noopener noreferrer"&gt;Haystack&lt;/a&gt;: we run a &lt;a href="https://colab.research.google.com/drive/1VMgK3DcVny_zTtAG_V3QSSdfSFBWAgmb?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;Colab notebook&lt;/a&gt; live and checked how the translation changes by introducing context about the invariants to the LLM making the translation. Last, &lt;a href="https://www.linkedin.com/in/brunobitter/" target="_blank" rel="noopener noreferrer"&gt;Bruno Bitter&lt;/a&gt; gave an overview of how you can use &lt;a href="https://www.blackbird.io/" target="_blank" rel="noopener noreferrer"&gt;Blackbird&lt;/a&gt; to integrate a system like this with existing CAT tools and manage the whole lifecycle of content translation.&lt;/p&gt;</description></item><item><title>ODSC Europe: Building Reliable Voice Agents with Open Source tools</title><link>https://www.zansara.dev/talks/2024-09-06-odsc-europe-voice-agents/</link><pubDate>Fri, 06 Sep 2024 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/talks/2024-09-06-odsc-europe-voice-agents/</guid><description>&lt;p&gt;&lt;a href="https://odsc.com/speakers/building-reliable-voice-agents-with-open-source-tools-2/" target="_blank" rel="noopener noreferrer"&gt;Announcement&lt;/a&gt;, &lt;a href="https://drive.google.com/file/d/1ubk7Q_l9C7epQgYrMttHMjW1AVfdm-LT/view?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;slides&lt;/a&gt; and &lt;a href="https://colab.research.google.com/drive/1NCAAs8RB2FuqMChFKMIVWV0RiJr9O3IJ?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;notebook&lt;/a&gt;. All resources can also be found on ODSC's website and in &lt;a href="https://drive.google.com/drive/folders/1rrXMTbfTZVuq9pMzneC8j-5GKdRQ6l2i?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;my archive&lt;/a&gt;. Did you miss the talk? Check out the write-up's &lt;a href="/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/"&gt;part 1&lt;/a&gt; and &lt;a href="/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/"&gt;part 2&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;(Note: this is a recording of the notebook walkthrough only)&lt;/em&gt;&lt;/p&gt;
&lt;iframe src="https://drive.google.com/file/d/15Kv8THmDsnnzfVBhHAf2O11RccpzAzYK/preview" width="800" height="500" allow="autoplay"&gt;&lt;/iframe&gt;

&lt;p&gt;At &lt;a href="https://odsc.com/europe/" target="_blank" rel="noopener noreferrer"&gt;ODSC Europe 2024&lt;/a&gt; I talked about building modern and reliable voice bots using Pipecat, a recently released open source tool. I gave an overview of the general structure of voice bots, of the improvements their underlying tech recently saw, and the new challenges that developers face when implementing one of these systems.&lt;/p&gt;
&lt;p&gt;The main highlight of the talk is the &lt;a href="https://colab.research.google.com/drive/1NCAAs8RB2FuqMChFKMIVWV0RiJr9O3IJ?usp=drive_link" target="_blank" rel="noopener noreferrer"&gt;notebook&lt;/a&gt; where I implement first a simple Pipecat bot from scratch, and then I give an overview of how to blend intent detection and system prompt switching to improve our control of how LLM bots interact with users.&lt;/p&gt;</description></item><item><title>EuroPython: Is RAG all you need? A look at the limits of retrieval augmented generation</title><link>https://www.zansara.dev/talks/2024-07-10-europython-rag/</link><pubDate>Wed, 10 Jul 2024 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/talks/2024-07-10-europython-rag/</guid><description>&lt;p&gt;&lt;a href="https://ep2024.europython.eu/session/is-rag-all-you-need-a-look-at-the-limits-of-retrieval-augmented-generation" target="_blank" rel="noopener noreferrer"&gt;Announcement&lt;/a&gt;, &lt;a href="https://drive.google.com/file/d/13OXMLaBQr1I_za7sqVHJWxRj5xFAg7KV/view?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;slides&lt;/a&gt;. Did you miss the talk? Check out the recording on &lt;a href="https://youtu.be/9wk7mGB_Gp4?feature=shared" target="_blank" rel="noopener noreferrer"&gt;Youtube&lt;/a&gt; or on my &lt;a href="https://drive.google.com/file/d/1OkYQ7WMt63QkdJTU3GIpSxBZmnLfZti6/view?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;backup&lt;/a&gt; (cut from the &lt;a href="https://www.youtube.com/watch?v=tcXmnCJIvFc" target="_blank" rel="noopener noreferrer"&gt;original stream&lt;/a&gt;), or read the &lt;a href="/posts/2024-04-29-odsc-east-rag"&gt;write-up&lt;/a&gt; of a previous edition of the same talk.&lt;/p&gt;
&lt;hr /&gt;
&lt;iframe src="https://drive.google.com/file/d/1OkYQ7WMt63QkdJTU3GIpSxBZmnLfZti6/preview" width="800" height="500" allow="autoplay"&gt;&lt;/iframe&gt;

&lt;p&gt;At &lt;a href="https://ep2024.europython.eu/" target="_blank" rel="noopener noreferrer"&gt;EuroPython 2024&lt;/a&gt; I talked about RAG: how it works, how it fails, and how to evaluate its performance objectively. I gave an overview of some useful open-source tools for RAG evalution such as &lt;a href="https://docs.relari.ai/v0.3?utm_campaign=europython-2024" target="_blank" rel="noopener noreferrer"&gt;continuous-eval&lt;/a&gt; and how to use them with &lt;a href="https://haystack.deepset.ai/?utm_campaign=europython-2024" target="_blank" rel="noopener noreferrer"&gt;Haystack&lt;/a&gt;, and then offered some ideas on how to expand your RAG architecture further than a simple two-step process.&lt;/p&gt;
&lt;p&gt;Some resources mentioned in the talk:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://haystack.deepset.ai/?utm_campaign=europython-2024" target="_blank" rel="noopener noreferrer"&gt;Haystack&lt;/a&gt;: open-source LLM framework for RAG and beyond.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.relari.ai/v0.3?utm_campaign=europython-2024" target="_blank" rel="noopener noreferrer"&gt;continuous-eval&lt;/a&gt; by &lt;a href="https://www.relari.ai/?utm_campaign=europython-2024" target="_blank" rel="noopener noreferrer"&gt;Relari AI&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Build and evaluate RAG with Haystack: &lt;a href="https://haystack.deepset.ai/tutorials/35_model_based_evaluation_of_rag_pipelines/?utm_campaign=europython-2024" target="_blank" rel="noopener noreferrer"&gt;https://haystack.deepset.ai/tutorials/35_model_based_evaluation_of_rag_pipelines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Use continuous-eval with Haystack: &lt;a href="https://github.com/relari-ai/examples/blob/main/examples/haystack/simple_rag/app.py" target="_blank" rel="noopener noreferrer"&gt;https://github.com/relari-ai/examples/blob/main/examples/haystack/simple_rag/app.py&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Perplexity.ai: &lt;a href="https://www.perplexity.ai/" target="_blank" rel="noopener noreferrer"&gt;https://www.perplexity.ai/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>ODSC East: RAG, the bad parts (and the good!)</title><link>https://www.zansara.dev/talks/2024-04-25-odsc-east-rag/</link><pubDate>Thu, 25 Apr 2024 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/talks/2024-04-25-odsc-east-rag/</guid><description>&lt;p&gt;&lt;a href="https://odsc.com/speakers/rag-the-bad-parts-and-the-good-building-a-deeper-understanding-of-this-hot-llm-paradigms-weaknesses-strengths-and-limitations/" target="_blank" rel="noopener noreferrer"&gt;Announcement&lt;/a&gt;, &lt;a href="https://drive.google.com/file/d/19EDFCqOiAo9Cvx5fxx6Wq1Z-EoMKwxbs/view?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;slides&lt;/a&gt;. Did you miss the talk? Check out the &lt;a href="/posts/2024-04-29-odsc-east-rag"&gt;write-up&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;At &lt;a href="https://odsc.com/boston/" target="_blank" rel="noopener noreferrer"&gt;ODSC East 2024&lt;/a&gt; I talked about RAG: how it works, how it fails, and how to evaluate its performance objectively. I gave an overview of some useful open-source tools for RAG evalution and how to use them with &lt;a href="https://haystack.deepset.ai/?utm_campaign=odsc-east" target="_blank" rel="noopener noreferrer"&gt;Haystack&lt;/a&gt;, and then offered some ideas on how to expand your RAG architecture further than a simple two-step process.&lt;/p&gt;
&lt;p&gt;Some resources mentioned in the talk:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Haystack: open-source LLM framework for RAG and beyond: &lt;a href="https://haystack.deepset.ai/?utm_campaign=odsc-east" target="_blank" rel="noopener noreferrer"&gt;https://haystack.deepset.ai/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Build and evaluate RAG with Haystack: &lt;a href="https://haystack.deepset.ai/tutorials/35_model_based_evaluation_of_rag_pipelines/?utm_campaign=odsc-east" target="_blank" rel="noopener noreferrer"&gt;https://haystack.deepset.ai/tutorials/35_model_based_evaluation_of_rag_pipelines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Evaluating LLMs with UpTrain: &lt;a href="https://docs.uptrain.ai/getting-started/introduction" target="_blank" rel="noopener noreferrer"&gt;https://docs.uptrain.ai/getting-started/introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Evaluating RAG end-to-end with RAGAS: &lt;a href="https://docs.ragas.io/en/latest/" target="_blank" rel="noopener noreferrer"&gt;https://docs.ragas.io/en/latest/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Semantic Answer Similarity (SAS) metric: &lt;a href="https://docs.ragas.io/en/latest/concepts/metrics/semantic_similarity.html" target="_blank" rel="noopener noreferrer"&gt;https://docs.ragas.io/en/latest/concepts/metrics/semantic_similarity.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Answer Correctness metric: &lt;a href="https://docs.ragas.io/en/latest/concepts/metrics/answer_correctness.html" target="_blank" rel="noopener noreferrer"&gt;https://docs.ragas.io/en/latest/concepts/metrics/answer_correctness.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Perplexity.ai: &lt;a href="https://www.perplexity.ai/" target="_blank" rel="noopener noreferrer"&gt;https://www.perplexity.ai/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Plus, shout-out to a very interesting LLM evaluation library I discovered at ODSC: &lt;a href="https://docs.relari.ai/v0.3" target="_blank" rel="noopener noreferrer"&gt;continuous-eval&lt;/a&gt;. Worth checking out especially if SAS or answer correctness are too vague and high level for your domain.&lt;/p&gt;</description></item><item><title>DataHour: Optimizing LLMs with Retrieval Augmented Generation and Haystack 2.0</title><link>https://www.zansara.dev/talks/2023-12-15-datahour-rag/</link><pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/talks/2023-12-15-datahour-rag/</guid><description>&lt;p&gt;&lt;a href="https://drive.google.com/file/d/1OkFr4u9ZOraJRF406IQgQh4YC8GLHbzA/view?usp=drive_link" target="_blank" rel="noopener noreferrer"&gt;Recording&lt;/a&gt;, &lt;a href="https://drive.google.com/file/d/1n1tbiUW2wZPGC49WK9pYEIZlZuCER-hu/view?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;slides&lt;/a&gt;, &lt;a href="https://drive.google.com/file/d/17FXuS7X70UF02IYmOr-yEDQYg_gp9cFv/view?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;Colab&lt;/a&gt;, &lt;a href="https://gist.github.com/ZanSara/6075d418c1494e780f7098db32bc6cf6" target="_blank" rel="noopener noreferrer"&gt;gist&lt;/a&gt;. All the material can also be found on &lt;a href="https://community.analyticsvidhya.com/c/datahour/optimizing-llms-with-retrieval-augmented-generation-and-haystack-2-0" target="_blank" rel="noopener noreferrer"&gt;Analytics Vidhya's community&lt;/a&gt; and on &lt;a href="https://drive.google.com/drive/folders/1KwCEDTCsm9hrRaFUPHpzdTpVsOJSnvGk?usp=drive_link" target="_blank" rel="noopener noreferrer"&gt;my backup&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;iframe src="https://drive.google.com/file/d/1OkFr4u9ZOraJRF406IQgQh4YC8GLHbzA/preview" width="800" height="500" allow="autoplay"&gt;&lt;/iframe&gt;

&lt;p&gt;In this hour-long workshop organized by &lt;a href="https://www.analyticsvidhya.com/" target="_blank" rel="noopener noreferrer"&gt;Analytics Vidhya&lt;/a&gt; I give an overview of what RAG is, what problems it solves, and how it works. &lt;/p&gt;
&lt;p&gt;After a brief introduction to Haystack, I show in practice how to use Haystack 2.0 to assemble a Pipeline that performs RAG on a local database and then on the Web with a simple change. &lt;/p&gt;
&lt;p&gt;I also mention how to use and implement custom Haystack components, and share a lot of resources on the topic of RAG and Haystack 2.0.&lt;/p&gt;
&lt;p&gt;This was my most popular talk to date, with over a hundred attendees watching live and several questions.&lt;/p&gt;
&lt;p&gt;Other resources mentioned in the talk are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://haystack.deepset.ai/blog/customizing-rag-to-summarize-hacker-news-posts-with-haystack2?utm_campaign=developer-relations&amp;amp;utm_source=data-hour-event&amp;amp;utm_medium=webinar" target="_blank" rel="noopener noreferrer"&gt;Blog post about custom components&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://haystack.deepset.ai/tutorials/28_structured_output_with_loop?utm_campaign=developer-relations&amp;amp;utm_source=data-hour-event&amp;amp;utm_medium=webinar" target="_blank" rel="noopener noreferrer"&gt;LLM structured output example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://haystack.deepset.ai/advent-of-haystack?utm_campaign=developer-relations&amp;amp;utm_source=data-hour-event&amp;amp;utm_medium=webinar" target="_blank" rel="noopener noreferrer"&gt;Advent of Haystack&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Pointer[183]: Haystack, creare LLM Applications in modo facile</title><link>https://www.zansara.dev/talks/2023-12-15-pointerpodcast-haystack/</link><pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/talks/2023-12-15-pointerpodcast-haystack/</guid><description>&lt;p&gt;&lt;a href="https://pointerpodcast.it/p/pointer183-haystack-creare-llm-applications-in-modo-facile-con-stefano-fiorucci-e-sara-zanzottera" target="_blank" rel="noopener noreferrer"&gt;Episode link&lt;/a&gt;. Backup recording &lt;a href="https://drive.google.com/file/d/1BOoAhfvWou_J4J7RstgKAHPs3Pre2YAw/view?usp=sharing" target="_blank" rel="noopener noreferrer"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;The podcast was recorded in Italian for &lt;a href="https://pointerpodcast.it" target="_blank" rel="noopener noreferrer"&gt;PointerPodcast&lt;/a&gt; with &lt;a href="https://www.linkedin.com/in/luca-corbucci-b6156a123/" target="_blank" rel="noopener noreferrer"&gt;Luca Corbucci&lt;/a&gt;, &lt;a href="https://www.linkedin.com/in/eugenio-paluello-851b3280/" target="_blank" rel="noopener noreferrer"&gt;Eugenio Paluello&lt;/a&gt; and &lt;a href="https://www.linkedin.com/in/stefano-fiorucci/" target="_blank" rel="noopener noreferrer"&gt;Stefano Fiorucci&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;div&gt;
&lt;audio controls src="https://hosting.pointerpodcast.it/records/pointer183.mp3" style="width: 100%"&gt;&lt;/audio&gt;
&lt;/div&gt;

&lt;p&gt;Per concludere in grande stile il 2023, in questa puntata ci occupiamo delle LLM che sono state un argomento centrale della scena tech dell'anno che sta per terminare. Abbiamo invitato due esperti del settore, Sara Zanzottera e Stefano Fiorucci.&lt;/p&gt;
&lt;p&gt;Entrambi i nostri ospiti lavorano per deepset come NLP Engineer. Deepset è l'azienda produttrice di Haystack uno dei framework opensource per LLM più noti, che ha da poco raggiunto la versione 2.0 beta. Proprio Haystack è stato uno degli argomenti di cui ci siamo occupati con i nostri ospiti, cercando di capirne le potenzialità. &lt;/p&gt;
&lt;p&gt;Ma è possibile riuscire a lavorare ad un framework di questo tipo rimanendo anche aggiornati sulle novità di un mondo in costante evoluzione? Questa è una delle tante domande a cui Sara e Stefano hanno risposto. Vi interessa il mondo delle LLM? Non perdetevi questa puntata! &lt;/p&gt;</description></item><item><title>Office Hours: RAG Pipelines</title><link>https://www.zansara.dev/talks/2023-10-12-office-hours-rag-pipelines/</link><pubDate>Thu, 12 Oct 2023 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/talks/2023-10-12-office-hours-rag-pipelines/</guid><description>&lt;p&gt;&lt;a href="https://drive.google.com/file/d/1UXGi4raiCQmrxOfOexL-Qh0CVbtiSm89/view?usp=drive_link" target="_blank" rel="noopener noreferrer"&gt;Recording&lt;/a&gt;, &lt;a href="https://gist.github.com/ZanSara/5975901eea972c126f8e1c2341686dfb" target="_blank" rel="noopener noreferrer"&gt;notebook&lt;/a&gt;. All the material can also be found &lt;a href="https://drive.google.com/drive/folders/17CIfoy6c4INs0O_X6YCa3CYXkjRvWm7X?usp=drive_link" target="_blank" rel="noopener noreferrer"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;iframe src="https://drive.google.com/file/d/1UXGi4raiCQmrxOfOexL-Qh0CVbtiSm89/preview" width="800" height="500" allow="autoplay"&gt;&lt;/iframe&gt;

&lt;p&gt;In this &lt;a href="https://discord.com/invite/VBpFzsgRVF" target="_blank" rel="noopener noreferrer"&gt;Office Hours&lt;/a&gt; I walk through the LLM support offered by Haystack 2.0 to this date: Generator, PromptBuilder, and how to connect them to different types of Retrievers to build Retrieval Augmented Generation (RAG) applications. &lt;/p&gt;
&lt;p&gt;In under 40 minutes we start from a simple query to ChatGPT up to a full pipeline that retrieves documents from the Internet, splits them into chunks and feeds them to an LLM to ground its replies.&lt;/p&gt;
&lt;p&gt;The talk indirectly shows also how Pipelines can help users compose these systems quickly, to visualize them, and helps them connect together different parts by producing verbose error messages.&lt;/p&gt;</description></item><item><title>Office Hours: Haystack 2.0</title><link>https://www.zansara.dev/talks/2023-08-03-office-hours-haystack-2.0-status/</link><pubDate>Thu, 03 Aug 2023 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/talks/2023-08-03-office-hours-haystack-2.0-status/</guid><description>&lt;p&gt;&lt;a href="https://drive.google.com/file/d/1PyAlvJ22Z6o1bls07Do5kx2WMTdotsM7/view?usp=drive_link" target="_blank" rel="noopener noreferrer"&gt;Recording&lt;/a&gt;, &lt;a href="https://drive.google.com/file/d/1QFNisUk2HzwRL_27bpr338maxLvDBr9D/preview" target="_blank" rel="noopener noreferrer"&gt;slides&lt;/a&gt;. All the material can also be found &lt;a href="https://drive.google.com/drive/folders/1zmXwxsSgqDgvYf2ptjHocdtzOroqaudw?usp=drive_link" target="_blank" rel="noopener noreferrer"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;iframe src="https://drive.google.com/file/d/1PyAlvJ22Z6o1bls07Do5kx2WMTdotsM7/preview" width="800" height="500" allow="autoplay"&gt;&lt;/iframe&gt;

&lt;p&gt;In this &lt;a href="https://discord.com/invite/VBpFzsgRVF" target="_blank" rel="noopener noreferrer"&gt;Office Hours&lt;/a&gt; I've presented for the first time to our Discord community a preview of the upcoming 2.0 release of Haystack, which has been in the works since the start of the year. As rumors started to arise at the presence of a &lt;code&gt;preview&lt;/code&gt; module in the latest Haystack 1.x releases, we took the opportunity to share this early draft of the project to collect early feedback.&lt;/p&gt;
&lt;p&gt;Haystack 2.0 is a total rewrite that rethinks many of the core concepts of the framework and makes LLMs support its primary concern, but makes sure to support all the usecases its predecessor enabled. The rewrite addresses some well-know, old issues about the pipeline's design, the relationship between the pipeline, its components, and the document stores, and aims at improving drastically the developer experience and the framework's extensibility.&lt;/p&gt;
&lt;p&gt;As the main designer of this rewrite, I walked the community through a slightly re-hashed version of the slide deck I've presented internally just a few days earlier in an All Hands on the same topic.&lt;/p&gt;</description></item><item><title>OpenNLP Meetup: A Practical Introduction to Image Retrieval</title><link>https://www.zansara.dev/talks/2022-12-01-open-nlp-meetup/</link><pubDate>Thu, 01 Dec 2022 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/talks/2022-12-01-open-nlp-meetup/</guid><description>&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=7Idjl3OR0FY" target="_blank" rel="noopener noreferrer"&gt;Youtube link&lt;/a&gt;, &lt;a href="https://gist.github.com/ZanSara/dc4b22e7ffe2a56647e0afba7537c46b" target="_blank" rel="noopener noreferrer"&gt;slides&lt;/a&gt;, &lt;a href="https://gist.github.com/ZanSara/9e8557830cc866fcf43a2c5623688c74" target="_blank" rel="noopener noreferrer"&gt;Colab&lt;/a&gt; (live coding). All the material can also be found &lt;a href="https://drive.google.com/drive/folders/1_3b8PsvykHeM0jSHsMUWQ-4h_VADutcX?usp=drive_link" target="_blank" rel="noopener noreferrer"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;iframe src="https://drive.google.com/file/d/19mxD-xUJ-14G-2XAqXEVpZfqR2MsSZTn/preview" width="800" height="500" allow="autoplay"&gt;&lt;/iframe&gt;

&lt;h2&gt;A Practical Introduction to Image Retrieval&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;by Sara Zanzottera from deepset&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Search should not be limited to text only. Recently, Transformers-based NLP models started crossing the boundaries of text data and exploring the possibilities of other modalities, like tabular data, images, audio files, and more. Text-to-text generation models like GPT now have their counterparts in text-to-image models, like Stable Diffusion. But what about search? In this talk we're going to experiment with CLIP, a text-to-image search model, to look for animals matching specific characteristics in a dataset of pictures. Does CLIP know which one is "The fastest animal in the world"?&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;For the 7th &lt;a href="https://www.meetup.com/open-nlp-meetup/" target="_blank" rel="noopener noreferrer"&gt;OpenNLP meetup&lt;/a&gt; I presented the topic of Image Retrieval, a feature that I've recently added to Haystack in the form of a &lt;a href="https://docs.haystack.deepset.ai/docs/retriever#multimodal-retrieval" target="_blank" rel="noopener noreferrer"&gt;MultiModal Retriever&lt;/a&gt; (see the &lt;a href="https://haystack.deepset.ai/tutorials/19_text_to_image_search_pipeline_with_multimodal_retriever" target="_blank" rel="noopener noreferrer"&gt;Tutorial&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The talk consists of 5 parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An introduction of the topic of Image Retrieval&lt;/li&gt;
&lt;li&gt;A mention of the current SOTA model (CLIP)&lt;/li&gt;
&lt;li&gt;An overview of Haystack&lt;/li&gt;
&lt;li&gt;A step-by-step description of how image retrieval applications can be implemented with Haystack&lt;/li&gt;
&lt;li&gt;A live coding session where I start from a blank Colab notebook and build a fully working image retrieval system from the ground up, to the point where I can run queries live.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Towards the end I mention briefly an even more advanced version of this image retrieval system, which I had no time to implement live. However, I later built a notebook implementing such system and you can find it here: &lt;a href="https://gist.github.com/ZanSara/31ed3fc8252bb74b1952f2d0fe253ed0" target="_blank" rel="noopener noreferrer"&gt;Cheetah.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The slides were generated from the linked Jupyter notebook with &lt;code&gt;jupyter nbconvert Dec_1st_OpenNLP_Meetup.ipynb --to slides --post serve&lt;/code&gt;.&lt;/p&gt;</description></item><item><title>ZanzoCam: An open-source alpine web camera</title><link>https://www.zansara.dev/talks/2021-05-24-zanzocam-pavia/</link><pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate><guid>https://www.zansara.dev/talks/2021-05-24-zanzocam-pavia/</guid><description>&lt;p&gt;Slides: &lt;a href="/talks/2021-05-24-zanzocam-pavia.pdf"&gt;ZanzoCam: An open-source alpine web camera&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;On May 24th 2021 I held a talk about the &lt;a href="https://zanzocam.github.io/en" target="_blank" rel="noopener noreferrer"&gt;ZanzoCam project&lt;/a&gt; as invited speaker for the &lt;a href="http://hsw2021.gnudd.com/" target="_blank" rel="noopener noreferrer"&gt;"Hardware and Software Codesign"&lt;/a&gt; course at &lt;a href="https://portale.unipv.it/it" target="_blank" rel="noopener noreferrer"&gt;Università di Pavia&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The slides go through the entire lifecycle of the &lt;a href="https://zanzocam.github.io/en" target="_blank" rel="noopener noreferrer"&gt;ZanzoCam project&lt;/a&gt;, from the very inception of it, the market research, our decision process, earlier prototypes, and then goes into a more detailed explanation of the design and implementation of the project from a hardware and software perspective, with some notes about our financial situation and project management.&lt;/p&gt;</description></item></channel></rss>