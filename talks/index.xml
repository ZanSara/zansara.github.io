<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Talks on Sara Zan</title>
    <link>https://www.zansara.dev/talks/</link>
    <description>Recent content in Talks on Sara Zan</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Thu, 06 Nov 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://www.zansara.dev/talks/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[UPCOMING] Embrace:AI // 2025.06 - Reasoning LLMs &amp; Multimodal Architecture</title>
      <link>https://www.zansara.dev/talks/2025-11-06-embrace-ai-meetup-reasoning-models/</link>
      <pubDate>Thu, 06 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/talks/2025-11-06-embrace-ai-meetup-reasoning-models/</guid>
      <description></description>
    </item>
    <item>
      <title>[UPCOMING] ODSC West: LLMs that think - Demystifying Reasoning Models</title>
      <link>https://www.zansara.dev/talks/2025-10-29-odsc-west-reasoning-models/</link>
      <pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/talks/2025-10-29-odsc-west-reasoning-models/</guid>
      <description></description>
    </item>
    <item>
      <title>ODSC East: LLMs that think - Demystifying Reasoning Models</title>
      <link>https://www.zansara.dev/talks/2025-05-13-odsc-east-reasoning-models/</link>
      <pubDate>Wed, 14 May 2025 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/talks/2025-05-13-odsc-east-reasoning-models/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://odsc.com/speakers/llms-that-think-demystifying-reasoning-models/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Announcement&lt;/a&gt;, &lt;a href=&#34;https://www.zansara.dev/posts/2025-05-12-beyond-hype-reasoning-models/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;teaser article&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/1Gmxx2G-H0aozBZtACCvGIqJNgybXB716/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;.&#xA;All resources can also be found in&#xA;&lt;a href=&#34;https://drive.google.com/drive/folders/1Iy_mJr7MYdrbb-W-g1U38gkPBjrV8dGu?usp=drive_link&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my archive&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;At &lt;a href=&#34;https://odsc.com/boston/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ODSC East 2025&lt;/a&gt; I talked about reasoning models: what they are, what they aren&amp;rsquo;t, how they work and when to use them. Is OpenAI&amp;rsquo;s o4 AGI? Is it an AI Agent? Or is it just a glorified chain-of-thought prompt under the hood? To answer these questions, I classified LLMs into a small &amp;ldquo;taxonomy&amp;rdquo; based on the post-training steps they go through, in order to highlight how reasoning models differ qualitatively from their predecessors just like instruction-tuned models were not simple text-completion models with a better prompt. I also covered the effect of increasing the reasoning effort of the model, clarifying when it&amp;rsquo;s useful and when it can lead to overthinking.&lt;/p&gt;</description>
    </item>
    <item>
      <title>PyData Global: Building LLM Voice Bots with Open Source Tools</title>
      <link>https://www.zansara.dev/talks/2024-12-03-pydata-global-voice-bots/</link>
      <pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/talks/2024-12-03-pydata-global-voice-bots/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://global2024.pydata.org/cfp/talk/T3YDBP/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Announcement&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/1rXb4-m-BWwhAqDCDBXpzw6nJ9OELOpSl/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/1bja0O8LG7790UIU7HpAYXat-BYXeUbK-/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;demo video&lt;/a&gt; and full video (&lt;a href=&#34;https://www.youtube.com/watch?v=Td5dFdG0wE4&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Youtube&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/1HTEEs-Zr8mZoJA8a7AuiJf61soGvQNPI/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;backup&lt;/a&gt;).&#xA;All resources can also be found in&#xA;&lt;a href=&#34;https://drive.google.com/drive/folders/1ZPkne2QxOnSXfchv08CWkAZG3duXxd4Z?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my archive&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;hahahugoshortcode68s0hbhb&#34;&gt;&#xA;  &lt;div class=&#39;iframe-wrapper&#39;&gt;&#xA;  &lt;iframe src=&#34;https://drive.google.com/file/d/1HTEEs-Zr8mZoJA8a7AuiJf61soGvQNPI/preview&#34; width=100% height=100% allow=&#34;autoplay&#34;&gt;&lt;/iframe&gt;&#xA;&lt;/div&gt;&#xA;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;demo&#34;&gt;&#xA;  Demo&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;During the talk I showed a recording of a demo built with &lt;a href=&#34;https://github.com/intentional-ai/intentional&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intentional&lt;/a&gt;, a new library to prompt voice bots in a way that takes inspiration from classic intent-based chatbots. Here are the instructions needed to run this same demo on your own machine and play with it.&lt;/p&gt;&#xA;&lt;div class=&#34;notice info&#34;&gt;&#xA;  &lt;div class=&#34;notice-content&#34;&gt;&lt;p&gt;&lt;em&gt;Intentional is still in its very first stages of development and highly unstable!&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>ODSC West: Building Reliable Voice Agents with Open Source tools</title>
      <link>https://www.zansara.dev/talks/2024-10-29-odsc-west-voice-agents/</link>
      <pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/talks/2024-10-29-odsc-west-voice-agents/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://odsc.com/speakers/building-reliable-voice-agents-with-open-source-tools-2/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Announcement&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/1H_8PKQY_kFIzwxakZc-RJU8Rtwnjajhl/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/drive/1CUX7JRYMU1MEJBZ6lWMg5EThPew19Zjs?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;notebook&lt;/a&gt;.&#xA;All resources can also be found in&#xA;&lt;a href=&#34;https://drive.google.com/drive/folders/1baO1Gv55CIjLT-KPWtSm2z4IOBdXkHjn?usp=drive_link&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my archive&lt;/a&gt;.&#xA;Did you miss the talk? Check out the write-up&amp;rsquo;s&#xA;&lt;a href=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/&#34; &gt;part 1&lt;/a&gt;&#xA;and &lt;a href=&#34;https://www.zansara.dev/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/&#34; &gt;part 2&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;hahahugoshortcode72s0hbhb&#34;&gt;&#xA;  &lt;div class=&#39;iframe-wrapper&#39;&gt;&#xA;  &lt;iframe src=&#34;https://drive.google.com/file/d/1uQTJFmHTinvUvCMHRq9rrbaClNw9I9i2/preview&#34; width=100% height=100% allow=&#34;autoplay&#34;&gt;&lt;/iframe&gt;&#xA;&lt;/div&gt;&#xA;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;At &lt;a href=&#34;https://odsc.com/california/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ODSC West 2024&lt;/a&gt; I talked about building modern and reliable voice bots using Pipecat,&#xA;a recently released open source tool. I gave an overview of the general structure of voice bots, of the improvements&#xA;their underlying tech recently saw, from the first Whisper release to GPT 4o Realtime, and the new challenges that&#xA;developers face when implementing one of these systems.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SNAIL Opening Day: Should I use an LLM Framework? (Private Event)</title>
      <link>https://www.zansara.dev/talks/2024-10-01-snail-opening-day-should-i-use-an-llm-framework/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/talks/2024-10-01-snail-opening-day-should-i-use-an-llm-framework/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1GQJ1qEY2hXQ6EBF-rtqzJqZzidfS7HfI/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides&lt;/a&gt;,&#xA;&lt;a href=&#34;https://colab.research.google.com/drive/11aOq-43wEWhSlxtkdXEAwPEarC0IQ3eN?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;notebook&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/ZanSara/seven-wonders&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RAG dataset&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/datasets/ZanSara/seven-wonders-eval&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;evaluation dataset&lt;/a&gt;&#xA;All resources can also be found in&#xA;&lt;a href=&#34;https://drive.google.com/drive/folders/1anl3adpxgbwq5nsFn8QXuofIWXX0jRKo?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my archive&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;div class=&#39;iframe-wrapper&#39;&gt;&#xA;  &lt;iframe src=&#34;https://drive.google.com/file/d/1AORVusaHVBqNvJ5OtctyB5TWQZSadoqT/preview&#34; width=100% height=100% allow=&#34;autoplay&#34;&gt;&lt;/iframe&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;p&gt;Find the transcript &lt;a href=&#34;https://drive.google.com/file/d/1wwnTFmGOANVmxUaVd1PC3cfztzIfSCEa/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;For the &lt;a href=&#34;https://group.springernature.com/gp/group&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Springer Nature&lt;/a&gt; AI Lab Opening Day I talk about LLM frameworks: what they are, when they can be useful, and how to choose and compare one framework to the other.&lt;/p&gt;&#xA;&lt;p&gt;After an overview of six application frameworks (&lt;a href=&#34;https://www.langchain.com/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LangChain&lt;/a&gt;, &lt;a href=&#34;https://www.llamaindex.ai/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LlamaIndex&lt;/a&gt;, &lt;a href=&#34;https://haystack.deepset.ai/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haystack&lt;/a&gt;, &lt;a href=&#34;https://neuml.github.io/txtai/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;txtai&lt;/a&gt;, &lt;a href=&#34;https://dspy-docs.vercel.app/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DSPy&lt;/a&gt; and &lt;a href=&#34;https://www.crewai.com/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CrewAI&lt;/a&gt;), we run a notebook where we used &lt;a href=&#34;https://docs.ragas.io/en/latest/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RAGAS&lt;/a&gt; to compare four small RAG applications and see which one performs better.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AMTA 2024 Virtual Tutorial Day: Controlling LLM Translations of Invariant Elements with RAG</title>
      <link>https://www.zansara.dev/talks/2024-09-18-amta-2024-controlling-invariants-rag/</link>
      <pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/talks/2024-09-18-amta-2024-controlling-invariants-rag/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://amtaweb.org/virtual-tutorial-day-program/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Announcement&lt;/a&gt;,&#xA;&lt;a href=&#34;https://colab.research.google.com/drive/1VMgK3DcVny_zTtAG_V3QSSdfSFBWAgmb?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;notebook&lt;/a&gt; and&#xA;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1A1zk-u-RTSqBfE8LksZxihnp7KxWO7YK/edit?usp=sharing&amp;amp;ouid=102297935451395786183&amp;amp;rtpof=true&amp;amp;sd=true&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;glossary&lt;/a&gt;.&#xA;All resources can also be found in&#xA;&lt;a href=&#34;https://drive.google.com/drive/folders/1Tdq92P_E_77sErGjz7jSPfJ-or9UZXvn?usp=drive_link&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my archive&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;div class=&#39;iframe-wrapper&#39;&gt;&#xA;  &lt;iframe src=&#34;https://drive.google.com/file/d/1BvcNbsAGWp25EDpiQ5ljYos3_eneo3wu/preview&#34; width=100% height=100% allow=&#34;autoplay&#34;&gt;&lt;/iframe&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Note: this was a tutorial session co-presented with &lt;a href=&#34;https://www.linkedin.com/in/christian-lang-8942b0145/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Christian Lang&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/brunobitter/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bruno Bitter&lt;/a&gt;. My section starts at 01:08.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;At the &lt;a href=&#34;https://amtaweb.org/virtual-tutorial-day-program/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AMTA 2024 Virtual Tutorial Day&lt;/a&gt; I talked about controlling invariant translation elements with RAG. During the talk several speakers intervened on the topic, each bringing a different perspective of it.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/georgkirchner/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Georg Kirchner&lt;/a&gt; introduced the concept of invariant translation elements, such as brand names, UI elements, and corporate slogans. &lt;a href=&#34;https://www.linkedin.com/in/christian-lang-8942b0145/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Christian Lang&lt;/a&gt; gave a comprehensive overview of the challenges of handling invariant translation elements with existing tools and how LLMs can help at various stages of the translation, covering several approaches, including RAG. Building on his overview, I showed how to implement a simple RAG system to handle these invariants properly using &lt;a href=&#34;https://haystack.deepset.ai/?utm_campaign=amta-2024&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haystack&lt;/a&gt;: we run a &lt;a href=&#34;https://colab.research.google.com/drive/1VMgK3DcVny_zTtAG_V3QSSdfSFBWAgmb?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colab notebook&lt;/a&gt; live and checked how the translation changes by introducing context about the invariants to the LLM making the translation. Last, &lt;a href=&#34;https://www.linkedin.com/in/brunobitter/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bruno Bitter&lt;/a&gt; gave an overview of how you can use &lt;a href=&#34;https://www.blackbird.io/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blackbird&lt;/a&gt; to integrate a system like this with existing CAT tools and manage the whole lifecycle of content translation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ODSC Europe: Building Reliable Voice Agents with Open Source tools</title>
      <link>https://www.zansara.dev/talks/2024-09-05-odsc-europe-voice-agents/</link>
      <pubDate>Fri, 06 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/talks/2024-09-05-odsc-europe-voice-agents/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://odsc.com/speakers/building-reliable-voice-agents-with-open-source-tools-2/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Announcement&lt;/a&gt;,&#xA;&lt;a href=&#34;https://drive.google.com/file/d/1ubk7Q_l9C7epQgYrMttHMjW1AVfdm-LT/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt; and&#xA;&lt;a href=&#34;https://colab.research.google.com/drive/1NCAAs8RB2FuqMChFKMIVWV0RiJr9O3IJ?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;notebook&lt;/a&gt;.&#xA;All resources can also be found on ODSC&amp;rsquo;s website and in&#xA;&lt;a href=&#34;https://drive.google.com/drive/folders/1rrXMTbfTZVuq9pMzneC8j-5GKdRQ6l2i?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my archive&lt;/a&gt;.&#xA;Did you miss the talk? Check out the write-up&amp;rsquo;s&#xA;&lt;a href=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/&#34; &gt;part 1&lt;/a&gt;&#xA;and &lt;a href=&#34;https://www.zansara.dev/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/&#34; &gt;part 2&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;&lt;em&gt;(Note: this is a recording of the notebook walkthrough only. The full recording will be shared soon).&lt;/em&gt;&lt;/p&gt;&#xA;&lt;div class=&#39;iframe-wrapper&#39;&gt;&#xA;  &lt;iframe src=&#34;https://drive.google.com/file/d/15Kv8THmDsnnzfVBhHAf2O11RccpzAzYK/preview&#34; width=100% height=100% allow=&#34;autoplay&#34;&gt;&lt;/iframe&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;p&gt;At &lt;a href=&#34;https://odsc.com/europe/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ODSC Europe 2024&lt;/a&gt; I talked about building modern and reliable voice bots using Pipecat,&#xA;a recently released open source tool. I gave an overview of the general structure of voice bots, of the improvements&#xA;their underlying tech recently saw, and the new challenges that developers face when implementing one of these systems.&lt;/p&gt;</description>
    </item>
    <item>
      <title>EuroPython: Is RAG all you need? A look at the limits of retrieval augmented generation</title>
      <link>https://www.zansara.dev/talks/2024-07-10-europython-rag/</link>
      <pubDate>Wed, 10 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/talks/2024-07-10-europython-rag/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://ep2024.europython.eu/session/is-rag-all-you-need-a-look-at-the-limits-of-retrieval-augmented-generation&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Announcement&lt;/a&gt;,&#xA;&lt;a href=&#34;https://drive.google.com/file/d/13OXMLaBQr1I_za7sqVHJWxRj5xFAg7KV/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;.&#xA;Did you miss the talk? Check out the recording on &lt;a href=&#34;https://youtu.be/9wk7mGB_Gp4?feature=shared&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Youtube&lt;/a&gt;&#xA;or on my &lt;a href=&#34;https://drive.google.com/file/d/1OkYQ7WMt63QkdJTU3GIpSxBZmnLfZti6/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;backup&lt;/a&gt; (cut from the &lt;a href=&#34;https://www.youtube.com/watch?v=tcXmnCJIvFc&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;original stream&lt;/a&gt;),&#xA;or read the &lt;a href=&#34;https://www.zansara.dev/posts/2024-04-29-odsc-east-rag&#34; &gt;write-up&lt;/a&gt; of a previous edition of the same talk.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;div class=&#39;iframe-wrapper&#39;&gt;&#xA;  &lt;iframe src=&#34;https://drive.google.com/file/d/1OkYQ7WMt63QkdJTU3GIpSxBZmnLfZti6/preview&#34; width=100% height=100% allow=&#34;autoplay&#34;&gt;&lt;/iframe&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;p&gt;At &lt;a href=&#34;https://ep2024.europython.eu/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EuroPython 2024&lt;/a&gt; I talked about RAG: how it works, how it fails, and how to evaluate its performance objectively. I gave an overview of some useful open-source tools for RAG evalution such as &lt;a href=&#34;https://docs.relari.ai/v0.3?utm_campaign=europython-2024&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;continuous-eval&lt;/a&gt; and how to use them with &lt;a href=&#34;https://haystack.deepset.ai/?utm_campaign=europython-2024&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haystack&lt;/a&gt;, and then offered some ideas on how to expand your RAG architecture further than a simple two-step process.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ODSC East: RAG, the bad parts (and the good!)</title>
      <link>https://www.zansara.dev/talks/2024-04-25-odsc-east-rag/</link>
      <pubDate>Thu, 25 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/talks/2024-04-25-odsc-east-rag/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://odsc.com/speakers/rag-the-bad-parts-and-the-good-building-a-deeper-understanding-of-this-hot-llm-paradigms-weaknesses-strengths-and-limitations/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Announcement&lt;/a&gt;,&#xA;&lt;a href=&#34;https://drive.google.com/file/d/19EDFCqOiAo9Cvx5fxx6Wq1Z-EoMKwxbs/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;.&#xA;Did you miss the talk? Check out the &lt;a href=&#34;https://www.zansara.dev/posts/2024-04-29-odsc-east-rag&#34; &gt;write-up&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;At &lt;a href=&#34;https://odsc.com/boston/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ODSC East 2024&lt;/a&gt; I talked about RAG: how it works, how it fails, and how to evaluate its performance objectively. I gave an overview of some useful open-source tools for RAG evalution and how to use them with &lt;a href=&#34;https://haystack.deepset.ai/?utm_campaign=odsc-east&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haystack&lt;/a&gt;, and then offered some ideas on how to expand your RAG architecture further than a simple two-step process.&lt;/p&gt;&#xA;&lt;p&gt;Some resources mentioned in the talk:&lt;/p&gt;</description>
    </item>
    <item>
      <title>DataHour: Optimizing LLMs with Retrieval Augmented Generation and Haystack 2.0</title>
      <link>https://www.zansara.dev/talks/2023-12-15-datahour-rag/</link>
      <pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/talks/2023-12-15-datahour-rag/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1OkFr4u9ZOraJRF406IQgQh4YC8GLHbzA/view?usp=drive_link&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recording&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/1n1tbiUW2wZPGC49WK9pYEIZlZuCER-hu/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/17FXuS7X70UF02IYmOr-yEDQYg_gp9cFv/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colab&lt;/a&gt;, &lt;a href=&#34;https://gist.github.com/ZanSara/6075d418c1494e780f7098db32bc6cf6&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gist&lt;/a&gt;. All the material can also be found on &lt;a href=&#34;https://community.analyticsvidhya.com/c/datahour/optimizing-llms-with-retrieval-augmented-generation-and-haystack-2-0&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Analytics Vidhya&amp;rsquo;s community&lt;/a&gt; and on &lt;a href=&#34;https://drive.google.com/drive/folders/1KwCEDTCsm9hrRaFUPHpzdTpVsOJSnvGk?usp=drive_link&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my backup&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;div class=&#39;iframe-wrapper&#39;&gt;&#xA;  &lt;iframe src=&#34;https://drive.google.com/file/d/1OkFr4u9ZOraJRF406IQgQh4YC8GLHbzA/preview&#34; width=100% height=100% allow=&#34;autoplay&#34;&gt;&lt;/iframe&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;p&gt;In this hour-long workshop organized by &lt;a href=&#34;https://www.analyticsvidhya.com/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Analytics Vidhya&lt;/a&gt; I give an overview of what RAG is, what problems it solves, and how it works.&lt;/p&gt;&#xA;&lt;p&gt;After a brief introduction to Haystack, I show in practice how to use Haystack 2.0 to assemble a Pipeline that performs RAG on a local database and then on the Web with a simple change.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pointer[183]: Haystack, creare LLM Applications in modo facile</title>
      <link>https://www.zansara.dev/talks/2023-12-15-pointerpodcast-haystack/</link>
      <pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/talks/2023-12-15-pointerpodcast-haystack/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://pointerpodcast.it/p/pointer183-haystack-creare-llm-applications-in-modo-facile-con-stefano-fiorucci-e-sara-zanzottera&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Episode link&lt;/a&gt;. Backup recording &lt;a href=&#34;https://drive.google.com/file/d/1BOoAhfvWou_J4J7RstgKAHPs3Pre2YAw/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;&lt;em&gt;The podcast was recorded in Italian for &lt;a href=&#34;https://pointerpodcast.it&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PointerPodcast&lt;/a&gt; with &lt;a href=&#34;https://www.linkedin.com/in/luca-corbucci-b6156a123/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Luca Corbucci&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/eugenio-paluello-851b3280/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Eugenio Paluello&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/stefano-fiorucci/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stefano Fiorucci&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&#xA;&#xA;&lt;div&gt;&#xA;&lt;audio controls src=&#34;https://hosting.pointerpodcast.it/records/pointer183.mp3&#34; style=&#34;width: 100%&#34;&gt;&lt;/audio&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;p&gt;Per concludere in grande stile il 2023, in questa puntata ci occupiamo delle LLM che sono state un argomento centrale della scena tech dell&amp;rsquo;anno che sta per terminare. Abbiamo invitato due esperti del settore, Sara Zanzottera e Stefano Fiorucci.&lt;/p&gt;&#xA;&lt;p&gt;Entrambi i nostri ospiti lavorano per deepset come NLP Engineer. Deepset è l&amp;rsquo;azienda produttrice di Haystack uno dei framework opensource per LLM più noti, che ha da poco raggiunto la versione 2.0 beta. Proprio Haystack è stato uno degli argomenti di cui ci siamo occupati con i nostri ospiti, cercando di capirne le potenzialità.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Office Hours: RAG Pipelines</title>
      <link>https://www.zansara.dev/talks/2023-10-12-office-hours-rag-pipelines/</link>
      <pubDate>Thu, 12 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/talks/2023-10-12-office-hours-rag-pipelines/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1UXGi4raiCQmrxOfOexL-Qh0CVbtiSm89/view?usp=drive_link&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recording&lt;/a&gt;, &lt;a href=&#34;https://gist.github.com/ZanSara/5975901eea972c126f8e1c2341686dfb&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;notebook&lt;/a&gt;. All the material can also be found &lt;a href=&#34;https://drive.google.com/drive/folders/17CIfoy6c4INs0O_X6YCa3CYXkjRvWm7X?usp=drive_link&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;div class=&#39;iframe-wrapper&#39;&gt;&#xA;  &lt;iframe src=&#34;https://drive.google.com/file/d/1UXGi4raiCQmrxOfOexL-Qh0CVbtiSm89/preview&#34; width=100% height=100% allow=&#34;autoplay&#34;&gt;&lt;/iframe&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;p&gt;In this &lt;a href=&#34;https://discord.com/invite/VBpFzsgRVF&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Office Hours&lt;/a&gt; I walk through the LLM support offered by Haystack 2.0 to this date: Generator, PromptBuilder, and how to connect them to different types of Retrievers to build Retrieval Augmented Generation (RAG) applications.&lt;/p&gt;&#xA;&lt;p&gt;In under 40 minutes we start from a simple query to ChatGPT up to a full pipeline that retrieves documents from the Internet, splits them into chunks and feeds them to an LLM to ground its replies.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Office Hours: Haystack 2.0</title>
      <link>https://www.zansara.dev/talks/2023-08-03-office-hours-haystack-2.0-status/</link>
      <pubDate>Thu, 03 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/talks/2023-08-03-office-hours-haystack-2.0-status/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1PyAlvJ22Z6o1bls07Do5kx2WMTdotsM7/view?usp=drive_link&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recording&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/1QFNisUk2HzwRL_27bpr338maxLvDBr9D/preview&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;. All the material can also be found &lt;a href=&#34;https://drive.google.com/drive/folders/1zmXwxsSgqDgvYf2ptjHocdtzOroqaudw?usp=drive_link&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;div class=&#39;iframe-wrapper&#39;&gt;&#xA;  &lt;iframe src=&#34;https://drive.google.com/file/d/1PyAlvJ22Z6o1bls07Do5kx2WMTdotsM7/preview&#34; width=100% height=100% allow=&#34;autoplay&#34;&gt;&lt;/iframe&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;p&gt;In this &lt;a href=&#34;https://discord.com/invite/VBpFzsgRVF&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Office Hours&lt;/a&gt; I&amp;rsquo;ve presented for the first time to our Discord community a preview of the upcoming 2.0 release of Haystack, which has been in the works since the start of the year. As rumors started to arise at the presence of a &lt;code&gt;preview&lt;/code&gt; module in the latest Haystack 1.x releases, we took the opportunity to share this early draft of the project to collect early feedback.&lt;/p&gt;</description>
    </item>
    <item>
      <title>OpenNLP Meetup: A Practical Introduction to Image Retrieval</title>
      <link>https://www.zansara.dev/talks/2022-12-01-open-nlp-meetup/</link>
      <pubDate>Thu, 01 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/talks/2022-12-01-open-nlp-meetup/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=7Idjl3OR0FY&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Youtube link&lt;/a&gt;,&#xA;&lt;a href=&#34;https://gist.github.com/ZanSara/dc4b22e7ffe2a56647e0afba7537c46b&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;, &lt;a href=&#34;https://gist.github.com/ZanSara/9e8557830cc866fcf43a2c5623688c74&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colab&lt;/a&gt; (live coding).&#xA;All the material can also be found &lt;a href=&#34;https://drive.google.com/drive/folders/1_3b8PsvykHeM0jSHsMUWQ-4h_VADutcX?usp=drive_link&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;div class=&#39;iframe-wrapper&#39;&gt;&#xA;  &lt;iframe src=&#34;https://drive.google.com/file/d/19mxD-xUJ-14G-2XAqXEVpZfqR2MsSZTn/preview&#34; width=100% height=100% allow=&#34;autoplay&#34;&gt;&lt;/iframe&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;h2 id=&#34;a-practical-introduction-to-image-retrieval&#34;&gt;&#xA;  A Practical Introduction to Image Retrieval&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;by Sara Zanzottera from deepset&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Search should not be limited to text only. Recently, Transformers-based NLP models started crossing the boundaries of text data and exploring the possibilities of other modalities, like tabular data, images, audio files, and more. Text-to-text generation models like GPT now have their counterparts in text-to-image models, like Stable Diffusion. But what about search? In this talk we&amp;rsquo;re going to experiment with CLIP, a text-to-image search model, to look for animals matching specific characteristics in a dataset of pictures. Does CLIP know which one is &amp;ldquo;The fastest animal in the world&amp;rdquo;?&lt;/p&gt;</description>
    </item>
    <item>
      <title>ZanzoCam: An open-source alpine web camera</title>
      <link>https://www.zansara.dev/talks/2021-05-24-zanzocam-pavia/</link>
      <pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate>
      <guid>https://www.zansara.dev/talks/2021-05-24-zanzocam-pavia/</guid>
      <description>&lt;p&gt;Slides: &lt;a href=&#34;https://www.zansara.dev/talks/2021-05-24-zanzocam-pavia.pdf&#34; &gt;ZanzoCam: An open-source alpine web camera&lt;/a&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;On May 24th 2021 I held a talk about the &lt;a href=&#34;https://zanzocam.github.io/en&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ZanzoCam project&lt;/a&gt;&#xA;as invited speaker for the &lt;a href=&#34;http://hsw2021.gnudd.com/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Hardware and Software Codesign&amp;rdquo;&lt;/a&gt; course at&#xA;&lt;a href=&#34;https://portale.unipv.it/it&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Università di Pavia&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The slides go through the entire lifecycle of the &lt;a href=&#34;https://zanzocam.github.io/en&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ZanzoCam project&lt;/a&gt;,&#xA;from the very inception of it, the market research, our decision process, earlier prototypes, and&#xA;then goes into a more detailed explanation of the the design and implementation of the project from&#xA;a hardware and software perspective, with some notes about our financial situation and project management.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
