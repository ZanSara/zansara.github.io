<!DOCTYPE html>
<html lang="en">

<head>
  <title>
  OpenNLP Meetup: A Practical Introduction to Image Retrieval · Sara Zan
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Sara Zan">
<meta name="description" content="Youtube link,
slides, Colab (live coding).
All the material can also be found here.


  



  A Practical Introduction to Image Retrieval
  

by Sara Zanzottera from deepset
Search should not be limited to text only. Recently, Transformers-based NLP models started crossing the boundaries of text data and exploring the possibilities of other modalities, like tabular data, images, audio files, and more. Text-to-text generation models like GPT now have their counterparts in text-to-image models, like Stable Diffusion. But what about search? In this talk we&rsquo;re going to experiment with CLIP, a text-to-image search model, to look for animals matching specific characteristics in a dataset of pictures. Does CLIP know which one is &ldquo;The fastest animal in the world&rdquo;?">
<meta name="keywords" content="blog,developer,personal,python,llm,nlp,swe,software-engineering,open-source,ai,genai">


  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="OpenNLP Meetup: A Practical Introduction to Image Retrieval">
  <meta name="twitter:description" content="Youtube link, slides, Colab (live coding). All the material can also be found here.
A Practical Introduction to Image Retrieval by Sara Zanzottera from deepset
Search should not be limited to text only. Recently, Transformers-based NLP models started crossing the boundaries of text data and exploring the possibilities of other modalities, like tabular data, images, audio files, and more. Text-to-text generation models like GPT now have their counterparts in text-to-image models, like Stable Diffusion. But what about search? In this talk we’re going to experiment with CLIP, a text-to-image search model, to look for animals matching specific characteristics in a dataset of pictures. Does CLIP know which one is “The fastest animal in the world”?">

<meta property="og:url" content="https://www.zansara.dev/talks/2022-12-01-open-nlp-meetup/">
  <meta property="og:site_name" content="Sara Zan">
  <meta property="og:title" content="OpenNLP Meetup: A Practical Introduction to Image Retrieval">
  <meta property="og:description" content="Youtube link, slides, Colab (live coding). All the material can also be found here.
A Practical Introduction to Image Retrieval by Sara Zanzottera from deepset
Search should not be limited to text only. Recently, Transformers-based NLP models started crossing the boundaries of text data and exploring the possibilities of other modalities, like tabular data, images, audio files, and more. Text-to-text generation models like GPT now have their counterparts in text-to-image models, like Stable Diffusion. But what about search? In this talk we’re going to experiment with CLIP, a text-to-image search model, to look for animals matching specific characteristics in a dataset of pictures. Does CLIP know which one is “The fastest animal in the world”?">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="talks">
    <meta property="article:published_time" content="2022-12-01T00:00:00+00:00">
    <meta property="article:modified_time" content="2022-12-01T00:00:00+00:00">


<meta name="msvalidate.01" content="CD2BB9B57B16AF914327870432D856C1" />
<meta name="yandex-verification" content="a886d3d5d2b57cb5" />


<link rel="canonical" href="https://www.zansara.dev/talks/2022-12-01-open-nlp-meetup/">


<link rel="preload" href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as="font" type="font/woff2" crossorigin>


  <link rel="stylesheet" href="/css/coder.css" crossorigin="anonymous" media="screen" />






  
    <link rel="stylesheet" href="/css/coder-dark.css" crossorigin="anonymous" media="screen" />
  



 

<link rel="icon" type="image/svg+xml" href="/me/avatar.svg" sizes="any">
<link rel="icon" type="image/png" href="/me/avatar.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/me/avatar.png">
<link rel="apple-touch-icon" sizes="180x180" href="/me/avatar.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">










  
  <meta name="image" content="/talks/2022-12-01-open-nlp-meetup.png">
  <meta name="og:image" content="/talks/2022-12-01-open-nlp-meetup.png">
  <meta name="twitter:image" content="/talks/2022-12-01-open-nlp-meetup.png">
  

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+HK:wght@200..900&family=Noto+Serif+Hebrew:wght@100..900&family=Noto+Naskh+Arabic:wght@400..700&family=Noto+Serif+JP&family=Noto+Serif+KR&family=Noto+Serif+SC&family=Noto+Serif+TC&family=Noto+Serif+Thai:wght@100..900&family=Noto+Serif:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

  <script data-goatcounter="https://zansaradev.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="/">
      <img src="https://www.zansara.dev/me/avatar.svg" style="width: 1em; height: 1em; margin-right: 5px;">
      Sara Zan&#39;s Blog
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list" >
        
          
            <li class="navigation-item">
              <a class="navigation-link" style="height: 30px;" href="/about">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" style="height: 30px;" href="/posts/">Posts</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" style="height: 30px;" href="/projects/">Projects</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" style="height: 30px;" href="/publications/">Publications</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" style="height: 30px;" href="/talks/">Talks</a>
            </li>
          
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="https://www.zansara.dev/talks/2022-12-01-open-nlp-meetup/">
              OpenNLP Meetup: A Practical Introduction to Image Retrieval
            </a>
          </h1>
        </div>
        <div class="post-meta">
          
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime="2022-12-01T00:00:00Z">
                December 1, 2022
              </time>
            </span>
            
          </div>
          
          
          
          
        </div>
      </header>

      <div class="post-content">
        
          <img style="width:100%;" src="/talks/2022-12-01-open-nlp-meetup.png" alt="Featured image"/>
        
        <p><a href="https://www.youtube.com/watch?v=7Idjl3OR0FY"  class="external-link" target="_blank" rel="noopener">Youtube link</a>,
<a href="https://gist.github.com/ZanSara/dc4b22e7ffe2a56647e0afba7537c46b"  class="external-link" target="_blank" rel="noopener">slides</a>, <a href="https://gist.github.com/ZanSara/9e8557830cc866fcf43a2c5623688c74"  class="external-link" target="_blank" rel="noopener">Colab</a> (live coding).
All the material can also be found <a href="https://drive.google.com/drive/folders/1_3b8PsvykHeM0jSHsMUWQ-4h_VADutcX?usp=drive_link"  class="external-link" target="_blank" rel="noopener">here</a>.</p>
<hr>
<div class='iframe-wrapper'>
  <iframe src="https://drive.google.com/file/d/19mxD-xUJ-14G-2XAqXEVpZfqR2MsSZTn/preview" width=100% height=100% allow="autoplay"></iframe>
</div>

<h2 id="a-practical-introduction-to-image-retrieval">
  A Practical Introduction to Image Retrieval
  
</h2>
<p><em>by Sara Zanzottera from deepset</em></p>
<p>Search should not be limited to text only. Recently, Transformers-based NLP models started crossing the boundaries of text data and exploring the possibilities of other modalities, like tabular data, images, audio files, and more. Text-to-text generation models like GPT now have their counterparts in text-to-image models, like Stable Diffusion. But what about search? In this talk we&rsquo;re going to experiment with CLIP, a text-to-image search model, to look for animals matching specific characteristics in a dataset of pictures. Does CLIP know which one is &ldquo;The fastest animal in the world&rdquo;?</p>
<hr>
<p>For the 7th <a href="https://www.meetup.com/open-nlp-meetup/"  class="external-link" target="_blank" rel="noopener">OpenNLP meetup</a> I presented the topic of Image Retrieval, a feature that I&rsquo;ve recently added to Haystack in the form of a <a href="https://docs.haystack.deepset.ai/docs/retriever#multimodal-retrieval"  class="external-link" target="_blank" rel="noopener">MultiModal Retriever</a> (see the <a href="https://haystack.deepset.ai/tutorials/19_text_to_image_search_pipeline_with_multimodal_retriever"  class="external-link" target="_blank" rel="noopener">Tutorial</a>).</p>
<p>The talk consists of 5 parts:</p>
<ul>
<li>An introduction of the topic of Image Retrieval</li>
<li>A mention of the current SOTA model (CLIP)</li>
<li>An overview of Haystack</li>
<li>A step-by-step description of how image retrieval applications can be implemented with Haystack</li>
<li>A live coding session where I start from a blank Colab notebook and build a fully working image retrieval system from the ground up, to the point where I can run queries live.</li>
</ul>
<p>Towards the end I mention briefly an even more advanced version of this image retrieval system, which I had no time to implement live. However, I later built a notebook implementing such system and you can find it here: <a href="https://gist.github.com/ZanSara/31ed3fc8252bb74b1952f2d0fe253ed0"  class="external-link" target="_blank" rel="noopener">Cheetah.ipynb</a></p>
<p>The slides were generated from the linked Jupyter notebook with <code>jupyter nbconvert Dec_1st_OpenNLP_Meetup.ipynb --to slides --post serve</code>.</p>

      </div>

    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
      2023 -
    
    2025 by
    &MediumSpace;
    <a href="/"><img src="https://www.zansara.dev/me/avatar.svg" style="width: 1em; height: 1em; margin-right: 5px;">Sara Zan</a>
  </section>
</footer>

  </main>

  

  <script src="/js/coder.js"></script>

  

  
</body>

</html>
