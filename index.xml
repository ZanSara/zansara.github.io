<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sara Zan</title>
    <link>https://www.zansara.dev/</link>
    <description>Recent content on Sara Zan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 06 Nov 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://www.zansara.dev/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[UPCOMING] Embrace:AI // 2025.06 - Reasoning LLMs &amp; Multimodal Architecture</title>
      <link>https://www.zansara.dev/talks/2025-11-06-embrace-ai-meetup-reasoning-models/</link>
      <pubDate>Thu, 06 Nov 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/talks/2025-11-06-embrace-ai-meetup-reasoning-models/</guid>
      <description></description>
    </item>
    
    <item>
      <title>[UPCOMING] ODSC West: LLMs that think - Demystifying Reasoning Models</title>
      <link>https://www.zansara.dev/talks/2025-10-29-odsc-west-reasoning-models/</link>
      <pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/talks/2025-10-29-odsc-west-reasoning-models/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Making sense of KV Cache optimizations, Ep. 4: System-level</title>
      <link>https://www.zansara.dev/posts/2025-10-29-kv-caching-optimizations-system-level/</link>
      <pubDate>Wed, 29 Oct 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2025-10-29-kv-caching-optimizations-system-level/</guid>
      <description>&lt;p&gt;In the previous posts we&amp;rsquo;ve seen &lt;a href=&#34;https://www.zansara.dev/posts/2025-10-23-kv-caching/&#34; &gt;what the KV cache is&lt;/a&gt; and what types of &lt;a href=&#34;https://www.zansara.dev/posts/2025-10-26-kv-caching-optimizations-intro/&#34; &gt;KV Cache management optimizations&lt;/a&gt; exist according to a &lt;a href=&#34;https://arxiv.org/abs/2412.19442&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recent survey&lt;/a&gt;. In this post we are going to focus on &lt;strong&gt;system-level&lt;/strong&gt; KV cache optimizations.&lt;/p&gt;
&lt;h2 id=&#34;what-is-a-system-level-optimization&#34;&gt;
  What is a system-level optimization?
  
&lt;/h2&gt;
&lt;p&gt;Real hardware is not only made of &amp;ldquo;memory&amp;rdquo; and &amp;ldquo;compute&amp;rdquo;, but is made of several different hardware and OS level elements, each with its specific tradeoff between speed, throughput, latency, and so on. Optimizing the KV cache to leverages this differences is the core idea of the optimizazions we&amp;rsquo;re going to see in this post.&lt;/p&gt;
&lt;p&gt;Here is an overview of the types of optimizations that exist today.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-10-29-kv-caching-optimizations-system-level/system-level.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.19442#figure.10&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As you can see in the diagram, they can be broadly grouped into three categories: memory management, scheduling strategies, and hardware-aware designs. These approaches are complementary and can be often used together, each addressing different aspects of performance, efficiency, and resource utilization tradeoffs.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see what&amp;rsquo;s the idea behind each of these categories. We won&amp;rsquo;t go into the details of the implementations of each: to learn more about a specific approach follow the links to the relevant sections of the survey, where you can find summaries and references.&lt;/p&gt;
&lt;h2 id=&#34;memory-management&#34;&gt;
  Memory Management
  
&lt;/h2&gt;
&lt;p&gt;Memory management techniques focus on using the different types of memory and storage available to the system in the most efficient way. There are two main approaches to this problem:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Architectural designs&lt;/strong&gt;, such as vLLM&amp;rsquo;s &lt;strong&gt;PagedAttention&lt;/strong&gt; and vTensor. These strategies adapt operating system memory management ideas to to create memory allocation systems that optimize the use of physical memory as much as possible. For example, PagedAttention adapts OS-inspired paging concepts by partitioning KV caches into fixed-size blocks with non-contiguous storage, and vLLM implements a virtual memory-like system that manages these blocks through a sophisticated mapping mechanism.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prefix-aware designs&lt;/strong&gt; like &lt;strong&gt;ChunkAttention&lt;/strong&gt; and MemServe. These center around the design of datastructures optimized for maximising cache de-duplication and sharing of common prefixes. For example, ChunkAttention restructures KV cache management by breaking down traditional monolithic KV cache tensors into smaller, manageable chunks organized within a prefix tree structure, enabling efficient runtime detection and sharing of common prefixes across multiple requests.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general, there&amp;rsquo;s a flurry of novel research focused on the way the KV cache is stored in memory. They bring classic OS memory management patterns and novel designs that leverage the properties of the KV cache at a memory layout level to increase the inference speed and memory consumption issues in a way that&amp;rsquo;s transparent from the model&amp;rsquo;s perspective. This makes these techniques widely applicable to many different LLMs and usually complementary to each other, which multiplies their effectiveness.&lt;/p&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href=&#34;https://arxiv.org/pdf/2412.19442#subsection.6.1&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;scheduling&#34;&gt;
  Scheduling
  
&lt;/h2&gt;
&lt;p&gt;Scheduling techniques focus on maximizing cache hits and minimize cache lifetime by grouping and distributing requests appropriately. In this category we can find a few distinct approaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Prefix-aware&lt;/strong&gt; scheduling strategies, such as BatchLLM and RadixAttention. For example, unlike traditional LRU caches, BatchLLM identifies global prefixes and coordinates the scheduling of requests sharing common KV cache content. This ensures optimal KV cache reuse while minimizing cache lifetime: requests with identical prefixes are deliberately scheduled together to maximize KV cache sharing efficiency.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Preemptive&lt;/strong&gt; and &lt;strong&gt;fairness-oriented&lt;/strong&gt; scheduling, such as FastServe and FastSwitch. For example, FastServe implements a proactive cache management strategy coordinates cache movement between GPU and host memory, overlapping data transmission with computation to minimize latency impact. The scheduler also prioritizes jobs based on input length.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Layer-specific&lt;/strong&gt; and hierarchical scheduling approaches, such as LayerKV and CachedAttention. For example, LayerKV focuses on reducing time-to-first-token (TTFT) through a fine-grained, layer-specific KV cache block allocation and management strategy. It also includes an SLO-aware scheduler that optimizes cache allocation decisions based on service level objectives.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href=&#34;https://arxiv.org/pdf/2412.19442#subsection.6.2&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;hardware-aware-design&#34;&gt;
  Hardware-aware Design
  
&lt;/h2&gt;
&lt;p&gt;These techiques focus on leveraging specific characteristics of the hardware in order to accelerate inference and increase efficiency. In this class of optimizazions we can find a few shared ideas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Single/Multi-GPU designs&lt;/strong&gt; focus on optimizing memory access patterns, GPU kernel designs for efficient attention computation, and parallel processing with load balancing. For example, shared prefix optimization approaches like HydraGen and DeFT focus on efficient GPU memory utilization through batched prefix computations and tree-structured attention patterns. Another example is distributed processing frameworks such as vLLM, that optimize multi-GPU scenarios through sophisticated memory management and synchronization mechanisms. Other techniques are phase-aware, like DistServe, which means that they separate prefill and decoding phases across GPU resources to optimize their distinct memory access patterns.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IO-based designs&lt;/strong&gt; optimize data movement across memory hierarchies through asynchronous I/O and intelligent prefetching mechanisms.
At the GPU level, approaches like FlashAttention optimize data movement between HBM and SRAM through tiling strategies and split attention computations. At the CPU-GPU boundary, systems like PartKVRec address tackles PCIe bandwidth bottlenecks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Heterogeneous designs&lt;/strong&gt; orchestrate computation and memory allocation across CPU-GPU tiers. Systems like NEO or FastDecode reditribute the workload by offloading to the CPU part of the attention computations, while others like FlexInfer introduce virtual memory abstractions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSD-based designs&lt;/strong&gt; have evolved from basic offloading approaches to more sophisticated designs. For example, FlexGen extends the memory hierarchy across GPU, CPU memory, and disk storage, optimizing high-throughput LLM inference on resource-constrained hardware. InstInfer instead leverages computational storage drives (CSDs) to perform in-storage attention computation, effectively bypassing PCIe bandwidth limitations. These techniques demonstrate how storage devices can be integrated into LLM inference systems either as memory hierarchy extensions or as computational resources.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href=&#34;https://arxiv.org/pdf/2412.19442#subsection.6.3&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conclusions&#34;&gt;
  Conclusions
  
&lt;/h2&gt;
&lt;p&gt;System-level KV cache optimizations show that working across the stack can bring impressive speedups and manage physical resources more efficiently than it could ever be done at the LLM&amp;rsquo;s abstraction level. Operating systems and hardware layouts offer plenty of space for optimizations of workloads that have somewhat predictable patterns such as attention computations and KV caching show, and these are just a few examples of what could be done in the near future.&lt;/p&gt;
&lt;p&gt;This is the end of our review. The original paper includes an additional section on long-context benchmarks which we&amp;rsquo;re not going to cover, so head to &lt;a href=&#34;https://arxiv.org/pdf/2412.19442#section.7&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the survey&lt;/a&gt; if you&amp;rsquo;re interested in the topic.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making sense of KV Cache optimizations, Ep. 3: Model-level</title>
      <link>https://www.zansara.dev/posts/2025-10-28-kv-caching-optimizations-model-level/</link>
      <pubDate>Tue, 28 Oct 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2025-10-28-kv-caching-optimizations-model-level/</guid>
      <description>&lt;p&gt;In the previous posts we&amp;rsquo;ve seen &lt;a href=&#34;https://www.zansara.dev/posts/2025-10-23-kv-caching/&#34; &gt;what the KV cache is&lt;/a&gt; and what types of &lt;a href=&#34;https://www.zansara.dev/posts/2025-10-26-kv-caching-optimizations-intro/&#34; &gt;KV Cache management optimizations&lt;/a&gt; exist according to a &lt;a href=&#34;https://arxiv.org/abs/2412.19442&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recent survey&lt;/a&gt;. In this post we are going to focus on &lt;strong&gt;model-level&lt;/strong&gt; KV cache optimizations.&lt;/p&gt;
&lt;h2 id=&#34;what-is-a-model-level-optimization&#34;&gt;
  What is a model-level optimization?
  
&lt;/h2&gt;
&lt;p&gt;We call a model-level optimization any modification of the architecture of the LLM that enables a more efficient reuse of the KV cache. In most cases, to apply these method to an LLM you need to either retrain or at least finetune the model, so it&amp;rsquo;s not easy to apply and is usually baked in advance in of-the-shelf models.&lt;/p&gt;
&lt;p&gt;Here is an overview of the types of optimizations that exist today.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-10-28-kv-caching-optimizations-model-level/model-level.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.19442#figure.7&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see what&amp;rsquo;s the idea behind each of these categories. We won&amp;rsquo;t go into the details of the implementations of each: to learn more about a specific approach follow the links to the relevant sections of the survey, where you can find summaries and references.&lt;/p&gt;
&lt;h2 id=&#34;attention-grouping-and-sharing&#34;&gt;
  Attention Grouping and Sharing
  
&lt;/h2&gt;
&lt;p&gt;One common technique to reduce the size of the KV cache is to group and/or share attention on different levels. There&amp;rsquo;s techniques being developed for different grades of attention grouping:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Intra-layer grouping&lt;/strong&gt;: focuses on grouping query, key, and value heads within individual layers&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cross-layer sharing&lt;/strong&gt;: shares key, value, or attention components across layers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At the &lt;strong&gt;intra-layer&lt;/strong&gt; level, the standard architecture of Transformers calls for full &lt;strong&gt;multi-headed attention&lt;/strong&gt; (MHA). As an alternative, it was proposed to have all attention heads share a single key and value, reducing dramatically the amount of compute and space needed. This technique, called &lt;strong&gt;multi-query attention&lt;/strong&gt; (MQA) is a radical strategy that would cause not just quality degradation, but also training instability. As a compromise, &lt;strong&gt;grouped-query attention&lt;/strong&gt; (GQA) was proposed by dividing the query heads into multiple groups, while each group shares its own keys and values. In addition, an uptraining process has been proposed to efficiently convert existing MHA models to GQA configurations by mean-pooling the key and value heads associated with each group. Empirical evaluations demonstrated that GQA models achieve performance close to the original MHA models.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-10-28-kv-caching-optimizations-model-level/attention-grouping.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A simplified illustration of different QKV grouping techniques: multi-headed attention (MHA), multi-query attention (MQA) and grouped-query attention (GQA).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Across layers&lt;/strong&gt;, cross-layer attention (CLA) was proposed to extends the idea of GQA. Its core idea is to share the key and value heads between adjacent layers. This achieves an additional 2× KV cache size reduction compared to MQA. Several other approaches exist to address cross-layer attention sharing, so check out &lt;a href=&#34;https://arxiv.org/pdf/2412.19442#subsection.5.1&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the survey&lt;/a&gt; if you want to learn more.&lt;/p&gt;
&lt;p&gt;In general, the main issue in this line of research regards the model modifications that needs to be applied. Current approaches often fail to generalize well to architecture they were not initially designed on, while more static and general grouping/sharing strategies fail to capture important variations in the various heads and layers, leading to a loss of output quality. In addition, the need to retrain the LLM after the changes limits strongly the portability of these methods.&lt;/p&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href=&#34;https://arxiv.org/pdf/2412.19442#subsection.5.1&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;architecture-alteration&#34;&gt;
  Architecture Alteration
  
&lt;/h2&gt;
&lt;p&gt;Another approach is to make more high-level architectural changes to reduce the required cache size. There seems to be two main directions in this area:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Enhanced Attention&lt;/strong&gt;: methods that refine the attention mechanism for KV cache efficiency. An example is DeepSeek-V2, which introduced Multi-Head Latent Attention (MLA). This technique adopts a low-rank KV joint compression mechanism and replaces the full KV cache with compressed latent vectors. The model adopts trainable projection and expansion matrices to do the compression. This compression mechanism is what enables the model to handle sequences of up to 128K tokens. You can learn more about MLA in &lt;a href=&#34;https://magazine.sebastianraschka.com/i/168650848/multi-head-latent-attention-mla&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this article&lt;/a&gt; by Sebastian Raschka.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Augmented Architecture&lt;/strong&gt;: methods that introduce structural changes for better KV management, for example novel decoder structures (such as YOCO, that included a self-decoder and a cross-decoder step).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many of these works build upon the broader landscape of efficient attention mechanisms (e.g., Linear Transformer, Performer, LinFormer, etc.) which already have &lt;a href=&#34;https://arxiv.org/abs/2404.14294&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;their own survey&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Although these approaches demonstrate significant progress in enabling longer context windows and faster inference, there are still big challenged ans unknowns. Some techniques in this category, for example, perform very well for some tasks but fail to generalize (for example they work well with RAG but not with non-RAG scenarios).&lt;/p&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href=&#34;https://arxiv.org/pdf/2412.19442#subsection.5.2&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;non-transformer-architecture&#34;&gt;
  Non-Transformer Architecture
  
&lt;/h2&gt;
&lt;p&gt;In this category we group all radical approaches that ditch the Transformers architecture partially or entirely and embrace alternative models, for example RNNs, which don&amp;rsquo;t have quadratic computation bottlenecks at all and sidestep the problem entirely.&lt;/p&gt;
&lt;p&gt;In the case of completely independent architectures, notable examples are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.00752&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mamba&lt;/a&gt;, based on state space sequence models (SSMs). Mamba improves SSMs by making parameters input-dependent, allowing information to be selectively propagated or forgotten along the sequence based on the current token. Mamba omits attention entirely.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/2305.13048&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RWKV&lt;/a&gt; (Receptance Weighted Key Value) integrates a linear attention mechanism, enabling parallelizable training like transformers while retaining the efficient inference characteristics of RNNs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Efficient non-Transformers also have their own surveys, so check out the paper to learn more.&lt;/p&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href=&#34;https://arxiv.org/pdf/2412.19442#subsection.5.3&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;
  Conclusion
  
&lt;/h2&gt;
&lt;p&gt;Model-level optimizations go from very light touches to the original Transformer model to architecture that have nothing to do with it, therefore not having any KV cache to deal with in the first place. In nearly all cases the principal barrier to adoption is the same: applying these techniques requires a &lt;strong&gt;full retraining of the model&lt;/strong&gt;, which can be impractical at best and prohibitively expensive at worst, even for users that have the right data and computing power. Model-level optimizations are mostly useful for LLM developers to get an intuition of the memory efficiency that can be expected from a model that includes one or more of these features out of the box.&lt;/p&gt;
&lt;p&gt;In the next post we&amp;rsquo;re going to address &lt;a href=&#34;https://www.zansara.dev/posts/2025-10-29-kv-caching-optimizations-system-level&#34; &gt;system-level&lt;/a&gt; optimizations. Stay tuned!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making sense of KV Cache optimizations, Ep. 2: Token-level</title>
      <link>https://www.zansara.dev/posts/2025-10-27-kv-caching-optimizations-token-level/</link>
      <pubDate>Mon, 27 Oct 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2025-10-27-kv-caching-optimizations-token-level/</guid>
      <description>&lt;p&gt;In the previous post we&amp;rsquo;ve seen &lt;a href=&#34;https://www.zansara.dev/posts/2025-10-23-kv-caching/&#34; &gt;what the KV cache is&lt;/a&gt; and what types of &lt;a href=&#34;https://www.zansara.dev/posts/2025-10-26-kv-caching-optimizations-intro/&#34; &gt;KV cache management optimizations&lt;/a&gt; exist according to a &lt;a href=&#34;https://arxiv.org/abs/2412.19442&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recent survey&lt;/a&gt;. In this post we are going to focus on &lt;strong&gt;token-level&lt;/strong&gt; KV cache optimizations.&lt;/p&gt;
&lt;h2 id=&#34;what-is-a-token-level-optimization&#34;&gt;
  What is a token-level optimization?
  
&lt;/h2&gt;
&lt;p&gt;The survey defined token-level optimizations every technique that focuses exclusively on improving the KV cache management based on the &lt;strong&gt;characteristics and patterns of the KV pairs&lt;/strong&gt;, without considering enhancements from model architecture improvements or system parallelization techniques.&lt;/p&gt;
&lt;p&gt;Here is an overview of the types of optimizations that exist today.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-10-27-kv-caching-optimizations-token-level/token-level.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.19442#figure.3&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see what&amp;rsquo;s the idea behind each of these categories. We won&amp;rsquo;t go into the details of the implementations of each: to learn more about a specific approach follow the links to the relevant sections of the survey, where you can find summaries and references.&lt;/p&gt;
&lt;h3 id=&#34;kv-cache-selection&#34;&gt;
  KV Cache Selection
  
&lt;/h3&gt;
&lt;p&gt;One key characteristic of the attention matrix is &lt;strong&gt;sparsity&lt;/strong&gt;: most of its values are very close to zero, and just a few cells have meaningful values. Instead of retrieving a full matrix of attention values every time (and retrieve a ton of close-to-zero, nearly useless values), KV Cache selection techniques identify the most relevant token pair and cache those only, reducing memory utilization and inference latency.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-10-27-kv-caching-optimizations-token-level/sparse-attention.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A simplified view of a cache selection strategy. In this case, the KV cache tends to have its highest values clustered near the diagonal (because most tokens refer to other tokens that are relatively close), so most of the lower-left side of the matrix can be safely assumed to be zero. That reduces drastically the number of values to store.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The researches identified two main cache selection strategies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Static KV cache selection&lt;/strong&gt;. In this family of optimizations, the KV cache compression only happens during the first decoding pass (when most of the prompt is loaded in the LLM state, also called &lt;strong&gt;prefill phase&lt;/strong&gt;) and remain fixed during all subsequent decoding steps, with no more compressions as the inference proceeds.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic KV cache selection&lt;/strong&gt;, which continuously updates and compresses the KV cache during all inference passes, enabling adaptive cache management. In dynamic KV cache selection, KV cache tokens that are not selected may be either permanently evicted or offloaded to hierarchical caching devices such as CPU memory. While more efficient in terms of memory usage, real-time KV cache selection during decoding may incur substantial computational overhead, which is usually the focus of any new technique developed in this space.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The tradeoff between static and dynamic KV cache selection is again one of &lt;strong&gt;latency versus efficiency&lt;/strong&gt;, or time vs space usage. Static KV cache selection is faster and slightly less efficient; dynamic KV cache compression is more efficient in terms of memory usage but has a sensible impact on inference speed and may cause issues due to excessive compression, throwing away or putting in cold caches token pairs that are actually relevant. A clear consensus about where the sweet spot lays hasn&amp;rsquo;t been found yet, and it&amp;rsquo;s mostly still open to investigation.&lt;/p&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href=&#34;https://arxiv.org/pdf/2412.19442#subsection.4.1&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;kv-cache-budget-allocation&#34;&gt;
  KV Cache Budget Allocation
  
&lt;/h3&gt;
&lt;p&gt;LLMs are hierarchical, with several layers within layers of computations. Each of these layers is identical in structure, but during training the weights that they learn make some of these layers more important than others and more impactful on the output&amp;rsquo;s quality.&lt;/p&gt;
&lt;p&gt;This means that not all of these steps should be compressed equally. If we could identify which layers are more impactful we could reduce the compression of the KV cache for these layers and increase it for the others. In this way the effects of compression on the output quality would be minimized.&lt;/p&gt;
&lt;p&gt;Budget allocation strategies tend either of these granularity levels:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Layer-wise budget allocation&lt;/strong&gt;, which assigns different compression ratios across the model&amp;rsquo;s decoding layers&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Head-wise budget allocation&lt;/strong&gt;, which enables precise memory distribution across individual attention heads within each layer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Despite recent advances and growing attention in this subset of techniques, there are still big question marks about how to distribute this computing budget in an effective way. For instance, a notable discrepancy exists between pyramid-shaped allocation strategies that advocate larger budgets for lower layers, and retrieval head-based studies, which demonstrate that lower layers rarely exhibit retrieval head characteristics and thus require minimal cache resources. On top of this, there is a lack of comprehensive experimental comparisons, such as the compatibility and performance benefits of head-wise budget allocation strategies with state-of-the-art frameworks like vLLM.&lt;/p&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href=&#34;https://arxiv.org/pdf/2412.19442#subsection.4.2&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;kv-cache-merging&#34;&gt;
  KV Cache Merging
  
&lt;/h3&gt;
&lt;p&gt;The idea behind KV cache merging is to compress or consolidate separate KV caches into a single one without significantly degrading model accuracy. This stems from the observation that the various layers and attention heads often shows redundant patterns that could be merged into one single representation to improve compression.&lt;/p&gt;
&lt;p&gt;Just like with the budget allocation techniques, KV cache merging strategies can be categorized into two primary approaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Intra-layer merging&lt;/strong&gt;, which focuses on consolidating KV caches within individual layers to reduce memory usage per layer&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cross-layer merging&lt;/strong&gt;, which targets redundancy across layers to eliminate unnecessary duplication.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general, KV cache merging can be very effective at optimizing memory utilization in LLMs by consolidating KV caches while maintaining high model accuracy, and it&amp;rsquo;s an active research direction that could provide more results in the near future by addressing narrower niches such as fine-tuning and adaptive merging strategies.&lt;/p&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href=&#34;https://arxiv.org/pdf/2412.19442#subsection.4.3&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;kv-cache-quantization&#34;&gt;
  KV Cache Quantization
  
&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-10-27-kv-caching-optimizations-token-level/quantization.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A simplified example of cache quantization. Reducing the precision of the values from float to int8 can drastically reduce the memory needs of the cache and accelerate inference.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Quantization techniques aim to convert full-precision values into integers, reducing computational and storage requirements. Quantization has also been used on other aspects of the LLM inference and training processes, such as with model parameters and data features quantization. KV cache quantization works in a similar way: by reducing the precision of numerical representations (e.g., from FP32 to INT8 or INT4) we can drastically compress the size of the KV cache and achieve up to 4x or more memory savings with respect to the full-precision floating point representation.&lt;/p&gt;
&lt;p&gt;One of the main challenges of KV cache quantization is the presence of outliers, especially when quantizing to a very low-bit representation. These extreme values, when reduced to a smaller magnitude, can lead to a substantial performance degradation.&lt;/p&gt;
&lt;p&gt;Depending on how they address this issue, quantization techniques can be grouped into three types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fixed-precision quantization&lt;/strong&gt;, where all Keys and Values are quantized to the same bit-width.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mixed-precision quantization&lt;/strong&gt;, which assigns higher precision to critical parts of the cache while using lower precision for less important components.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Outlier redistribution&lt;/strong&gt;, which redistributes or smooths the outliers in Keys and Values to improve quantization quality. Some approaches to outliers redistribution include redistributing the outliers into newly appended virtual tokens or applying equivalent transformation functions to smooth the keys and values for improved quantization accuracy.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href=&#34;https://arxiv.org/pdf/2412.19442#subsection.4.4&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;kv-cache-low-rank-decomposition&#34;&gt;
  KV Cache Low-rank Decomposition
  
&lt;/h3&gt;
&lt;p&gt;Existing studies have demonstrated that the majority of information within KV caches can be captured by a small subset of their singular elements or sub-matrices with a smaller dimension, called &lt;strong&gt;low-rank components&lt;/strong&gt;. Decomposing the matrix into low-rank components can effectively reduce memory requirements while preserving output quality by &amp;ldquo;picking out&amp;rdquo; the components of the KV matrix that matter the most and throwing out the rest.&lt;/p&gt;
&lt;p&gt;Currently there are three main ways to perform low-rank decomposition of the cached KV matrix:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Singular Value Decomposition (SVD)&lt;/strong&gt;: retains the most critical singular values.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tensor Decomposition&lt;/strong&gt;: factorizes KV matrices into smaller matrices/tensors.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learned Low-rank Approximation&lt;/strong&gt;: adaptive mechanisms to optimize compression based on learned low-rank representations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Current methods primarily rely on fixed low-rank approximations applied uniformly across all layers or tokens, but future advancements could focus on dynamic rank adjustment, where the rank is tailored based on token importance, sequence length, or layer-specific properties.&lt;/p&gt;
&lt;p&gt;For a more detailed description of each technique, check out &lt;a href=&#34;https://arxiv.org/pdf/2412.19442#subsection.4.5&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the survey&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;
  Conclusion
  
&lt;/h2&gt;
&lt;p&gt;This was just a brief overview of the various techniques that have been tested to compress the KV cache, but the exploration of the space between highest accuracy, fastest inference and strongest compression is far from complete. Most of these techniques optimize for just one or two of these properties, with no clear winner that beats them all. Expect a lot more experimentation in this field in the months and years to come.&lt;/p&gt;
&lt;p&gt;On the other hand, these are only compression techniques that apply at the token-level, without any support from the model architecture. For model-level approaches to the problem, check out the next post, where we continue exploring the survey to see how the basic architecture of the Transformer&amp;rsquo;s decoding layer can be optimized to reduce the amount of values to cache in the first place.&lt;/p&gt;
&lt;p&gt;In the next post we&amp;rsquo;re going to address &lt;a href=&#34;https://www.zansara.dev/posts/2025-10-28-kv-caching-optimizations-model-level&#34; &gt;model-level&lt;/a&gt; optimizations. Stay tuned!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making sense of KV Cache optimizations, Ep. 1: An overview</title>
      <link>https://www.zansara.dev/posts/2025-10-26-kv-caching-optimizations-intro/</link>
      <pubDate>Sun, 26 Oct 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2025-10-26-kv-caching-optimizations-intro/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://www.zansara.dev/posts/2025-10-23-kv-caching/&#34; &gt;KV cache&lt;/a&gt; is an essential mechanism to avoid the quadratic time complexity of LLM inference and make modern LLMs usable despite huge parameters count and context lengths. However, simply caching everything indiscriminately is not a successful strategy. By swapping time for space complexity, now our problem is &lt;strong&gt;GPU memory&lt;/strong&gt;. Adding more memory can only bring you so far: at some point, you&amp;rsquo;re going to need much more efficient ways to decide what to cache, when and how. But classic cache management techniques were not designed for LLMs, and they often fall short.&lt;/p&gt;
&lt;p&gt;With time, a veritable zoo of optimization strategies arose to get around this problem, and making sense of which optimizations can be applied to which model can be a challenge in itself. Fortunately a  &lt;a href=&#34;https://arxiv.org/abs/2412.19442&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;very comprehensive survey&lt;/a&gt; on KV caching recently collected all techniques that make up the state of the art in this field, giving practitioners a handy starting point to understand this field. The amount of techniques reviewed is staggering, so we&amp;rsquo;re going to need more than one post to go through the most interesting approaches and compare them.&lt;/p&gt;
&lt;p&gt;For now, let&amp;rsquo;s see how we can start to make sense of things.&lt;/p&gt;
&lt;h2 id=&#34;the-challenges&#34;&gt;
  The challenges
  
&lt;/h2&gt;
&lt;p&gt;Most of the techniques we&amp;rsquo;re going to see address one or more of these issues.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cache Eviction&lt;/strong&gt;: Determining which items to evict when the cache reaches its capacity. Popular policies like Least Recently Used (LRU) or Least Frequently Used (LFU) do not always align with LLM usage patterns, leading to suboptimal performance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory Management&lt;/strong&gt;: The memory required for the KV cache grows linearly with both the input length and the number of layers, which can quickly exceed the hardware memory limits. It&amp;rsquo;s possible to overcome such limits by distributing the storage of this cache across different types of storage hardware (e.g., GPU, CPU or external memory), but this brings its own set of challenges.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Latency&lt;/strong&gt;: Accessing and updating the cache at each decoding step can introduce latency.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compression&lt;/strong&gt;: Compressing the KV cache can reduce memory usage but may degrade model performance if key information is lost.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Workloads&lt;/strong&gt;: Handling dynamic and unpredictable workloads, where access patterns and data requirements frequently change, requires adaptive caching strategies that can respond in real time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distributed Coordination&lt;/strong&gt;: In distributed KV caches, maintaining coordination across multiple nodes to ensure consistency, fault tolerance, and efficient resource usage adds significant complexity.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;a-taxonomy&#34;&gt;
  A taxonomy
  
&lt;/h2&gt;
&lt;p&gt;In order to make sense of the vast amount of known techniques, the authors categorized them into a comprehensive taxonomy.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-10-26-kv-caching-optimizations-intro/taxonomy.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.19442#figure.2&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It starts with three major categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Token-Level Optimization&lt;/strong&gt;: improving KV cache management efficiency by focusing on the fine-grained selection, organization, and compression at the token level. These techniques can be applied to any model, as they require no architectural changes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model-level Optimization&lt;/strong&gt;: designing an efficient model structure to optimize KV cache management. These optimizations are strictly model-dependent, because they&amp;rsquo;re backed into the model&amp;rsquo;s architecture.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;System-level Optimization&lt;/strong&gt;: optimizing the KV Cache management through techniques closer to the OS and/or the hardware. These techniques may require specialized hardware to implement, so they&amp;rsquo;re not at everyone&amp;rsquo;s reach.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;token-level-optimizations&#34;&gt;
  Token-Level Optimizations
  
&lt;/h2&gt;
&lt;p&gt;Token-level optimizations are the most readily accessible to most developers, as they require no dedicated support from the LLM and no specialized hardware. Therefore, these are usually the most interesting. In this category we find:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;KV cache selection&lt;/strong&gt;: focuses on prioritizing and storing only the most relevant tokens.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;KV cache budget allocation&lt;/strong&gt;: dynamically distributes memory resources across tokens to ensure efficient cache utilization under limited memory.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;KV cache merging&lt;/strong&gt;: reduces redundancy by combining similar or overlapping KV pairs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;KV cache quantization&lt;/strong&gt;: minimizes the memory footprint by reducing the precision of cached KV pairs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;KV cache low-rank decomposition&lt;/strong&gt;: uses low-rank decomposition techniques to reduce cache size.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;model-level-optimizations&#34;&gt;
  Model-Level Optimizations
  
&lt;/h2&gt;
&lt;p&gt;Model-level optimizations, as the name says, are baked into the model&amp;rsquo;s architecture and therefore are either not applicable or always present in the models you&amp;rsquo;re running. These optimizations are usually interesting for people that design their own model architecture and train them, rather than developers that work with off-the-shelf models. In this category we find:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Attention grouping and sharing methods&lt;/strong&gt;: examine the redundant functionality of keys and values and group and share KV cache within or across transformer layers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Architecture alterations&lt;/strong&gt;: emerge to design new attention mechanisms or construct extrinsic modules for KV optimization.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Non-transformer architectures&lt;/strong&gt;: architectures that adopt other memory-efficient designs like recurrent neural networks to optimize the KV cache in traditional transformers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;system-level-optimizations&#34;&gt;
  System-level Optimizations
  
&lt;/h2&gt;
&lt;p&gt;These optimizations work across the stack to provide the best possible support to the LLM&amp;rsquo;s inference, and they&amp;rsquo;re sometimes baked into the inference engine, such as vLLM&amp;rsquo;s PagedAttention. They occasionally require dedicated hardware and OS optimizations, so they&amp;rsquo;re not always readily available for everyday experimentation. They include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Memory management&lt;/strong&gt;: focuses on architectural innovations like virtual memory adaptation, intelligent prefix sharing, and layer-aware resource allocation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scheduling&lt;/strong&gt;: addresses diverse optimization goals through prefix-aware methods for maximizing cache reuse, preemptive techniques for fair context switching, and layer-specific mechanisms for fine-grained cache control.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hardware acceleration&lt;/strong&gt;: including single/multi-GPU, I/O-based solutions, heterogeneous computing and SSD-based solutions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;
  Conclusion
  
&lt;/h2&gt;
&lt;p&gt;KV cache optimization is still an open research area, with new techniques and improvements being published regularly. A good overview of what types of optimizations exist can help you make sense of the zoo of acronyms and claims being made about them, and give you the foundations you need to understand if a particular technique is relevant for your situation. Stay tuned for the &lt;a href=&#34;https://www.zansara.dev/posts/2025-10-27-kv-caching-optimizations-token-level&#34; &gt;next posts&lt;/a&gt;, where we will dive deeper into each of these categories.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How does prompt caching work?</title>
      <link>https://www.zansara.dev/posts/2025-10-23-kv-caching/</link>
      <pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2025-10-23-kv-caching/</guid>
      <description>&lt;hr&gt;
&lt;p&gt;&lt;em&gt;This is episode 3 of a series of shorter blog posts answering questions I received during the course of my work and reflect common misconceptions and doubts about various generative AI technologies. You can find the whole series here: &lt;a href=&#34;https://www.zansara.dev/series/practical-questions&#34; &gt;Practical Questions&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;In the previous post we saw what is prompt caching, what parts of the prompts is useful to cache, and explained at a high level why it&amp;rsquo;s so effective. In this post I want to go one step further and explain &lt;em&gt;how&lt;/em&gt; in practice inference engines cache prompt prefixes. How can you take a complex system like an LLM, cache some of its computations mid-prompt, and reload them?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s find out.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Disclaimer: to avoid overly complex and specific diagrams, the size of the vector and matrices shown is not accurate neither in size nor in shape. Check the links at the bottom of the post for more detailed resources with more accurate diagrams.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;llms-are-autoregressive&#34;&gt;
  LLMs are autoregressive
  
&lt;/h2&gt;
&lt;p&gt;Large Language Models are built on the Transformer architecture: a neural network design that excels at processing sequence data. Explaining the whole structure of a Transformer goes beyond the scope of this small post: if you&amp;rsquo;re interested in the details, head to this &lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;amazing writeup&lt;/a&gt; by Jay Alammar about Transformers, or &lt;a href=&#34;https://jalammar.github.io/illustrated-gpt2/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this one about GPT-2&lt;/a&gt; if you&amp;rsquo;re familiar with Transformers but you want to learn more about the decoder-only architecture (which includes all current LLMs).&lt;/p&gt;
&lt;p&gt;The point that interests us is that according to the original implementation, during inference the LLM generate text one token at a time in an &lt;em&gt;autoregressive&lt;/em&gt; fashion, meaning each new token is predicted based on &lt;strong&gt;all&lt;/strong&gt; previously generated tokens. After producing (or &amp;ldquo;decoding&amp;rdquo;) a token, that token is appended to the input sequence and the model computes everything all over again to generate the next one. This loop continues until a stopping condition is reached (such as an end-of-sequence token or a length limit).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-10-23-kv-caching/auto-regression.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In an autoregressive system, the output is generated token by token by appending the previous pass&amp;rsquo; output to its input and recomputing everything again. Starting from the token &amp;ldquo;This&amp;rdquo;, the LLM produces &amp;ldquo;is&amp;rdquo; as output. Then the output is concatenated to the input in the string &amp;ldquo;This is&amp;rdquo;, which is fed again to the LLM to produce &amp;ldquo;a&amp;rdquo;, and so on until an [END] token is generated. That halts the loop.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This iterative process is very computationally expensive (quadratic time complexity in the number of tokens, so O(n^2) where n is the number of tokens), and the impact is felt especially for long sequences, because each step must account for an ever-growing history of generated tokens.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-10-23-kv-caching/auto-regression-2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Simplified view of the increasing computation load. At each pass, the increasing length of the input sentence translated into larger matrices to be handled during inference, where each row corresponds to one input token. This means more computations and, in turn, slower inference.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;However, there seems to be an evident chance for optimization here. If we could store the internal state of the LLM after each token&amp;rsquo;s generation and reuse it at the next step, we could save a lot of repeated computations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-10-23-kv-caching/auto-regression-cached.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If we could somehow reuse part of the computations we did during earlier passes and only process new information as it arrives, not only the computation speed will increase dramatically, but it will stay constant during the process instead of slowing down as more tokens are generated.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This is not only true during a single request (because we won&amp;rsquo;t be recomputing the whole state from the start of the message for every new token we&amp;rsquo;re generating), but also across requests in the same chat (by storing the state at the end of the last assistant token) and across different chats as well (by storing the state of shared prefixes such as system prompts).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-10-23-kv-caching/prefix-caching.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example of prefix caching in different scenarios (gray text is caches, black is processed). By caching the system prompt, its cache can be reused with every new chat. By also caching by longest prefix, the prompts may occasionally match across chats, although it depends heavily on your applications. In any case, caching the chat as it progresses keeps the number of new tokens to process during the chat to one, making inference much faster.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;But how can it be done? What exactly do we need to cache? To understand this we need to go one step deeper.&lt;/p&gt;
&lt;h2 id=&#34;the-inference-process&#34;&gt;
  The inference process
  
&lt;/h2&gt;
&lt;p&gt;At a high level, the inference process of a modern decoder-only Transformer such as a GPT works as follows.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tokenization&lt;/strong&gt;: The chat history is broken down into tokens by a tokenizer. This is a fast, deterministic process that transforms a single string into a list of sub-word fragments (the tokens) plus a bunch of signalling tokens (to delimit messages, to signal the end of the message, to distinguish different types of input or output tokens such as thinking tokens, function calls, system prompts, etc)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embedding&lt;/strong&gt;: the tokenized text passes through an embedding step, where each token is translated into an embedding (a 1-dimensional vector) using a lookup table. At this point, our input text has become a matrix of values with as many rows as tokens, and a fixed number of columns that depends on the LLM.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decoding&lt;/strong&gt;: this matrix is passed through a series of 12 identical decoding steps. Each of these blocks outputs a matrix of the same shape and size of the original one, but with updated contents. These steps are &amp;ldquo;reading&amp;rdquo; the prompt and accumulating information to select the next best token to generate.that is passed as input to the next step&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: After the last decoding step, a final linear output layer projects the matrix into an output vector. Its values are multiplied by the lookup table we used during the embedding step: this way we obtain a list of values that represents the probability of each token to be the &amp;ldquo;correct&amp;rdquo; next token.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sampling&lt;/strong&gt;: From this list, one of the top-k best tokens is selected as the next token, gets added to the chat history, and the loop restarts.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;End token&lt;/strong&gt;: the decoding stops when the LLM picks an END token or some other condition is met (for example, max output length).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-10-23-kv-caching/llm-inference.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Simplified representation of the inference steps needed for an LLM to generate each output token. The most complex by far is the decoding step, which we are going to analyze in more detail.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As you can see from this breakdown, the LLM computes its internal representation of the chat history through its decoding steps, and recomputes such representation for all tokens every time we want to generate a new one. So let&amp;rsquo;s zoom in even more and check what&amp;rsquo;s going on inside these decoding steps.&lt;/p&gt;
&lt;h2 id=&#34;the-decoding-step&#34;&gt;
  The decoding step
  
&lt;/h2&gt;
&lt;p&gt;LLMs may have a variable number of decoding steps (although it&amp;rsquo;s often 12), but they are all identical, except for the weights they contain. This means that we can look into one and then keep in mind that the same identical process is repeated several times.&lt;/p&gt;
&lt;p&gt;Each decoding step contains two parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a multi-headed, masked self-attention layer&lt;/li&gt;
&lt;li&gt;a feed-forward layer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first layer, the multi headed masked self attention, sound quite complicated. To make things easier, let&amp;rsquo;s break it down into smaller concepts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Attention&lt;/strong&gt; is the foundation of the Transformers&amp;rsquo; incredible text understanding skills and can be roughly summarized as a technique that shows the model which tokens are the most relevant to the token we&amp;rsquo;re processing right now.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Self attention&lt;/strong&gt; means that the tokens we&amp;rsquo;re looking at belong to the same sentence we&amp;rsquo;re processing (which is not the case, for example, during translation tasks where we have a source sentence and a translation).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Masked self attention&lt;/strong&gt; means that we&amp;rsquo;re only looking at tokens that precede the one we&amp;rsquo;re processing (which is not the case, for example, in encoder models such as BERT that encode the whole sentence at once).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Multi-headed&lt;/strong&gt; attention means that the same operation is performed several times with slightly different parameters. Each set of parameters is called an &lt;strong&gt;attention head&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To understand what attention does, let&amp;rsquo;s take the sentence &amp;ldquo;I like apples because they&amp;rsquo;re sweet&amp;rdquo;. When processing the token &amp;ldquo;they&amp;rdquo;, the masked self-attention layer will give a high score to &amp;ldquo;apples&amp;rdquo;, because that&amp;rsquo;s what &amp;ldquo;they&amp;rdquo; refers to. Keep in mind that &amp;ldquo;sweet&amp;rdquo; will not be considered while processing &amp;ldquo;they&amp;rdquo;, because masked self-attention only includes tokens that precede the token in question.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-10-23-kv-caching/masked-self-attention.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A simplified visualization of a masked self-attention head. For each token, the attention calculations will assign a score to each preceding token. The score will be higher for all preceding tokens that have something to do with the current one, highlighting semantic relationships.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-qkv-matrices&#34;&gt;
  The Q/K/V Matrices
  
&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s now look at how is this score calculated. Self-attention is implemented as a series of matrix multiplications that involves three matrices.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Q (Query matrix)&lt;/strong&gt;: The query is a representation of the tokens we are &amp;ldquo;paying attention to&amp;rdquo; (for example, &amp;ldquo;they&amp;rdquo;. In practice all tokens will be computed at the same time, so we will be dealing with a Q matrix).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;K (Key matrix)&lt;/strong&gt;: Key vectors are like labels for all the other preceding tokens in the input. They’re what we match against in our search for relevant tokens (for example &amp;ldquo;I&amp;rdquo;, &amp;ldquo;like&amp;rdquo;, &amp;ldquo;apples&amp;rdquo;, etc ). Each token will only see the keys of tokens that precede it, so the query of &amp;ldquo;they&amp;rdquo; will not be multiplied with the key for &amp;ldquo;sweet&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;V (Value matrix)&lt;/strong&gt;: Value vectors are actual token representations. Once we’ve scored how relevant each token is, these are the values we add up to represent the token we&amp;rsquo;re paying attention to. In our example, this means that the vector for &amp;ldquo;they&amp;rdquo; will be computed as a weighted average of all the previous tokens (&amp;ldquo;I&amp;rdquo;, &amp;ldquo;like&amp;rdquo;, &amp;ldquo;apples&amp;rdquo;, &amp;ldquo;because&amp;rdquo;), but &amp;ldquo;apples&amp;rdquo; will be weighted much higher than any other, so the end result for the token &amp;ldquo;they&amp;rdquo; will be very close to the value vector for &amp;ldquo;apples&amp;rdquo;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These Q/K/V matrices are computed by multiplying the input of the decoding layer by three matrices (Wq, Wk and Wv) whose values are computed during training and constitute many of the LLM&amp;rsquo;s parameters. These three matrices are addressed together as an attention head, as we mentioned earlier. Modern LLMs usually include several attention heads for each step, so you&amp;rsquo;ll have several different matrices in each decoding step (and that&amp;rsquo;s why they&amp;rsquo;re said to use multi-headed attention).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-10-23-kv-caching/Q-K-V.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Simplified view of the Q/K/V matrices in a single self-attention head. The matrices go through a few more steps (softmax, regularization etc) which are not depicted here&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This process of computing the output vector for each token is called &lt;em&gt;scaled dot-product attention&lt;/em&gt; and, as we mentioned earlier, happens in every attention head of every decoding step. In summary, &lt;strong&gt;keys (K)&lt;/strong&gt; and &lt;strong&gt;values (V)&lt;/strong&gt; are the transformed representations of each preceding token that are used to compute attention, and they enable each token to gather information from the rest of the sequence by matching queries to keys and aggregating values.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s pay close attention to these computations. We know that LLMs generate output one token at a time. This means that the LLM will recompute the K-V values for the tokens of the prompt over and over again for each new output token it generates. If you have already generated, say, 100 tokens of output, producing the 101st token requires recomputing a forward pass over all 100 tokens. A naive implementation would repeatedly recalculate a lot of the same intermediate results for the older tokens at every step of generation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-10-23-kv-caching/KV-caching-no.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Detail of the Q/K multiplication. As you can see, the content of the QK matrix is essentially the same at all steps, except for the last row. This means that as soon as we accumulate a few input tokens, most of the QK matrix will be nearly identical every time. Something very similar happens for the final QKV matrix.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;By the third generation step (three tokens in context), the model computes six attention scores (a 3×3 / 2 matrix); many of these correspond to interactions that were already computed in earlier steps. For example, the attention of token &amp;ldquo;I&amp;rdquo; with itself was computed in the first step, yet the naive approach computes it again when processing the sequence &amp;ldquo;I like&amp;rdquo; and &amp;ldquo;I like apples&amp;rdquo; and so on. In fact, by the time the sentence is complete, the majority of the query-key pairs being calculated are repeats of prior computations. This redundancy makes inference much slower as the sequence length grows: the model wastes time recalculating attention contributions for tokens that haven’t changed.&lt;/p&gt;
&lt;p&gt;Clearly we want to avoid recomputing things like the key and value vectors for past tokens at every step. That’s exactly what &lt;strong&gt;KV caching&lt;/strong&gt; achieves.&lt;/p&gt;
&lt;h2 id=&#34;the-kv-cache&#34;&gt;
  The KV Cache
  
&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;KV caching&lt;/strong&gt; is an optimization that saves the key and value tensors from previous tokens so that the model doesn’t need to recompute them for each new token. The idea is straightforward: as the model generates tokens one by one, we store the keys and values produced at each layer for each token in a cache (which is just a reserved chunk of memory, typically in GPU RAM for speed). When the model is about to generate the next token, instead of recomputing all keys and values from scratch for the entire sequence, it retrieves the already-computed keys and values for the past tokens from this cache, and only computes the new token’s keys and values.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-10-23-kv-caching/KV-caching-yes.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Computing the QK matrix by reusing the results of earlier passes makes the number of calculations needed at each step nearly linear, speeding up inference several times and preventing slowdowns related to the input size.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In essence, with KV caching the transformer&amp;rsquo;s attention in each layer will take the new token’s query and concatenate it with the cached keys of prior tokens, then do the same for values, and move on immediately. The result is that each generation step’s workload is greatly reduced: the model focuses on what’s new instead of re-hashing the entire context every time.&lt;/p&gt;
&lt;h2 id=&#34;implementation&#34;&gt;
  Implementation
  
&lt;/h2&gt;
&lt;p&gt;Modern libraries implement KV caching under the hood by carrying a “past key values” or similar object through successive generation calls. For example, the Hugging Face Transformers library’s &lt;code&gt;generate&lt;/code&gt; function uses a &lt;code&gt;use_cache&lt;/code&gt; flag that is True by default, meaning it will automatically store and reuse past keys/values between decoding steps. Conceptually, you can imagine that after the first forward pass on the prompt, the model keeps all the K and V tensors. When generating the next token, it feeds only the new token through each layer along with the cached K and V from previous tokens, to compute the next output efficiently.&lt;/p&gt;
&lt;p&gt;In summary, KV caching transforms the workload of each generation step. Without caching, each step &lt;em&gt;repeats&lt;/em&gt; the full attention computation over the entire context. With caching, each step adds only the computations for the new token and the necessary interactions with prior tokens. This makes the per-step cost roughly constant. The longer the generation goes on, the more time is saved relative to the naive approach. KV caching is thus a &lt;strong&gt;time-memory trade-off&lt;/strong&gt;: we trade some memory to store the cache in order to save a lot of compute time on each step.&lt;/p&gt;
&lt;h2 id=&#34;limitations&#34;&gt;
  Limitations
  
&lt;/h2&gt;
&lt;p&gt;It’s important to note that KV caching only applies in &lt;em&gt;auto-regressive decoder&lt;/em&gt; models (where the output is generated sequentially). Models like BERT that process entire sequences in one go (and are not generative) do not use KV caching, since they don’t generate token-by-token or reuse past internal states. But for any generative LLM built on a decoder-only Transformer architecture, KV caching is a standard technique to speed up inference.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s worth noting that the KV cache needs to be managed just like every other type of cache. We&amp;rsquo;re going to analyze some ways to handle this cache effectively in another post.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;
  Conclusion
  
&lt;/h2&gt;
&lt;p&gt;The takeaway is clear: &lt;strong&gt;always leverage KV caching for autoregressive LLM inference&lt;/strong&gt; (and practically all libraries do this for you) unless you have a very specific reason not to. It will make your LLM deployments run faster and more efficiently.&lt;/p&gt;
&lt;p&gt;KV caching exemplifies how understanding the internals of transformer models can lead to substantial engineering improvements. By recognizing that keys and values of the attention mechanism can be reused across time steps, we unlock a simple yet powerful optimization. This ensures that even as our LLMs get larger and our prompts get longer, we can keep inference running quickly, delivering the snappy responses users expect from AI-driven applications.&lt;/p&gt;
&lt;h2 id=&#34;learn-more&#34;&gt;
  Learn more
  
&lt;/h2&gt;
&lt;p&gt;Here are some useful resources I used to write this post:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Illustrated Transformer&lt;/a&gt; by Jay Alammar&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jalammar.github.io/illustrated-gpt2/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The illustrated GPT-2&lt;/a&gt; by Jay Alammar&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://platform.openai.com/docs/guides/latency-optimization/3-use-fewer-input-tokens#use-fewer-input-tokens&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Latency optimization tips&lt;/a&gt; by OpenAI&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@joaolages/kv-caching-explained-276520203249&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KV Caching explained&lt;/a&gt; by João Lages&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://neptune.ai/blog/transformers-key-value-caching&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KV Caching&lt;/a&gt; by Neptune.ai&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.manning.com/books/build-a-large-language-model-from-scratch&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Build a Large Language Model (from scratch)&lt;/a&gt; by Sebastian Raschka&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>What is prompt caching?</title>
      <link>https://www.zansara.dev/posts/2025-10-17-prompt-caching/</link>
      <pubDate>Fri, 17 Oct 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2025-10-17-prompt-caching/</guid>
      <description>&lt;hr&gt;
&lt;p&gt;&lt;em&gt;This is episode 2 of a series of shorter blog posts answering questions I received during the course of my work and reflect common misconceptions and doubts about various generative AI technologies. You can find the whole series here: &lt;a href=&#34;https://www.zansara.dev/series/practical-questions&#34; &gt;Practical Questions&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A common piece of advice to improve speed and reduce cost of inference in LLMs is to use prompt caching. However, it&amp;rsquo;s often not clear what this means. What exactly is cached? When and why the improvements are really impactful? Understanding prompt caching starts with a deeper awareness of how computation and costs scale with large contexts.&lt;/p&gt;
&lt;h2 id=&#34;llms-are-stateless&#34;&gt;
  LLMS are stateless
  
&lt;/h2&gt;
&lt;p&gt;Each time an LLM processes input, it handles every token of the provided context. LLMs are stateless: this means that for every new message added to an existing chat, your application needs to submit the whole history which could include system prompts, documents, examples, and all the chat history.
The model recomputes all of those tokens each time.
This is a massive inefficiency. For example, with an input cost around $1 per 1 million tokens, sending 100,000 tokens across 1,000 requests would cost approximately $100, while about 95% of those tokens remain unchanged across requests. In essence, a large portion of computation is wasted on repeatedly processing information that never changes: the message history.&lt;/p&gt;
&lt;h2 id=&#34;stateless-vs-stateful-design&#34;&gt;
  Stateless vs stateful design
  
&lt;/h2&gt;
&lt;p&gt;Naive API implementations that omit caching force the model to process the entire context anew each time. This &amp;ldquo;stateless&amp;rdquo; method is simpler to implement, but wastefully expensive. The system pays repeatedly to recompute static context, which could otherwise be reused.&lt;/p&gt;
&lt;p&gt;In contrast, with a stateful cache strategy, the system stores parts of the context and only processes new inputs (queries). Consider the following case:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the system prompt is 10,000 tokens long&lt;/li&gt;
&lt;li&gt;each user message is about 100 tokens&lt;/li&gt;
&lt;li&gt;each assistant response is about 1000 tokens&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In both cases, the first request processes 10,100 tokens (1 system prompt + 1 user message). On the second message, a stateless request (no caching) needs to process 11,200 tokens (1 system prompt + first user message + first assistant response + the next user message) while a stateful one can first load the cache and then process only 1100 new tokens (the assistant response + the new user message). That&amp;rsquo;s an order of magnitude less tokens!
On top of that, as the chat continues, a stateful app will always need to only process the next new 1100 tokens, while the stateless version will process a chat history that grows by 1100 every time. For example, by the 10th request, with caching you need to process 1100 tokens, while without you need to deal with 20,000! (10,000 system prompt tokens + 9,000 assistant reply tokens + 1000 user message tokens).&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a recap to highlight the difference:&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
&lt;table style=&#34;width:100%; border: 2px solid black;&#34;&gt;
&lt;tr&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;No Prompt Caching&lt;/th&gt;
    &lt;th&gt;With Prompt Caching&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;td&gt;1st request&lt;/td&gt;
    &lt;td&gt;10,100 tokens&lt;br&gt;&lt;small&gt;10,000tk sys + 100tk user&lt;/small&gt;&lt;/td&gt;
    &lt;td&gt;10,100 tokens&lt;br&gt;&lt;small&gt;10,000tk sys + 100tk user&lt;/small&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;td&gt;2nd request&lt;/td&gt;
    &lt;td&gt;11,200 tokens&lt;br&gt;&lt;small&gt;10,000tk sys + 1000tk llm + (100 * 2) tk user&lt;/small&gt;&lt;/td&gt;
    &lt;td&gt;1100 tokens&lt;br&gt;&lt;small&gt;1000tk llm + 100tk user&lt;/small&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;td&gt;...&lt;/td&gt;
    &lt;td&gt;...&lt;/td&gt;
    &lt;td&gt;...&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;td&gt;10th request&lt;/td&gt;
    &lt;td&gt;20,000 tokens&lt;br&gt;&lt;small&gt;10,000tk sys + (1000 * 9)tk llm + (100 * 10) tk user&lt;/small&gt;&lt;/td&gt;
    &lt;td&gt;1100 tokens&lt;br&gt;&lt;small&gt;1000tk llm + 100tk user&lt;/small&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;While cache warm-up is not free, it can make a huge difference in the latency of your responses and, if you&amp;rsquo;re paying by the output token, reduce the costs by orders of magnitude.&lt;/p&gt;
&lt;h2 id=&#34;cache-hierarchies&#34;&gt;
  Cache Hierarchies
  
&lt;/h2&gt;
&lt;p&gt;Caching’s benefits come with architectural tradeoffs. Stateless designs are straightforward and predictably expensive: every token is always processed. Caching drastically reduces costs by reusing prior computation, but requires complexity in cache management, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cache invalidation: deciding how and when to refresh cached segments.&lt;/li&gt;
&lt;li&gt;Cache misses: when requested information isn’t in the cache, leading to full recomputation and latency spikes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because of these challenges, a single monolithic cache usually not enough to see many benefits. The most effective solution is a &lt;strong&gt;hierarchical cache strategy&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Effective prompt caching leverages multiple layers with varied lifetimes and hit rates:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;L1: System Prompt (e.g., 5,000 tokens)&lt;/strong&gt;: it rarely changes, so it has the best hit rate. In most chat you&amp;rsquo;ll at least hit this cache.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;L2: System Prompt + Examples and Tools (e.g., +20,000 tokens)&lt;/strong&gt;: may change per task, so it can has a lower hit rate than the system prompt, but eventually it depends completely on your application type. Agentic apps that make heavy use of tools benefit the most from caching them, as they follow the system prompt and might not depend at all from the user query or the agent&amp;rsquo;s decisions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;L3: System Prompt + Examples and Tools + Documents (e.g., +50,000 tokens)&lt;/strong&gt;: if you&amp;rsquo;re working with documents, caching any initial retrieval can help too. These documents are likely to change per user and/or per session, so it has a moderate/low hit rate. However, the size of these chunks usually makes it worth it if you have some spare capacity or a small and static knowledge base to retrieve from.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A layered approach like balances freshness and reuse, optimizing both cost and performance.&lt;/p&gt;
&lt;h2 id=&#34;automatic-prefix-caching&#34;&gt;
  Automatic prefix caching
  
&lt;/h2&gt;
&lt;p&gt;If you&amp;rsquo;re using a modern inference engine, prompt caching can also be done through &lt;strong&gt;automatic prefix caching&lt;/strong&gt;, where the engine itself takes the responsibility to identify and cache frequently used prefixes. Here you can find more details about the availability of this feature in &lt;a href=&#34;https://docs.vllm.ai/en/latest/design/prefix_caching.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;vLLM&lt;/a&gt;, &lt;a href=&#34;https://docs.sglang.ai/advanced_features/hicache_best_practices.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SDLang&lt;/a&gt; and &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/discussions/8947&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;llama.cpp&lt;/a&gt;, but there are many other engines supporting it.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-10-17-prompt-caching/optimizations_table.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A feature comparison across inference engines from &lt;a href=&#34;https://arxiv.org/pdf/2505.01658&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this May 2025 review&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;semantic-caching&#34;&gt;
  Semantic caching
  
&lt;/h2&gt;
&lt;p&gt;In extreme cases where cost, load or latency must be reduced to the maximum, semantic caching can also be employed. Semantic caching allows you to cache also the user queries and the assistant responses by keeping a registry of already processes user queries and performing a semantic search step between the new query and the cached ones. If a match is found, instead of invoking the LLM to generate a new answer, the cached reply is sent to the user immediately.&lt;/p&gt;
&lt;p&gt;Semantic caching however has several disadvantages that makes it worthwhile only in rare situations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Access control&lt;/strong&gt;. Caching must be done per user if each user has access to a different set of resources, to avoid accidental sharing of data and/or resources across users.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Very high similarity needed&lt;/strong&gt;: In order the the reply to be relevant, the semantic similarity between the two must be extremely high, or you risk that the answer returned to the user won&amp;rsquo;t match their question. Semantic similarity tends to overlook details which are often very important to an accurate reply: for example, &amp;ldquo;What&amp;rsquo;s the sum of these numbers: 1,2,3,4,5,6,7?&amp;rdquo; and  &amp;ldquo;What&amp;rsquo;s the sum of these numbers: 1,2,3,4,5,6,7,8?&amp;rdquo; will have an extremely high similarity, but returning the response of the first to the second would not be a good idea.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Language management&lt;/strong&gt;: what to do when the exact same question is asked in two different languages? Semantic similarity may be perfect if your embedder is multilingual, but the user won&amp;rsquo;t be pleased to receive a cached answer in a language different from their own.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Such constraints make cache misses extremely frequent, which defies the point of keeping a cache and simply adds complexity and latency to the system instead of reducing it. The similarity pitfalls introduces also nasty accuracy problems.&lt;/p&gt;
&lt;p&gt;In my personal experience, semantic caching is only useful for extremely high volume, low cost, public facing interfaces where accuracy is not critical. A perfect example could be a virtual assistant for anonymous customer support, or a helper bot for a software&amp;rsquo;s documentation search. In any case, you usually need additional checks on the output in order to trust such a system.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;
  Conclusion
  
&lt;/h2&gt;
&lt;p&gt;Prompt caching is not just about cutting costs or speeding things up: it is a necessary architectural approach that addresses the quadratic computational cost inherent in large-context LLM processing. Without it, your backend will repeatedly recompute largely static information, wasting resources and imposing latency penalties that impact your user&amp;rsquo;s experience. By adopting hierarchical, stateful caching and carefully designing prompts, you can reduce token processing costs and response speed by orders of magnitude, which is key for building sustainable, high-performance applications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why using a reranker?</title>
      <link>https://www.zansara.dev/posts/2025-10-09-rerankers/</link>
      <pubDate>Thu, 09 Oct 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2025-10-09-rerankers/</guid>
      <description>&lt;hr&gt;
&lt;p&gt;&lt;em&gt;This is episode 1 of a series of shorter blog posts answering questions I received during the course of my work and reflect common misconceptions and doubts about various generative AI technologies. You can find the whole series here: &lt;a href=&#34;https://www.zansara.dev/series/practical-questions&#34; &gt;Practical Questions&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Retrieval-Augmented Generation (RAG) systems are essential to connect large language models  with external knowledge sources. While in theory the retrieval step is enough to gather documents that are relevant to the user&amp;rsquo;s request, it&amp;rsquo;s often recommended to add an additional ranking step, the &lt;em&gt;reranking&lt;/em&gt;, to further filter the results.&lt;/p&gt;
&lt;p&gt;But why do we need rerankers? Isn’t semantic search good enough? The answer lies in understanding the limitations of traditional embedding-based retrieval.&lt;/p&gt;
&lt;h2 id=&#34;bi-encoders-vs-cross-encoders&#34;&gt;
  Bi-encoders vs Cross-encoders
  
&lt;/h2&gt;
&lt;p&gt;At the heart of modern, scalable semantic search systems lies the &lt;strong&gt;bi-encoder&lt;/strong&gt; model. This architecture creates independent vector representations for the query and the document; relevance is then computed through a similarity measure like the dot product or cosine similarity between those vectors.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-10-09-rerankers/bi-encoders.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This design scales well: You can precompute document embeddings, store them in your vector DB, and compare any incoming query against millions of documents very efficiently. However, this convenience comes at a cost: &lt;strong&gt;the system never truly reads the document in the context of the query&lt;/strong&gt;. There’s no token-level interaction between the query and document embedding to judge whether the document actually answers the question or it simply happen to be talking about the same topic, and therefore semantically similar.&lt;/p&gt;
&lt;p&gt;For example, the query &amp;ldquo;How to protect my application from DDOS attacks?&amp;rdquo; may be semantically close to the statement &amp;ldquo;You should always take steps to protect your systems from DDOS attacks&amp;rdquo;, but the statement does not contain the answer to the question. Without reranking, embedding-based retrieval systems often perform well at recall but poorly at precision.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cross-encoders&lt;/strong&gt; remedy the limitations of bi-encoders by encoding the query and document together, typically separated by a special token (like &lt;code&gt;[SEP]&lt;/code&gt;) using an encoder-only Transformer such as BERT. Then they include an additional fully connected layer that acts as a classifier, learning fine-grained token-level interactions that capture query/document word alignments, answer containment (i.e., does this passage actually answer the question?) and overall contextual relevance.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-10-09-rerankers/cross-encoders.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This difference (from separate encodings to a joint representation) gives cross-encoders their power but also their cost. Since relevance depends on the specific query, you can’t precompute the document embeddings: in fact, the concept of &amp;ldquo;query embedding&amp;rdquo; and &amp;ldquo;document embeddings&amp;rdquo; disappears.  Every query-document pair requires a fresh forward pass through the whole model, which can be prohibitively expensive on a large corpus.&lt;/p&gt;
&lt;h2 id=&#34;to-each-their-place&#34;&gt;
  To each their place
  
&lt;/h2&gt;
&lt;p&gt;No production system can afford to run interaction-rich models such as cross-encoders on millions of documents per query. Therefore, the two-stage retrieval pipeline remains the industry standard:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semantic Search (Bi-Encoder)&lt;/strong&gt; – Quickly narrows a massive corpus (e.g., millions of document chunks) down to a small candidate set (e.g., top 100 chunks). Bi-encoders can be built with any embedding model: popular closed source embedders include OpenAI&amp;rsquo;s, Voyage.ai, Cohere&amp;rsquo;s, Gemini and more, while on the open-source front you can find BGE embedders, Mistral&amp;rsquo;s models, Jina.ai, Gemma, IBM Granite, &lt;a href=&#34;https://huggingface.co/models?search=embedding&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;and more&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reranking (Cross-Encoder)&lt;/strong&gt; – Evaluates those top 100 candidates more deeply by jointly encoding the query and document. A popular closed source choice for reranking models is Cohere&amp;rsquo;s, while on the open source front you can find several Qwen-based rerankers, Jina.ai models, IBM&amp;rsquo;s Granite rerankers, BGE rerankers, and &lt;a href=&#34;https://huggingface.co/models?search=reranker&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;many more&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-10-09-rerankers/two-tiered-system.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;making-reranking-practical&#34;&gt;
  Making Reranking Practical
  
&lt;/h2&gt;
&lt;p&gt;Even in this two-tiered system, reranking may turn out to be too expensive for your latency constrains, but several engineering and modeling strategies have emerged to make it viable in production. Let’s break down a few of these methods.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Model Distillation&lt;/strong&gt;&lt;br&gt;
Distillation transfers the knowledge from a large, high-performing cross-encoder (often based on 12-layer BERT or similar) into a smaller student model (e.g., 6 layers, or even lighter). The process involves training the smaller model to mimic the scores or output logits of the larger one on large query–document pairs. While distillation inevitably loses some performance, careful tuning, domain-specific data, and intermediate-layer supervision can retain more than 90% of the original ranking quality at a fraction of the inference cost. You can learn more about model distillation &lt;a href=&#34;https://www.sbert.net/examples/sentence_transformer/training/distillation/README.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Listwise Reranking&lt;/strong&gt;&lt;br&gt;
Instead of scoring each query–document pair independently, listwise reranking generates scores for all top-k candidates in a single forward pass. This approach rearranges candidates into a batched tensor, leveraging GPU parallelism to process them together, reducing overhead from repeated encoder calls. Some implementations also use listwise loss functions (such as ListNet or LambdaMART-inspired objectives) to better preserve ranking order during training. To learn more about ML ranking, have a look at &lt;a href=&#34;https://towardsdatascience.com/ranking-basics-pointwise-pairwise-listwise-cd5318f86e1b/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this post&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Late Interaction Models (e.g., ColBERT)&lt;/strong&gt;&lt;br&gt;
Late interaction approaches store token-level embeddings of documents from fine-tuned contextual models. At query time, the system encodes the query tokens and performs efficient maximum similarity matching between query tokens and stored document tokens. By avoiding a full joint encoding across all tokens, these models approximate cross-encoder analysis but keep retrieval latency close to bi-encoder speeds. This approach can either substitute or complement cross-encoders by quickly reducing the candidates list returned from the vector database. To learn more about this approach, have a look at &lt;a href=&#34;https://medium.com/@aimichael/cross-encoders-colbert-and-llm-based-re-rankers-a-practical-guide-a23570d88548&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this blog post&lt;/a&gt; or &lt;a href=&#34;https://arxiv.org/abs/2004.12832&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the ColBERT paper&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Candidate Filtering and Adaptive k&lt;/strong&gt;&lt;br&gt;
Rather than always reranking a fixed top-k (like 100 documents), systems can use heuristics or intermediate classifiers to select fewer candidates when confidence in retrieval is high. This adaptive approach can cut reranking costs significantly while preserving precision in challenging cases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Approximate Cross-Attention Mechanisms&lt;/strong&gt;&lt;br&gt;
Instead of computing full self-attention across combined query and document tokens, some approaches reduce complexity by limiting cross-attention depth or dimensionality — for example, attending only to the top N most informative tokens, or pruning low-importance attention heads. This can drastically lower token computations while maintaining critical interaction signals.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Caching for Frequent Queries&lt;/strong&gt;&lt;br&gt;
In platforms where certain queries or query patterns repeat, caching reranking results or partial computations can remove the need to rerun the full cross-encoder. Combined with normalization and paraphrase detection, such caches can return precise results instantly for repeated requests.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In production pipelines, these methods are often stacked: for example, using late interaction for most queries, distillation for cost control, and adaptive candidate selection to minimize unnecessary work. The overarching theme is balancing precision and latency, ensuring that rerankers deliver their interaction-driven relevance boost without overwhelming the system’s budget or responsiveness.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Trying to play &#34;Guess Who&#34; with an LLM</title>
      <link>https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/</link>
      <pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/</guid>
      <description>

&lt;style&gt;
    p img {
        max-width: 500px;
    }

    p:has(&gt; img) {
        text-align:center!important;
    }

    pre {
        overflow: wrap;
    }
&lt;/style&gt;


&lt;p&gt;A few days ago I came to a realization. Modern LLMs can do a lot of things: they can &lt;a href=&#34;https://www.anthropic.com/news/claude-for-chrome&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;use a browser&lt;/a&gt; just like a human, they can (&lt;a href=&#34;https://dynomight.net/chess/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sometimes&lt;/a&gt;) &lt;a href=&#34;https://maxim-saplin.github.io/llm_chess/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;play chess&lt;/a&gt;, and they seem to be so smart that they apparently can be trusted as personal assistants: they can read and reply to emails, organize trips, do shopping online on your behalf, and so on.&lt;/p&gt;
&lt;p&gt;If that&amp;rsquo;s the case, I thought, it should be possible to also play some tabletop games with them!&lt;/p&gt;
&lt;p&gt;After all, many simple tabletop games don&amp;rsquo;t require a lot of skill to play. You need to be able to read and understand the rules (very easy for an LLM), you need eyes to see the board (piece of cake for a multimodal LLM), and some ways to interact with the board (most LLM are able to call tools nowadays). So I figured it would be a nice idea to try and figure out which of these LLMs is the most fun to play with. Maybe the charming personality of GPT-4o? Or the clever Claude Opus 4?&lt;/p&gt;
&lt;p&gt;I did not expect any of the results I got.&lt;/p&gt;
&lt;h1 id=&#34;building-the-game&#34;&gt;
  Building the game
  
&lt;/h1&gt;
&lt;p&gt;In order to be fair to dumber LLMs, I decided to start with a very simple tabletop game: &lt;a href=&#34;https://en.wikipedia.org/wiki/Guess_Who%3F&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Guess Who&lt;/a&gt;. If you are not familiar with &amp;ldquo;Guess Who&amp;rdquo;, here is a quick recap of the rules:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each player has a board full of characters.&lt;/li&gt;
&lt;li&gt;Each players draws an additional random character.&lt;/li&gt;
&lt;li&gt;Your goal is to guess which character the other player has received by asking yes/no questions, such as &amp;ldquo;Is your character male?&amp;rdquo; or &amp;ldquo;Does your character have black hair?&amp;rdquo; and so on&lt;/li&gt;
&lt;li&gt;The first player to guess the opponent character&amp;rsquo;s name wins.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As you can see we&amp;rsquo;re not talking of a complex game like Catan or a strategy game like chess, but a simple, fun tabletop game suitable for kids too.&lt;/p&gt;
&lt;p&gt;In order to build the game, as I am no frontend developer, I spent a few too many bucks on my favorite vibe-coding tool, &lt;a href=&#34;https://www.anthropic.com/claude-code&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Claude Code&lt;/a&gt;, padded in a bit of &lt;a href=&#34;https://github.com/google-gemini/gemini-cli&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gemini CLI&lt;/a&gt; when I run out of credits, made a few tweaks by hand when asking the bots to do so felt overkill, and a few evenings later I had &lt;a href=&#34;https://www.zansara.dev/guess-who/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this nice Guess Who game&lt;/a&gt; live.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/game-ui.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Feel free to play a few round using your favorite LLM. The game supports OpenAI compatible endpoints, plus Anthropic&amp;rsquo;s and Google&amp;rsquo;s API. And if you don&amp;rsquo;t trust me with your API key, go ahead and &lt;a href=&#34;https://github.com/ZanSara/guess-who&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fork or clone the game&lt;/a&gt; (and maybe leave a ⭐ while you&amp;rsquo;re at it ), host it where you like (it&amp;rsquo;s a single HTML page with a bit of vanilla JS at the side) and have fun.&lt;/p&gt;
&lt;p&gt;Now for the spoilers.&lt;/p&gt;
&lt;h1 id=&#34;not-as-many-llms&#34;&gt;
  Not as many LLMs
  
&lt;/h1&gt;
&lt;p&gt;One of the first surprises was that, in practice, there aren&amp;rsquo;t as many models that are capable of vision and tool calling at the same time. Apart from flagship models such as GPTs and Claude, OSS options were limited. Even GPT-OSS, unfortunately, does not support vision. I was especially surprised to learn that I could not play with any version of popular Chinese models such as Qwen or Deepseek, as they&amp;rsquo;re either text only or unable to call tools.&lt;/p&gt;
&lt;p&gt;Either way, using a mix of proprietary hosting, &lt;a href=&#34;https://openrouter.ai/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenRouter&lt;/a&gt; and &lt;a href=&#34;https://www.together.ai/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Together.ai&lt;/a&gt;, I had plenty of models to try and ended up trying out 21:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Nova Pro v1&lt;/li&gt;
&lt;li&gt;Amazon Nova Lite v1&lt;/li&gt;
&lt;li&gt;Claude Opus 4.1&lt;/li&gt;
&lt;li&gt;Claude Opus 4.0&lt;/li&gt;
&lt;li&gt;Claude Sonnet 4.0&lt;/li&gt;
&lt;li&gt;Claude Sonnet 3.7&lt;/li&gt;
&lt;li&gt;Gemini 2.5 Pro&lt;/li&gt;
&lt;li&gt;Gemini 2.5 Flash&lt;/li&gt;
&lt;li&gt;Gemini 2.5 Flash Lite&lt;/li&gt;
&lt;li&gt;GML 4.5&lt;/li&gt;
&lt;li&gt;Grok 4&lt;/li&gt;
&lt;li&gt;GPT 5&lt;/li&gt;
&lt;li&gt;GPT 5 Nano&lt;/li&gt;
&lt;li&gt;GPT 5 Mini&lt;/li&gt;
&lt;li&gt;GPT 4o&lt;/li&gt;
&lt;li&gt;Llama 4 Maverick&lt;/li&gt;
&lt;li&gt;Llama 4 Scout&lt;/li&gt;
&lt;li&gt;Mistral Medium 3.1&lt;/li&gt;
&lt;li&gt;Mistral Small 3.2&lt;/li&gt;
&lt;li&gt;Sonoma Dusk Alpha&lt;/li&gt;
&lt;li&gt;Sonoma Sky Alpha&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It may sound like a lot of work, but as you&amp;rsquo;ll see in a minute, for many of them it didn&amp;rsquo;t take me long to form an opinion about their skill.&lt;/p&gt;
&lt;h1 id=&#34;the-prompts&#34;&gt;
  The prompts
  
&lt;/h1&gt;
&lt;p&gt;Starting from the assumption that playing Guess Who should be within the cognitive abilities of most modern LLMs, I decided to settle for a simple system prompt, something that resembles the way I would explain the game to a fellow human.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You are an AI assistant playing &amp;ldquo;Guess Who&amp;rdquo; against the user. Here&amp;rsquo;s how the game works:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You&amp;rsquo;ll receive the board and your character image&lt;/li&gt;
&lt;li&gt;You must try to guess the user&amp;rsquo;s character by asking yes/no questions&lt;/li&gt;
&lt;li&gt;You must answer the user&amp;rsquo;s yes/no questions about your character&lt;/li&gt;
&lt;li&gt;One question per player per turn, no exceptions&lt;/li&gt;
&lt;li&gt;You can eliminate characters from your board based on the user&amp;rsquo;s answers using the eliminateCharacter tool (this will only update the UI, so keep in mind who you&amp;rsquo;re eliminating)&lt;/li&gt;
&lt;li&gt;The first player to correctly guess the opponent&amp;rsquo;s character wins the game. When the user guesses your character or you guess theirs, call the endGame tool&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;After this system prompt, I send two more prompts:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Here is the board:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/full-board.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;and here is your character:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div&gt;
&lt;div style=&#34;max-width: 100px; margin: auto;&#34;&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/Amy.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Unfortunately these two prompts need to be user prompts (not system prompts) because some LLMs (looking at you, Mistral!) do not support images in their system prompts.&lt;/p&gt;
&lt;p&gt;Last, when the user presses the Start button, one more system message is sent:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Generate a brief, friendly greeting message to start a Guess Who game.
Tell the user whether you received the images of your board and your character and ask them for their first question.
Keep it conversational and under 2 sentences.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The LLM also receives two tools to use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;eliminateCharacter&lt;/code&gt;, described as &amp;ldquo;Eliminate a character from your board when you learn they cannot be the user&amp;rsquo;s character&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;endGame&lt;/code&gt;, described as &amp;ldquo;When you or the the user guess correctly, call this tool to end the game.&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;playing&#34;&gt;
  Playing
  
&lt;/h1&gt;
&lt;p&gt;With the game implemented and ready to go, I finally started playing a bit. I was especially curious how small models could deal with a game like this, so I began with GPT-5 Mini. Here is what happens:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/gpt-5-mini-example.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Ahah, GPT 5 Mini is far dumber than I thought! Let&amp;rsquo;s try Gemini 2.5 Flash instead.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/gemini-2.5-flash-example.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Oh wow this is incredible. Ok, time to try a smarter model and have some actual fun. Claude Sonnet 4.0 will do for now.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/claude-sonnet-4-example.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;At this point it started to become unbelievable. Did I fail to explain the game? Is something wrong with the prompts? It couldn&amp;rsquo;t be, because some other models (such as the almighty GPT-4o) do what I expect instead:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/gpt-4o-example.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;While others left me shocked:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/claude-opus-4.1-example.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;How can a flagship model like &lt;em&gt;Claude Opus 4.1&lt;/em&gt; fail this way? I kept trying several other LLMs in disbelief, slowly coming to terms with the fact that most of them don&amp;rsquo;t readily understand the concept of playing adversarial games, even simple ones as Guess Who.&lt;/p&gt;
&lt;h1 id=&#34;a-systematic-review&#34;&gt;
  A systematic review
  
&lt;/h1&gt;
&lt;p&gt;At this point I felt the duty to document this problem across all the models that had enough capabilities (vision + tool calling) to play this game. If I ever want an LLM personal assistant to handle my private data and to act on my behalf, I&amp;rsquo;d better make sure they understand that they can&amp;rsquo;t just hand out my credentials to the first kind thief that asks them.&lt;/p&gt;
&lt;p&gt;Here is a systematic review of the results, ordered roughly from worst to best. However, keep in mind that this is all based on a very small test sample, and although most models consistently fail the same way every time, there were some with a far more erratic behavior, looking very smart at times and incredibly dumb the next.&lt;/p&gt;
&lt;p&gt;First of all I list and disqualify all models that do not hide the identity of their character. Of the survivors, I ranked them by whether or not you can actually play with them in any capacity (many can&amp;rsquo;t see well enough to tell the characters apart) and if the game is actually playable, how easy it is to break it.&lt;/p&gt;
&lt;h2 id=&#34;unplayable-models&#34;&gt;
  Unplayable models
  
&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Can&amp;rsquo;t understand the instructions at all&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These models understood only part of the system prompt (if any), resulting in unpredictable answers.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Amazon Nova Lite v1&lt;/summary&gt;
&lt;p&gt;Possibly the most unpredictable model. Every run was a surprise. This is just a small sample to give you an idea.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/cant-play-at-all/amazon-nova-lite-v1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;strong&gt;Reveals their charater unprompted in the first message&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Shockingly common issue among all tested models. They just volunteer the information unprompted. I assume they don&amp;rsquo;t understand they&amp;rsquo;re not supposed to help the user, or that this is an information they should hide.&lt;/p&gt;
&lt;p&gt;All these models have been tested several times to ensure this is their default behavior and not an exception. Some other models do occasionally fail this way (looking at you, Mistral Medium 3.1), but only rarely. Models listed here fail in this way very consistently.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Claude Opus 4.1&lt;/summary&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/claude-opus-4-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;Claude Opus 4.0&lt;/summary&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/claude-opus-4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;Claude Sonnet 4.0&lt;/summary&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/claude-sonnet-4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;Claude Sonnet 3.7&lt;/summary&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/claude-sonnet-3-7.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;Gemini 2.5 Flash&lt;/summary&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/gemini-2.5-flash.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;Gemini 2.5 Flash Lite&lt;/summary&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/gemini-2.5-flash-lite.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;GPT 5 Mini&lt;/summary&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/gpt-5-mini.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;GPT 5 Nano&lt;/summary&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/gpt-5-nano.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;Llama 4 Scout&lt;/summary&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/llama-4-scout.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;Sonoma Sky Alpha&lt;/summary&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/sonoma-sky-alpha.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;GML 4.5&lt;/summary&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-immediately/gml-4.5.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;strong&gt;Reveals their charater as soon as asked&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Some models did not volunteer the information but didn&amp;rsquo;t exactly protect it either.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Amazon Nova Pro v1&lt;/summary&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-when-asked/amazon-nova-pro-v1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;Llama 4 Maverick&lt;/summary&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-when-asked/llama-4-maverick.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;Mistral Small 3.2&lt;/summary&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/reveals-name-when-asked/mistral-small-3.2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;h2 id=&#34;game-looks-playable-but-its-actually-broken&#34;&gt;
  Game looks playable but it&amp;rsquo;s actually broken
  
&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Low vision skills&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These models are smart enough to understand the basics of the game, but it&amp;rsquo;s impossible to play with them due to their &lt;strong&gt;weak vision skills&lt;/strong&gt;. These models simply can&amp;rsquo;t see well enough to delete the right character from the board or answer correctly all questions about their own. They will then hallucinate random answers and delete random characters from their boards, making the game unplayable.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Gemini 2.5 Pro&lt;/summary&gt;
&lt;p&gt;Gemini 2.5 Pro evidently has issues seeing both the board and the characters. Here it shows both flaws by deleting the wrong characters and lying about its character in a single response:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/cant-see-well/gemini-2.5-pro.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/cant-see-well/gemini-2.5-pro-board.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;GPT-4o&lt;/summary&gt;
&lt;p&gt;GPT-4o also has issues seeing the board and the characters, but its blind spots less predictable than for Gemini 2.5 Pro, so it can occasionally manage to play for a while. It also frequently forgets to eliminate any characters from its board. GPT-4o also tends to get distracted, lose track of the turns, and so on.&lt;/p&gt;
&lt;p&gt;Here it deletes the wrong characters and loses track of the turns:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/cant-see-well/gpt-4o-wrong-removal.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/cant-see-well/gpt-4o-wrong-removal-board.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;and here it has trouble seeing its character:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/cant-see-well/gpt-4o-character.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;Mistral Medium 3.1&lt;/summary&gt;
&lt;p&gt;Mistral Medium 3.1 has been hard to place. It seems that its biggest weakness is removing the correct characters from the board, although it does a much better job than Gemini 2.5 or GPT-4o. I&amp;rsquo;ve never seen it failing to describe its own character correctly, but it occasionally behaves in a very dumb way (on occasion it even revealed its character in the first message!). You may have flawless runs with this model or it might fail on the get-go.&lt;/p&gt;
&lt;p&gt;Here it deletes a couple of unrelated characters:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/cant-see-well/mistral-medium-3.1-wrong-removal.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/cant-see-well/mistral-medium-3.1-wrong-removal-board.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;strong&gt;No tool calling&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is debatable whether the inability of a model to do tool calling should be considered a penalty: in theory LLMs remember everything perfectly, so they could choose what to ask next based on what they asked earlier and what characters still could match the opponent&amp;rsquo;s. However, in practice no LLM could be trusted keeping track of the game this way, and I decided that the inability to invoke tools when instructed is a big enough flaw to disqualify them.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Sonoma Dusk Alpha&lt;/summary&gt;
&lt;p&gt;Assessing the vision skills of this model has been difficult due to its unwillingness to ever call the &lt;code&gt;eliminateCharacter&lt;/code&gt; tool. Sonoma Dusk Alpha doesn&amp;rsquo;t seem to have issues seeing its character, but it&amp;rsquo;s too weak to be considered playable: won&amp;rsquo;t enforce turn taking, can be convinced I won the game without naming its character, and it&amp;rsquo;s likely not really trying to narrow down on my character, it&amp;rsquo;s just asking some questions.&lt;/p&gt;
&lt;p&gt;Here is an example gameplay.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/cant-see-well/sonoma-dusk-alpha.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;h2 id=&#34;playable-models&#34;&gt;
  Playable models
  
&lt;/h2&gt;
&lt;p&gt;These models seems to understand the game, don&amp;rsquo;t have issues seeing all the features of the characters, but they&amp;rsquo;re still quite vulnerable to basic manipulation attempts. Typical issues are related to &lt;strong&gt;prompt hacking&lt;/strong&gt;, where the LLM simply does what I say rather than enforcing the game rules, and &lt;strong&gt;low tool handling ability&lt;/strong&gt;, where the LLM doesn&amp;rsquo;t use the available tools when it should or uses them incorrectly.&lt;/p&gt;
&lt;p&gt;To test these skills, I checked whether the model will enforce turn taking when asking the question, and what happens when I claim to have won without naming the LLM&amp;rsquo;s hidden character.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Grok 4&lt;/summary&gt;
&lt;p&gt;Grok 4 is a decent player but by far not a good one. It clearly sees the board and the character, it eliminates characters correctly most of the times, but fails to enforce turns.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/playable/grok-4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/playable/grok-4-board.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here an example of a game where a couple of mistakes were enough to prevent the model from winning (my character was Amy again).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/playable/grok-4-failing-game-last-minute.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;An award to this model for resisting my attempt to unilaterally declare victory without breaking the game! This is the only model that succeeded at this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/playable/grok-4-resists-winning.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;GPT 5&lt;/summary&gt;
&lt;p&gt;GPT 5 is probably the best model to play with in terms of raw capabilities. It makes very occasional mistakes deleting characters but it&amp;rsquo;s mostly on point.&lt;/p&gt;
&lt;p&gt;However it was really slow and annoying to get it to play at all. It generally can&amp;rsquo;t seem to use tools and ask the next question at the same time, even if its response structure suggests it should be able to do it: this means that to play you must answer its question, wait for it to delete its character, and only then you can ask your own.&lt;/p&gt;
&lt;p&gt;It is also unbelievably slow compared to any other LLM I played with, which kills the fun.&lt;/p&gt;
&lt;p&gt;Here you can see GPT 5 enforcing turn-taking (plus a gratuitous pun?!):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/playable/gpt-5-no-sense-of-humor.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;When claiming that I won, GPT 5 almost manages to understand that it might be not the case, but still ruins the game. Unfortunately this is not a fluke, GPT 5 consistently reveals the character in this situation. It won&amp;rsquo;t call the tool just yet, but once it reveals the character the game is over.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/misbehaves-when-I-say-I-won/gpt-5.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/misbehaves-when-I-say-I-won/gpt-5-again.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here is an example of a game where GPT 5 actually wins:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/games/playable/gpt-5-full-winning-game.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case the &lt;code&gt;endGame&lt;/code&gt; tool was also invoked correctly.&lt;/p&gt;
&lt;/details&gt;
&lt;h1 id=&#34;can-this-be-fixed&#34;&gt;
  Can this be fixed?
  
&lt;/h1&gt;
&lt;p&gt;My guess was that you can fix this behavior with a better system prompt. After this experiment I went back to the system prompt and described the game in far more detail.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You are an AI assistant playing &amp;ldquo;Guess Who&amp;rdquo; against the user. Here&amp;rsquo;s how the game works.&lt;/p&gt;
&lt;h1 id=&#34;game-rules&#34;&gt;
  Game Rules
  
&lt;/h1&gt;
&lt;p&gt;You will receive an image of the full Guess Who board showing all available characters. You will also receive an image of a specific character. This is YOUR character that the user must try to guess. REMEMBER: don&amp;rsquo;t reveal who the character is! That&amp;rsquo;s the point of the game!&lt;/p&gt;
&lt;p&gt;Your goal is to ask the user questions to identify THEIR hidden character while answering their questions about YOUR character. You need to ask the user yes/no questions about their character&amp;rsquo;s appearance (e.g., &amp;ldquo;Does your character have glasses?&amp;rdquo;, &amp;ldquo;Is your character male?&amp;rdquo;). When the user tells you something about THEIR character, you must eliminate characters that don&amp;rsquo;t fit the description from your board using the eliminateCharacter tool. Keep in mind that this tool only updated the UI: you have to keep track of which characters are eliminated in your mind. Think carefully about which characters to eliminate and explain your reasoning out loud before calling the tool. Make sure to only eliminate characters that definitely do not match the user&amp;rsquo;s description. If you make mistakes it will become impossible for you to win the game!&lt;/p&gt;
&lt;p&gt;When the user asks you questions about YOUR character, answer concisely and truthfully based on the character image you received.&lt;/p&gt;
&lt;p&gt;Each player can only ask ONE question and receive ONE answer - asking more than one question or asking another before your opponent had a chance to ask theirs is cheating! You must not cheat!&lt;/p&gt;
&lt;p&gt;The first player to correctly guess the opponent&amp;rsquo;s character name wins the game, so try to guess when you&amp;rsquo;re reasonably confident. A good time to guess is when your board only has one or two characters left. When you think you know the user&amp;rsquo;s character, make your guess clearly (e.g., &amp;ldquo;Is your character [Name]?&amp;rdquo;) This is how you can manage to win the game.&lt;/p&gt;
&lt;p&gt;When the user guesses correctly, call the endGame tool to finish the game. When the user tells you that you guessed their character, call the endGame tool to finish the game.&lt;/p&gt;
&lt;p&gt;Now you will receive YOUR board and YOUR character. Let&amp;rsquo;s play!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can load this prompt &lt;a href=&#34;https://www.zansara.dev/guess-who/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;in the game&lt;/a&gt; by checking the Advanced tab in the settings.&lt;/p&gt;
&lt;p&gt;This prompt helps a lot the models understand that they can&amp;rsquo;t reveal the character&amp;rsquo;s identity: however it&amp;rsquo;s also not solving the problem entirely. For example this is what Claude Opus 4.1 does with this prompt:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-09-15-playing-guess-who-with-an-llm/claude-opus-4.1-spelled-out-prompt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Guess what? There&amp;rsquo;s only one character with gray hair and glasses on the board, and that&amp;rsquo;s Emily&amp;hellip; Should I review my system prompt again, make it even more detailed?&lt;/p&gt;
&lt;p&gt;At this point I gave up. Feel free to iterate on the prompt until you get one that works, and if you manage, I beg you to share it with me.&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;
  Conclusion
  
&lt;/h1&gt;
&lt;p&gt;In the near future I plan to make a proper leaderboard for this simple game, to make an automated system to assess the model&amp;rsquo;s skills and (hopefully) track their progress in this field.&lt;/p&gt;
&lt;p&gt;In the meantime, feel free to try your own favorite LLMs here and form your own opinion.&lt;/p&gt;
&lt;p&gt;However, let&amp;rsquo;s be honest: if we need this level of effort to make Claude play such a simple game as Guess Who without messing up, how can we trust LLMs in general to handle our data and our money in the far more ambiguous and complex world out there? I suppose LLMs are not ready (yet) to be left unsupervised.&lt;/p&gt;
&lt;p class=&#34;fleuron&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-05-06-teranoptia/&#34;&gt;SDE&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Guess Who</title>
      <link>https://www.zansara.dev/projects/guess-who/</link>
      <pubDate>Mon, 01 Sep 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/projects/guess-who/</guid>
      <description>&lt;p&gt;LLMs nowadays are headed for superintelligence, they say. They should make good tabletop game partners then, I guess?&lt;/p&gt;
&lt;p&gt;In this repo you can find a simple implementation of a Guess Who game. If you&amp;rsquo;re not familiar with the rules, here they are: each player receives a character and has to guess the opponent&amp;rsquo;s character name by asking one yes/no question at a time. For example: &amp;ldquo;Is your character blonde?&amp;rdquo; or &amp;ldquo;Does your character have glasses?&amp;rdquo;. Whoever guesses the opponent&amp;rsquo;s character first wins.&lt;/p&gt;
&lt;p&gt;A very simple game that should be within the understanding of any multimodal language model, right? &lt;a href=&#34;https://zansara.dev/guess-who/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Try for yourself&lt;/a&gt; to find out.&lt;/p&gt;
&lt;p&gt;You can also find the game on &lt;a href=&#34;https://github.com/ZanSara/guess-who&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;. Contributions of all kinds are welcome 🙇&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>L-systems</title>
      <link>https://www.zansara.dev/projects/l-systems/</link>
      <pubDate>Fri, 01 Aug 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/projects/l-systems/</guid>
      <description>&lt;p&gt;In order to find a quick way to design images for my more abstract posts, I figured that an &lt;a href=&#34;http://paulbourke.net/fractals/lsys/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;L-system&lt;/a&gt; generator could help.&lt;/p&gt;
&lt;p&gt;You can find it here: &lt;a href=&#34;https://www.zansara.dev/L-systems&#34; &gt;L-systems generator&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Kudos to the original developer anvaka, whose work you can find &lt;a href=&#34;https://anvaka.github.io/lsystem&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Can you really interrupt an LLM?</title>
      <link>https://www.zansara.dev/posts/2025-06-02-can-you-really-interrupt-an-llm/</link>
      <pubDate>Mon, 02 Jun 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2025-06-02-can-you-really-interrupt-an-llm/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;
  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2025-06-02-can-you-really-interrupt-an-llm/Can%20you%20really%20interrupt%20an%20LLM%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/0382cc35-21be-4455-8ff7-dfa6ce99a4f4?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;
  &lt;audio controls style=&#34;width:100%;&#34;&gt;
    &lt;source src=&#34;https://www.zansara.dev/posts/2025-06-02-can-you-really-interrupt-an-llm/Can%20you%20really%20interrupt%20an%20LLM%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt; 
  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;
    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;
    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;With the recent release of &lt;a href=&#34;https://support.anthropic.com/en/articles/11101966-using-voice-mode-on-claude-mobile-apps&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Voice Mode&lt;/a&gt; for &lt;a href=&#34;https://www.anthropic.com/claude&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Claude&lt;/a&gt;, it seems like Voice AI is a solved problem. Now that LLMs can speak natively, there&amp;rsquo;s apparently no more need for any of the &lt;a href=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/&#34; &gt;complex voice pipelines&lt;/a&gt; that used to be necessary last year: no need to do voice activity detection, no need to pipe data from the speech-to-text model to the LLM and then back to the text-to-speech engine at blazing speed in order to achieve a natural conversation flow. Modern LLMs can &lt;a href=&#34;https://vimeo.com/945587944&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;laugh and sing&lt;/a&gt;: what else could we need?&lt;/p&gt;
&lt;p&gt;It turns out, a lot is still missing. Here is an example:&lt;/p&gt;
&lt;div style=&#34;display: flex; align-content: center;&#34;&gt;
  &lt;video style=&#34;margin:auto;&#34; height=&#34;814&#34; width=&#34;382&#34; controls &gt;
      &lt;source src=&#34;https://www.zansara.dev/posts/2025-06-02-can-you-really-interrupt-an-llm/claude.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;/video&gt;
&lt;/div&gt;
&lt;p&gt;Is this an issue with Claude? Have a look at Gemini:&lt;/p&gt;
&lt;div style=&#34;display: flex; align-content: center;&#34;&gt;
  &lt;video style=&#34;margin:auto;&#34; height=&#34;796&#34; width=&#34;384&#34; controls &gt;
      &lt;source src=&#34;https://www.zansara.dev/posts/2025-06-02-can-you-really-interrupt-an-llm/gemini.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;/video&gt;
&lt;/div&gt;
&lt;p&gt;or even at the venerable GPT-4o, the most mature Voice AI out there:&lt;/p&gt;
&lt;div style=&#34;display: flex; align-content: center;&#34;&gt;
  &lt;video style=&#34;margin:auto;&#34; height=&#34;814&#34; width=&#34;382&#34; controls &gt;
      &lt;source src=&#34;https://www.zansara.dev/posts/2025-06-02-can-you-really-interrupt-an-llm/gpt-4o.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;/video&gt;
&lt;/div&gt;
&lt;p&gt;What&amp;rsquo;s going on?&lt;/p&gt;
&lt;p&gt;This simple exercise highlights two core issues that are often overlooked when developing Voice AI agents. Let&amp;rsquo;s see them.&lt;/p&gt;
&lt;h1 id=&#34;problem-1-llms-dont-perceive-time&#34;&gt;
  Problem #1: LLMs don&amp;rsquo;t perceive time
  
&lt;/h1&gt;
&lt;p&gt;As algorithms trained to predict the most likely next word, LLMs don&amp;rsquo;t have any concept of time. When dealing with text, this issue is not visible; however as soon as we cross over the domain of voice, their lack of understanding of time becomes a much bigger problem. LLMs still perceive the conversation as a series of tokens, with no concept of speed, pauses, or anything of that sort. They are often trained to control cadence, tone, to imitate pauses and adjust their talking speed, but they don&amp;rsquo;t &lt;em&gt;perceive&lt;/em&gt; these features as we do: they are just additional properties of the output tokens.&lt;/p&gt;
&lt;p&gt;This means that an LLM will have a very hard time understanding requests that involve altering the timing of the response unless there is additional, external tooling to help them. &amp;ldquo;Please wait three second before replying&amp;rdquo;, for example, is a meaningless query to an LLM that doesn&amp;rsquo;t have a timer tool of some sort.&lt;/p&gt;
&lt;p&gt;For example, here is what GPT-4o (the LLM that handles time best) can do when asked to wait for a few seconds:&lt;/p&gt;
&lt;div style=&#34;display: flex; align-content: center;&#34;&gt;
  &lt;video style=&#34;margin:auto;&#34; height=&#34;814&#34; width=&#34;382&#34; controls &gt;
      &lt;source src=&#34;https://www.zansara.dev/posts/2025-06-02-can-you-really-interrupt-an-llm/wait-before-replying.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;/video&gt;
&lt;/div&gt;
&lt;h1 id=&#34;problem-2-interruptions-are-not-a-native-capability&#34;&gt;
  Problem #2: Interruptions are not a native capability
  
&lt;/h1&gt;
&lt;p&gt;Most Voice AIs out there feature the possibility to interrupt them. However, not having any innate concept of time, the ability to interrupt the model has to be implemented on the application end: and this is where it usually goes wrong.&lt;/p&gt;
&lt;p&gt;Voice LLMs are very fast: they generate the response in a fraction of the time needed to play it out. When you prompt an LLM, the model will start generate audio tokens and streaming them, but by the time the first one reaches the user, in most cases the majority of the response (if not the entirety of it) has already been generated and is queued in the audio buffer, waiting to be played.&lt;/p&gt;
&lt;p&gt;When a user interrupts the LLM, the app normally stops the playback as soon as possible and &lt;strong&gt;empties the audio buffer&lt;/strong&gt;, regardless of its content.&lt;/p&gt;
&lt;p&gt;However, unless the app notifies the LLM of this action, &lt;strong&gt;the LLM has no way to know that only part of the response was played to the user.&lt;/strong&gt; This is why most models believe they finished their countdown when in practice they were interrupted earlier.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-06-02-can-you-really-interrupt-an-llm/naive-interruption.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;can-it-be-solved&#34;&gt;
  Can it be solved?
  
&lt;/h1&gt;
&lt;p&gt;If you paid close attention you may have noticed that GPT-4o, while it still stops at the wrong number, it does not believe it completed the countdown, but it understood that the counting was interrupted at some point before the end.&lt;/p&gt;
&lt;p&gt;This is possible because OpenAI&amp;rsquo;s Realtime API provides the possibility to tell the model at which point it was interrupted. In the Realtime API documentation you can find this feature implemented with the event &lt;code&gt;conversation.item.truncate&lt;/code&gt; (see the &lt;a href=&#34;https://platform.openai.com/docs/api-reference/realtime-client-events/conversation/item/truncate&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;docs&lt;/a&gt;):&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;{
    &amp;#34;event_id&amp;#34;: &amp;#34;event_678&amp;#34;,
    &amp;#34;type&amp;#34;: &amp;#34;conversation.item.truncate&amp;#34;,
    &amp;#34;item_id&amp;#34;: &amp;#34;msg_002&amp;#34;,
    &amp;#34;content_index&amp;#34;: 0,
    &amp;#34;audio_end_ms&amp;#34;: 1500
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In this event, the &lt;code&gt;audio_end_ms&lt;/code&gt; is what signals the model that the audio was interrupted at a certain time, before its natural end. This event in turn also trims the transcript to make the LLM know what the user heard and was was never played out. Precision however is not trivial to accomplish, because it&amp;rsquo;s very easy for the application to register the interruption later than when it actually occurred and, like in the case of the ChatGPT app, convince the LLM that the interruption happened in the wrong point.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-06-02-can-you-really-interrupt-an-llm/gpt-4o-interruption.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the case of Gemini, there is a &lt;a href=&#34;https://ai.google.dev/gemini-api/docs/live#interruptions&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Handling Interruptions&amp;rdquo;&lt;/a&gt; section in its Live API documentation. However the feature seems incomplete, as they state:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Users can interrupt the model&amp;rsquo;s output at any time. When Voice activity detection (VAD) detects an interruption, the ongoing generation is canceled and discarded. &lt;strong&gt;Only the information already sent to the client is retained in the session history&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As we&amp;rsquo;ve seen, this is not sufficient to handle interruptions correctly. It&amp;rsquo;s likely that this issue is not currently fixable.&lt;/p&gt;
&lt;p&gt;In the case of Claude we don&amp;rsquo;t know yet if that&amp;rsquo;s an inherent limitation or a bug in the app, because at the time of writing there is no Live/Realtime API available for Claude.&lt;/p&gt;
&lt;h1 id=&#34;wrapping-up&#34;&gt;
  Wrapping up
  
&lt;/h1&gt;
&lt;p&gt;Voice Mode for LLMs is a huge step forward for voice AI, but it&amp;rsquo;s not a silver bullet. LLMs are first and foremost text prediction algorithms, and even when adapted to work with voice, some of their limitations persists. In order to have complete control, building a &lt;a href=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/&#34; &gt;full pipeline for voice&lt;/a&gt; may still be your best bet if you have the infrastructure to achieve a low enough latency; otherwise, always make sure to test the behavior of your LLMs in these corner cases and stick to more well-tested models (in this case, OpenAI&amp;rsquo;s) for better handling of time.&lt;/p&gt;
&lt;p class=&#34;fleuron&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-05-06-teranoptia/&#34;&gt;SDH&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A simple vibecoding exercise</title>
      <link>https://www.zansara.dev/posts/2025-05-21-vibecoding/</link>
      <pubDate>Wed, 21 May 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2025-05-21-vibecoding/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;
  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2025-05-21-vibecoding/A%20simple%20vibecoding%20exercise%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/d422a5f5-80b9-432f-a40c-68003e520044?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;
  &lt;audio controls style=&#34;width:100%;&#34;&gt;
    &lt;source src=&#34;https://www.zansara.dev/posts/2025-05-21-vibecoding/A%20simple%20vibecoding%20exercise%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt; 
  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;
    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;
    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Sometimes, after an entire day of coding, the last thing you want to do is to code some more. It would be so great if I could just sit down and enjoy some Youtube videos&amp;hellip;&lt;/p&gt;
&lt;p&gt;Being abroad, most of the videos I watch are in a foreign language, and it helps immensely to have subtitles when I&amp;rsquo;m not in the mood for hard focus. However, Youtube subtitles are often terrible or missing entirely.&lt;/p&gt;
&lt;p&gt;Can the magic of modern Generative AI fix this problem?&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ve all heard of &lt;a href=&#34;https://x.com/karpathy/status/1886192184808149383&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;vibecoding&lt;/a&gt;: sitting in front of your IDE, telling an AI what you want the code to do and letting it loose to create &lt;em&gt;something&lt;/em&gt; that achieves that goal. In this case, the goal is rather simple: given a video file, generate subtitles for it using &lt;a href=&#34;https://deepgram.com/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepgram&lt;/a&gt;&amp;rsquo;s SDK (since it has a &lt;a href=&#34;https://deepgram.com/pricing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;generous free tier&lt;/a&gt;). It seems such a simple task that even an LLM should be able to reach it with minimal or no assistance. Right?&lt;/p&gt;
&lt;h1 id=&#34;the-first-shot-openai&#34;&gt;
  The first shot: OpenAI
  
&lt;/h1&gt;
&lt;p&gt;For this simple experiment I decided not to use a dedicated IDE or VSCode plugin, but to stick to text based tools. After all, I expected this task to be sorted with a single Python script made by OpenAI&amp;rsquo;s famed &lt;a href=&#34;https://openai.com/index/introducing-o3-and-o4-mini/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;o4-mini-high&lt;/code&gt;&lt;/a&gt;, advertized as &amp;ldquo;Great at coding and visual reasoning&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-05-21-vibecoding/openai-model-selector.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The prompt was very simple:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Write me a Python script that, given a video file, returns me an &lt;a href=&#34;https://en.wikipedia.org/wiki/SubRip&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;.srt&lt;/a&gt; subtitle file using Deepgram&amp;rsquo;s API.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As expected, the model thought about it, did some web searches, and then cooked up a script that used &lt;code&gt;deepgram-sdk&lt;/code&gt; and &lt;code&gt;deepgram-captions&lt;/code&gt;. Looked good, but as soon as I tried to run it, issues came up. Deepgram&amp;rsquo;s SDK complained about wrong formats, wrong SDK version, HTTP errors&amp;hellip; Copy-pasting the errors back to &lt;code&gt;o4-mini-high&lt;/code&gt; was vain: the model seems to understand that the Deepgram API had a major upgrade since the model was trained, but fails to use the new version. After four or five attempts (including one full restart of the chat), I realized this was going nowhere and I looked for another option.&lt;/p&gt;
&lt;h1 id=&#34;the-backup-option-claude-code&#34;&gt;
  The backup option: Claude Code
  
&lt;/h1&gt;
&lt;p&gt;I&amp;rsquo;ve heard many times that the best LLMs for vibecoding belong to the &lt;a href=&#34;https://www.anthropic.com/claude&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Claude&lt;/a&gt; family. On top of that, there&amp;rsquo;s a cool TUI utility called &lt;a href=&#34;https://www.anthropic.com/claude-code&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Claude Code&lt;/a&gt; that allows you to vibecode from the terminal, no IDE required. It uses &lt;a href=&#34;https://www.anthropic.com/claude/sonnet&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Claude 3.7 Sonnet&lt;/a&gt; under the hood, so the expectations are high.&lt;/p&gt;
&lt;p&gt;Time to give it a try.&lt;/p&gt;
&lt;p&gt;Installing the utility is matter of a single command (&lt;code&gt;npm install -g @anthropic-ai/claude-code&lt;/code&gt;) and a few emails to authenticate the utility into my Anthropic account. Once done we&amp;rsquo;re ready to go.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2025-05-21-vibecoding/claude-code-intro.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The prompt is the same:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Write me a Python script that, given a video file, returns me an .srt subtitle file using Deepgram&amp;rsquo;s API.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Sure enough, Claude&amp;rsquo;s first attempt also fails for the same reason as o4 did: their knowledge is outdated, and they both use the Deepgram&amp;rsquo;s API in a way that&amp;rsquo;s not compabible with its new v3 API. However, after a few attempts, Claude actually produces a script that &lt;em&gt;mostly&lt;/em&gt; works.&lt;/p&gt;
&lt;h1 id=&#34;results&#34;&gt;
  Results
  
&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://gist.github.com/ZanSara/4bab5db89376d595128e0688804d694c&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Here&lt;/a&gt; is the output (I pasted the &lt;code&gt;README&lt;/code&gt; and the &lt;code&gt;requirements.txt&lt;/code&gt; at the top of the file for simplicity). I only needed to replace &lt;a href=&#34;https://developers.deepgram.com/docs/models-languages-overview#nova-2&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;nova-2&lt;/code&gt;&lt;/a&gt; with &lt;a href=&#34;https://developers.deepgram.com/docs/models-languages-overview#nova-3&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;nova-3&lt;/code&gt;&lt;/a&gt; to get the best possible transcription for Portuguese (other languages may get better transcription with &lt;code&gt;nova-2&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;To summarize:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Is it perfect? No.&lt;/strong&gt; I can easily spot a lot of improvements to the code just by looking at it. It&amp;rsquo;s quite verbose, for example.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Was it cheap? No.&lt;/strong&gt; This script costed me a few dollars worth of tokens and about half a hour of trial and errors, about the hourly rate of a US software engineer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Is it enough for my purposes? Absolutely.&lt;/strong&gt; Now I am finally able to enjoy my videos with good quality subtitles without too much hassle.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Could somebody that can&amp;rsquo;t program do this?&lt;/strong&gt; I&amp;rsquo;m not so sure. Given how simple this task was, I was a bit disappointed by how long it took and I am rather skeptical about the ability of today&amp;rsquo;s LLMs to handle more complex requests without oversight - at least with the tools I used.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, looking at the big picture, the trend is clear. Three years ago, LLMs could just about write coherent sentences. Today, they can write decent helper scripts. Soon the may be able to implement your side projects from start to finish.&lt;/p&gt;
&lt;p&gt;Will it feel like a blessing or a curse? We&amp;rsquo;ll soon find out.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Edit 22/05/2025: Claude 4 has been released the day after I published this post, so here is a video of myself reimplementing this same script with the new model ✨&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#39;iframe-wrapper&#39;&gt;
  &lt;iframe src=&#34;https://drive.google.com/file/d/1cTo-VD8sFYYau900zIwFSCgkxLDt9iWO/preview&#34; width=100% height=100% allow=&#34;autoplay&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p class=&#34;fleuron&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-05-06-teranoptia/&#34;&gt;{z&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Llama Models in the EU</title>
      <link>https://www.zansara.dev/posts/2025-05-16-llama-eu-ban/</link>
      <pubDate>Fri, 16 May 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2025-05-16-llama-eu-ban/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;
  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2025-05-16-llama-eu-ban/Using%20Llama%20Models%20in%20the%20EU%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/a3c834c1-2832-4b5e-8f3b-27ae94f33dfe?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;
  &lt;audio controls style=&#34;width:100%;&#34;&gt;
    &lt;source src=&#34;https://www.zansara.dev/posts/2025-05-16-llama-eu-ban/Using%20Llama%20Models%20in%20the%20EU%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt; 
  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;
    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;
    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&#34;https://ai.meta.com/blog/llama-4-multimodal-intelligence/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Llama 4 family&lt;/a&gt; has been released over a month ago and I finally found some time to explore it. Or so I wished to do, until I realized one crucial issue with these models:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;They are banned in the EU.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Apparently Meta can’t be bothered to comply with EU regulations on AI, and therefore opted for a wide ban that should prevent such laws to apply to them. Of course, while this limitation is technically valid for each and every person and company domiciled in the EU, the problem arises primarily for companies that want to use Llama 4 to offer services and for researchers planning to work with these models, be it for evaluation, fine-tuning, distillation or other derivative work. Always keep in mind that I’m not a lawyer, so nothing of what I’m writing here constitutes as legal advice.&lt;/p&gt;
&lt;h1 id=&#34;the-terms&#34;&gt;
  The terms
  
&lt;/h1&gt;
&lt;p&gt;The interesting part of this ban can be found by reading the &lt;a href=&#34;https://github.com/meta-llama/llama-models/blob/main/models/llama4/USE_POLICY.md&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;terms&lt;/a&gt; of the Acceptable Usage Policy (AUP):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;With respect to any &lt;strong&gt;multimodal models&lt;/strong&gt; included in Llama 4, the rights granted under Section 1(a) of the Llama 4 Community License Agreement are not being granted to you if you are an individual domiciled in, or a company with a principal place of business in, the European Union.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As you can see, the restriction applies strictly to multimodal LLMs. Llama4 models are all multimodal, and that’s why the entire family of models is not accessible from the EU. However, if anyone releases a derivative model that is not multimodal, in theory the ban would not apply. I’m yet to see any such derivative model: if you know of any, let me know!&lt;/p&gt;
&lt;p&gt;Interestingly, the terms also state that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This restriction does not apply to end users of a product or service that incorporates any such multimodal models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So if you’re a company outside of the EU and provide services based on Llama4 to EU customers, you’re probably off the hook. Such interpretation seems to be confirmed by &lt;a href=&#34;https://www.llama.com/faq/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Meta’s FAQ&lt;/a&gt; about Llama models, which state:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Can a non-EU based company develop a product or service using the Llama multimodal models and distribute such product or service within the EU?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Yes, if you are a company with a principal place of business outside of the EU, you may distribute products or services that contain the Llama multimodal models in accordance with your standard global distribution business practices [&amp;hellip;]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://www.llama.com/faq/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Meta’s FAQ&lt;/a&gt; are actually quite throughout, so if you have any doubt about your specific case you should head there and read more.&lt;/p&gt;
&lt;h1 id=&#34;what-about-other-llamas&#34;&gt;
  What about other Llamas?
  
&lt;/h1&gt;
&lt;p&gt;This wide EU ban is not new: it was introduced with &lt;a href=&#34;https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Llama 3.2 Vision&lt;/a&gt;, the first multimodal model released by Meta. The clause does not exist for any model older than Llama 3.2.&lt;/p&gt;
&lt;p&gt;To summarize, here is a list of which models can be used in the EU:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Llama 4: all banned because they&amp;rsquo;re all multimodal (&lt;a href=&#34;https://github.com/meta-llama/llama-models/blob/main/models/llama4/USE_POLICY.md&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;terms&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Llama 3.3: allowed because it&amp;rsquo;s not multimodal (&lt;a href=&#34;https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/USE_POLICY.md&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;terms&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Llama 3.2: text only models are allowed, vision models are not allowed (&lt;a href=&#34;https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/USE_POLICY.md&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;terms&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Llama 3.1 and earlier: allowed because there&amp;rsquo;s no such clause (&lt;a href=&#34;https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/USE_POLICY.md&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;terms&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So for now this is the state of Llama licenses. My take is that with the implementation and rollout of the EU AI Act in 2025 and 2026, Meta will eventually adapt to make sure that the models are compliant with the way the Act is enforced in practice and relax, if not lift, the ban on newer models.&lt;/p&gt;
&lt;p&gt;Also, Llama4 has not been shining in the &lt;a href=&#34;https://lmarena.ai/?leaderboard&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;benchmarks&lt;/a&gt; (scroll down, I promise you it’s there)&amp;hellip; we Europeans may not be missing much.&lt;/p&gt;
&lt;p class=&#34;fleuron&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-05-06-teranoptia/&#34;&gt;AZ*&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ODSC East: LLMs that think - Demystifying Reasoning Models</title>
      <link>https://www.zansara.dev/talks/2025-05-13-odsc-east-reasoning-models/</link>
      <pubDate>Wed, 14 May 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/talks/2025-05-13-odsc-east-reasoning-models/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://odsc.com/speakers/llms-that-think-demystifying-reasoning-models/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Announcement&lt;/a&gt;, &lt;a href=&#34;https://www.zansara.dev/posts/2025-05-12-beyond-hype-reasoning-models/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;teaser article&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/1Gmxx2G-H0aozBZtACCvGIqJNgybXB716/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;.
All resources can also be found in
&lt;a href=&#34;https://drive.google.com/drive/folders/1Iy_mJr7MYdrbb-W-g1U38gkPBjrV8dGu?usp=drive_link&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my archive&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;At &lt;a href=&#34;https://odsc.com/boston/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ODSC East 2025&lt;/a&gt; I talked about reasoning models: what they are, what they aren&amp;rsquo;t, how they work and when to use them. Is OpenAI&amp;rsquo;s o4 AGI? Is it an AI Agent? Or is it just a glorified chain-of-thought prompt under the hood? To answer these questions, I classified LLMs into a small &amp;ldquo;taxonomy&amp;rdquo; based on the post-training steps they go through, in order to highlight how reasoning models differ qualitatively from their predecessors just like instruction-tuned models were not simple text-completion models with a better prompt. I also covered the effect of increasing the reasoning effort of the model, clarifying when it&amp;rsquo;s useful and when it can lead to overthinking.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Beyond the hype of reasoning models: debunking three common misunderstandings</title>
      <link>https://www.zansara.dev/posts/2025-05-12-beyond-hype-reasoning-models/</link>
      <pubDate>Mon, 12 May 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2025-05-12-beyond-hype-reasoning-models/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;
  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2025-05-12-beyond-hype-reasoning-models/Beyond%20the%20hype%20of%20reasoning%20models:%20debunking%20three%20common%20misunderstandings%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/9a99c7a3-fda2-49ac-b27e-ed2f0e4e93b9?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;
  &lt;audio controls style=&#34;width:100%;&#34;&gt;
    &lt;source src=&#34;https://www.zansara.dev/posts/2025-05-12-beyond-hype-reasoning-models/Beyond%20the%20hype%20of%20reasoning%20models:%20debunking%20three%20common%20misunderstandings%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt; 
  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;
    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;
    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;With the release of OpenAI’s o1 and similar models such as DeepSeek R1, Gemini 2.0 Flash Thinking, Phi 4 Reasoning and more, a new type of LLMs entered the scene: the so-called reasoning models. With their unbelievable scores in the toughest benchmarks for machine intelligence, reasoning models immediately got the attention of most AI enthusiasts, sparking speculations about their capabilities and what those could mean for the industry.&lt;/p&gt;
&lt;p&gt;However, as often in the field of Generative AI, the hype makes it very difficult to understand at a glance what these models can really do. But before we jump into the details let’s clarify what we’re talking about.&lt;/p&gt;
&lt;h1 id=&#34;what-is-a-reasoning-model&#34;&gt;
  What is a reasoning model?
  
&lt;/h1&gt;
&lt;p&gt;Reasoning models are LLMs that are able to “think”. Instead of generating a reply immediately after the user’s prompt, like every other LLM, they first generate a series of “reasoning tokens”, which is nothing more than the model thinking out loud, breaking down a complex problem into smaller steps, checking all its assumptions, asking itself whether it made any mistakes, double-checking its results, and so on. Once the model is satisfied by its conclusions, it starts generating actual response tokens that summarize the conclusions reached during the reasoning phase and presents those tokens to the user.&lt;/p&gt;
&lt;p&gt;In the case of some models such as OpenAI’s, the reasoning tokens are hidden from the user. In otner models, such as most open source ones, the reasoning output can be returned to the user as well. However, this trace is not optimized to be read by people, so it often looks odd and contrived even when it reaches the correct conclusions.&lt;/p&gt;
&lt;p&gt;Now that we understand better what a reasoning model is, let’s discuss a few common misunderstandings related to them.&lt;/p&gt;
&lt;h1 id=&#34;are-reasoning-models-agi&#34;&gt;
  Are reasoning models AGI?
  
&lt;/h1&gt;
&lt;p&gt;AGI stands for Artificial General Intelligence, and it’s one of the most ill-defined terms in Generative AI. Several people have tried to offer a more precise definition of this term, out of which my favourite is the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;AGI is an AI that is better than any human at any economically valuable task.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Under this light it’s clear that no current LLM, not even the most advanced reasoning model, it yet at the level where it could replace any human at any task. They can surely offer very valuable help with their vast knowledge and their growing ability to reason, but they’re not yet at the point where they can take onto any job without further specialization and complex tooling around them.&lt;/p&gt;
&lt;h1 id=&#34;are-reasoning-models-ai-agents&#34;&gt;
  Are reasoning models AI agents?
  
&lt;/h1&gt;
&lt;p&gt;An AI agent is usually defined as any application that can use tools to achieve complex goals. Considering that reasoning models are usually able to use tools, it’s natural to think that they themselves may be considered AI agents.&lt;/p&gt;
&lt;p&gt;In practice, however, reasoning models on their own hardly qualify as agents. Many powerful agents systems do have an LLM at their core: they use it to understand the user’s request and plan the actions to take to achieve the goal they’re set to. Reasoning models are a perfect fit as the minds of agents like that, due to their advanced capabilities to break down problems into smaller, manageable parts and self-correct their strategy on the fly if something goes wrong. Taken in isolation, though, reasoning models can’t be called AI agents.&lt;/p&gt;
&lt;h1 id=&#34;are-reasoning-models-glorified-cot-prompts&#34;&gt;
  Are reasoning models glorified CoT prompts?
  
&lt;/h1&gt;
&lt;p&gt;If you have worked with AI agents and other LLM systems designed to solve problems, you’ve surely come across Chain of Thought prompting. In short, this technique involves adding in the system prompt of your LLM instructions to “think step by step” before replying. This makes the LLM think out loud before reaching a conclusion and, even in regular non-reasoning LLMs, improves significantly their problem solving skills.&lt;/p&gt;
&lt;p&gt;At a first glance, the output of a reasoning model may look precisely like the output of a CoT prompt, so some experts may think that their reasoning capabilities are the same. This is a mistake. Reasoning models are much more powerful than regular LLMs, even when these are equipped with a CoT prompt: this is because reasoning models pass through one additional step during their training where they learn to refine their “thinking step by step” skills through supervised learning on prompts with verifiable output, such as mathematical problems. Reasoning models are not zero-shot resoners like regular LLMs: they’re fine-tuned for it.&lt;/p&gt;
&lt;h1 id=&#34;wrapping-up&#34;&gt;
  Wrapping up
  
&lt;/h1&gt;
&lt;p&gt;Reasoning models may not be the super-human intelligence some of us are waiting for, but they surely are a significant step forward toward LLMs with very strong reasoning abilities.&lt;/p&gt;
&lt;p&gt;If you want to learn more about what reasoning models can do, how they reason, when to use them and more, make sure to attend my talk &lt;a href=&#34;https://odsc.com/speakers/llms-that-think-demystifying-reasoning-models/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LLMs that Think: Demystifying Reasoning Models&lt;/a&gt; at this year’s virtual edition of &lt;a href=&#34;https://odsc.com/boston/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ODSC East&lt;/a&gt;. See you there!&lt;/p&gt;
&lt;p class=&#34;fleuron&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-05-06-teranoptia/&#34;&gt;ifo&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Real Life Subtitles</title>
      <link>https://www.zansara.dev/projects/real-life-subtitles/</link>
      <pubDate>Thu, 01 May 2025 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/projects/real-life-subtitles/</guid>
      <description>&lt;p&gt;Have you ever struggled to listen to a native speaker of the language you&amp;rsquo;re learning and wished you could see subtitles in real life as well?&lt;/p&gt;
&lt;p&gt;With Real Life Subtitles, the problem is solved ✨ Just open &lt;a href=&#34;https://www.zansara.dev/real-life-subtitles/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this page&lt;/a&gt;, insert your &lt;a href=&#34;https://deepgram.com/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepgram&lt;/a&gt; API key, press the Listen button and enjoy your realtime transcriptions!&lt;/p&gt;
&lt;p&gt;And if you want to experiment, you can &lt;a href=&#34;https://github.com/ZanSara/real-life-subtitles&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fork it on GitHub&lt;/a&gt; and make your own.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PyData Global: Building LLM Voice Bots with Open Source Tools</title>
      <link>https://www.zansara.dev/talks/2024-12-03-pydata-global-voice-bots/</link>
      <pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/talks/2024-12-03-pydata-global-voice-bots/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://global2024.pydata.org/cfp/talk/T3YDBP/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Announcement&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/1rXb4-m-BWwhAqDCDBXpzw6nJ9OELOpSl/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/1bja0O8LG7790UIU7HpAYXat-BYXeUbK-/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;demo video&lt;/a&gt; and full video (&lt;a href=&#34;https://www.youtube.com/watch?v=Td5dFdG0wE4&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Youtube&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/1HTEEs-Zr8mZoJA8a7AuiJf61soGvQNPI/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;backup&lt;/a&gt;).
All resources can also be found in
&lt;a href=&#34;https://drive.google.com/drive/folders/1ZPkne2QxOnSXfchv08CWkAZG3duXxd4Z?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my archive&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;hahahugoshortcode68s0hbhb&#34;&gt;
  &lt;div class=&#39;iframe-wrapper&#39;&gt;
  &lt;iframe src=&#34;https://drive.google.com/file/d/1HTEEs-Zr8mZoJA8a7AuiJf61soGvQNPI/preview&#34; width=100% height=100% allow=&#34;autoplay&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

  
&lt;/h2&gt;
&lt;h2 id=&#34;demo&#34;&gt;
  Demo
  
&lt;/h2&gt;
&lt;p&gt;During the talk I showed a recording of a demo built with &lt;a href=&#34;https://github.com/intentional-ai/intentional&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intentional&lt;/a&gt;, a new library to prompt voice bots in a way that takes inspiration from classic intent-based chatbots. Here are the instructions needed to run this same demo on your own machine and play with it.&lt;/p&gt;
&lt;div class=&#34;notice info&#34;&gt;
  &lt;div class=&#34;notice-content&#34;&gt;&lt;p&gt;&lt;em&gt;Intentional is still in its very first stages of development and highly unstable!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;I am looking for contributors to help this project come to life, so if you would like to help there are many ways to do so: leave a star on &lt;a href=&#34;https://github.com/intentional-ai/intentional&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the repo&lt;/a&gt;, &lt;a href=&#34;https://intentional-ai.github.io/intentional/docs/home/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;test the library&lt;/a&gt; on your machine, &lt;a href=&#34;https://github.com/intentional-ai/intentional/issues/new&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;open an issue&lt;/a&gt;, reach out to me to leave feedback (you can find my contact &lt;a href=&#34;https://www.zansara.dev/&#34; &gt;on the homepage&lt;/a&gt;), &lt;a href=&#34;https://github.com/intentional-ai/intentional&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;spread the word&lt;/a&gt; about Intentional, or even &lt;a href=&#34;https://intentional-ai.github.io/intentional/CONTRIBUTING/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;contribute to the project&lt;/a&gt; with a PR.&lt;/em&gt;&lt;/p&gt;
&lt;!-- Place this tag in your head or just before your close body tag. --&gt;
&lt;script async defer src=&#34;https://buttons.github.io/buttons.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;a class=&#34;github-button&#34; href=&#34;https://github.com/intentional-ai&#34; data-color-scheme=&#34;no-preference: light; light: light; dark: dark;&#34; data-size=&#34;large&#34; data-show-count=&#34;true&#34; aria-label=&#34;Follow @intentional-ai on GitHub&#34;&gt;Follow @intentional-ai&lt;/a&gt;
&lt;a class=&#34;github-button&#34; href=&#34;https://github.com/intentional-ai/intentional/subscription&#34; data-color-scheme=&#34;no-preference: light; light: light; dark: dark;&#34; data-size=&#34;large&#34; data-show-count=&#34;true&#34; aria-label=&#34;Watch intentional-ai/intentional on GitHub&#34;&gt;Watch&lt;/a&gt;
&lt;a class=&#34;github-button&#34; href=&#34;https://github.com/intentional-ai/intentional&#34; data-color-scheme=&#34;no-preference: light; light: light; dark: dark;&#34; data-size=&#34;large&#34; data-show-count=&#34;true&#34; aria-label=&#34;Star intentional-ai/intentional on GitHub&#34;&gt;Star&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;First, install Intentional and the plugin for &lt;a href=&#34;https://textual.textualize.io/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Textual&lt;/a&gt;&amp;rsquo;s UI:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install intentional intentional-textual-ui
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;notice info&#34;&gt;
  &lt;div class=&#34;notice-content&#34;&gt;&lt;em&gt;This demo was only tested on Linux (Ubuntu). You will need &lt;code&gt;portaudio&lt;/code&gt; in order for Intentional to handle audio from your microphone, so if you face errors during the installation, try:&lt;/em&gt; &lt;code&gt;sudo apt install portaudio19-dev&lt;/code&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Next, you&amp;rsquo;ll need the configuration file where the conversation tree is defined. Intentional bots are, at their core, entirely defined by this configuration file (with the partial exception of tools, as you can see &lt;a href=&#34;https://intentional-ai.github.io/intentional/docs/home/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;in the documentation&lt;/a&gt;). Download the demo configuration file &lt;a href=&#34;https://drive.google.com/file/d/1dkvxpCH6uny8ew3wrsgh7SPZdqvKuTyd/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;from this link&lt;/a&gt; and save it as &lt;code&gt;demo.yml&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now, make your OpenAI API key available to the CLI:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;export &lt;span style=&#34;color:#79c0ff&#34;&gt;OPENAI_API_KEY&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&amp;lt;your api key&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now you&amp;rsquo;re ready to run the bot. Intentional comes with a simple CLI that can directly run bots from a config file and draw their conversation graph. To run the bot and talk to it as shown in the video, run:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;intentional demo.yml
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You should see a UI such as this coming up (with the chat history empty):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/talks/2024-12-03-pydata-global-voice-bots-demo-ui.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Just start speaking and the bot will quickly reply to you.&lt;/p&gt;
&lt;div class=&#34;notice info&#34;&gt;
  &lt;div class=&#34;notice-content&#34;&gt;&lt;em&gt;Occasionally the transcriptions don&amp;rsquo;t work well. The bot is generating such transcriptions exclusively for your convenience, so even if they&amp;rsquo;re mangled, in most cases you can be confident that the model actually heard you well.&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;To see the conversation graph, run this command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;intentional demo.yml --draw
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The PNG file will be created next to &lt;code&gt;demo.yml&lt;/code&gt; and will be called &lt;code&gt;demo.png&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intentional</title>
      <link>https://www.zansara.dev/projects/intentional/</link>
      <pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/projects/intentional/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/intentional-ai/intentional/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intentional&lt;/a&gt; is an open-source framework to build reliable chatbots that actually do what you expect from them.&lt;/p&gt;
&lt;p&gt;Pure LLM-based chatbots are very hard to control: when a lot of very specific instructions are pushed into their system prompt, their performance will get worse and worse the more instructions you add. These bots will work very well on small demos, but don&amp;rsquo;t scale to real use cases, where you may need the bot to follow a very specific set of workflows depending on the situation it find itself in, without improvising.&lt;/p&gt;
&lt;p&gt;Intentional introduces a new way of prompting the LLM in a way that gives the developer full control on the conversation at scale while retaining the  smooth conversational skills of the LLM.&lt;/p&gt;
&lt;p&gt;Intentional is still in its very early stages: &lt;a href=&#34;https://github.com/intentional-ai/intentional/issues&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;get in touch&lt;/a&gt; if you want to give any feedback or contribute!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building Reliable Voice Bots with Open Source Tools - Part 2</title>
      <link>https://www.zansara.dev/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/</link>
      <pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;
  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools/Building%20Reliable%20Voice%20Bots%20with%20Open%20Source%20Tools%20-%20Part%201%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/b61c5d80-3b5c-42d7-92b7-bd60607ce454?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;
  &lt;audio controls style=&#34;width:100%;&#34;&gt;
    &lt;source src=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools/Building%20Reliable%20Voice%20Bots%20with%20Open%20Source%20Tools%20-%20Part%201%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt; 
  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;
    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;
    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;This is part two of the write-up of my talk at &lt;a href=&#34;https://www.zansara.dev/talks/2024-09-05-building-voice-agents-with-open-source-tools/&#34; &gt;ODSC Europe 2024&lt;/a&gt; and &lt;a href=&#34;https://www.zansara.dev/talks/2024-10-29-odsc-west-voice-agents/&#34; &gt;ODSC West 2024&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;In the last few years, the world of voice agents saw dramatic leaps forward in the state of the art of all its most basic components. Thanks mostly to OpenAI, bots are now able to understand human speech almost like a human would, they&amp;rsquo;re able to speak back with completely naturally sounding voices, and are able to hold a free conversation that feels extremely natural.&lt;/p&gt;
&lt;p&gt;But building reliable and effective voice bots is far from a solved problem. These improved capabilities are raising the bar, and even users accustomed to the simpler capabilities of old bots now expect a whole new level of quality when it comes to interacting with them.&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/&#34; &gt;Part 1&lt;/a&gt; we&amp;rsquo;ve seen mostly &lt;strong&gt;the challenges&lt;/strong&gt; related to building such bot: we discussed the basic structure of most voice bots today, their shortcomings and the main issues that you may face on your journey to improve the quality of the conversation.&lt;/p&gt;
&lt;p&gt;In this post instead we will focus on &lt;strong&gt;the solutions&lt;/strong&gt; that are available today and we are going to build our own voice bot using &lt;a href=&#34;https://www.pipecat.ai&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pipecat&lt;/a&gt;, a recently released open-source library that makes building these bots a lot simpler.&lt;/p&gt;
&lt;h1 id=&#34;outline&#34;&gt;
  Outline
  
&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Start from &lt;a href=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/&#34; &gt;Part 1&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#a-modern-voice-bot&#34; &gt;A modern voice bot&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#voice-activity-detection-vad&#34; &gt;Voice Activity Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tools&#34; &gt;Tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#llm-based-intent-detection&#34; &gt;LLM-based intent detection&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#intent-detection&#34; &gt;Intent detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#prompt-building&#34; &gt;Prompt building&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reply-generation&#34; &gt;Reply generation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-about-latency&#34; &gt;What about latency&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-code&#34; &gt;The code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#looking-forward&#34; &gt;Looking forward&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;a-modern-voice-bot&#34;&gt;
  A modern voice bot
  
&lt;/h1&gt;
&lt;p&gt;At this point we have a &lt;a href=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/&#34; &gt;comprehensive view&lt;/a&gt; of the issues that we need to solve to create a reliable, usable and natural-sounding voice agents. How can we actually build one?&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s take a look at the structure we defined earlier and see how we can improve on it.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools/structure-of-a-voice-bot-2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;voice-activity-detection-vad&#34;&gt;
  Voice Activity Detection (VAD)
  
&lt;/h2&gt;
&lt;p&gt;One of the simplest improvements to this architecture is the addition of a robust Voice Activity Detection (VAD) model. VAD gives the bot the ability to hear interruptions from the user and react to them accordingly, helping to break the classic, rigid turn-based interactions of old-style bots.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools/structure-of-a-voice-bot-vad.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;However, on its own VAD models are not enough. To make a bot truly interruptible we also need the rest of the pipeline to be aware of the possibility of an interruption and be ready to handle it: speech-to-text models need to start transcribing and the text-to-speech component needs to stop speaking as soon as the VAD picks up speech.&lt;/p&gt;
&lt;p&gt;The logic engine also needs to handle a half-spoken reply in a graceful way: it can&amp;rsquo;t just assume that the whole reply was spoken out, and neither it can drop the whole reply as it never started. Most LLMs can handle this scenario by altering the last message in their conversation history, but implementing this workflow in practice is often not straightorward, because you need to keep track of how much of the reply was heard by the user, and when exactly this interruption happened.&lt;/p&gt;
&lt;p&gt;The quality of your VAD model matters a lot, as well as tuning its parameters appropriately. You don&amp;rsquo;t want the bot to interrupt itself at every ambient sound it detects, but you also want the interruption to happen promptly, with a few hundreds of milliseconds of delay. Some of the best and most used models out there are &lt;a href=&#34;https://github.com/snakers4/silero-vad&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Silero&lt;/a&gt;&amp;rsquo;s VAD models, or alternatively &lt;a href=&#34;https://picovoice.ai/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Picovoice&lt;/a&gt;&amp;rsquo;s &lt;a href=&#34;https://picovoice.ai/platform/cobra/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cobra&lt;/a&gt; models.&lt;/p&gt;
&lt;h2 id=&#34;tools&#34;&gt;
  Tools
  
&lt;/h2&gt;
&lt;p&gt;Tools are often a major component of you bot&amp;rsquo;s functionality. Modern and effective voice bots today are often able to take basic actions such as looking up data in a database or calling simple functions.&lt;/p&gt;
&lt;p&gt;Function calling is a feature of most of today&amp;rsquo;s LLMs, so it&amp;rsquo;s often a low-hanging fruit in terms of improvements to your bot. Simple actions like looking up the current time, or searching a knowledge base before replying (a technique called &lt;a href=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/#agentic-rag&#34; &gt;Agentic RAG&lt;/a&gt;), may make a huge difference in terms of the quality of its responses.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools/structure-of-a-voice-bot-tools.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;llm-based-intent-detection&#34;&gt;
  LLM-based intent detection
  
&lt;/h2&gt;
&lt;p&gt;Despite the &lt;a href=&#34;posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/#logic-engine&#34; &gt;distinction we made earlier&lt;/a&gt; between tree-based, intent-based and LLM-based bots, often the logic of voice bots is implemented in a blend of more than one style. Intent-based bots may contain small decision trees, as well as LLM prompts. Often these approaches deliver the best results by taking the best of each to compensate for the weaknesss of the others.&lt;/p&gt;
&lt;p&gt;One of the most effective approaches is to use intent detection to help control the flow of an LLM conversation. Let&amp;rsquo;s see how.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools/structure-of-a-voice-bot-intent.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Suppose we&amp;rsquo;re building a general purpose customer support bot.&lt;/p&gt;
&lt;p&gt;A bot like this needs to be able to handle a huge variety of requests: helping the user renew subscriptions, buy or return items, update them on the state of a shipping, telling the opening hours of the certified repair shop closer to their home, explaining the advantages of a promotion, and more.&lt;/p&gt;
&lt;p&gt;If we decide to implement this chatbot based on intents, we risk that in many cases users won&amp;rsquo;t be able to find out how to achieve their goal, because many intent will look similar and there are many corner case requests that the original developers may not have foreseen.&lt;/p&gt;
&lt;p&gt;However, if we decide to implement this chatbot with an LLM, it becomes really hard to check its replies and make sure that the bot is not lying, because the amount of instructions its system prompt will end up containing is huge. The bot may also perform actions that it is not supposed to, like letting users return an item they have no warranty on anymore.&lt;/p&gt;
&lt;p&gt;There is an intermediate solution: &lt;strong&gt;first try to detect intent, then leverage the LLM&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;intent-detection&#34;&gt;
  Intent detection
  
&lt;/h3&gt;
&lt;p&gt;Step one is detecting the intention of the user. This step can be done with an LLM by sending it a message such as this:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Given the following conversation, select the intent
of the user:
1: Return an item
2: Help to use the product
3: Apply for a subscription
4: Get information about official repair centers
5: Find the nearest retail center near them
6: Learn about current promotion campaigns

Conversation:
assistant: Hello! How can I help you today?
user: Hello, can you tell me if there&amp;#39;s a repair shop for your product ABC open right now in Queens?
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can see that at this stage we don&amp;rsquo;t need to micromanage the model and we can stick to macro-categories safely. No need to specify &amp;ldquo;Find the opening hours of certified repair shops in New York&amp;rdquo;, bur rather &amp;ldquo;Find information on certified repair shops&amp;rdquo; in general will suffice.&lt;/p&gt;
&lt;p&gt;This first steps narrows down drastically the scope of the conversation and, as a consequence, the amount of instructions that the LLM needs to handle to carry on the conversation effectively.&lt;/p&gt;
&lt;p&gt;So the next step is to retrieve these instructions&lt;/p&gt;
&lt;h3 id=&#34;prompt-update&#34;&gt;
  Prompt update
  
&lt;/h3&gt;
&lt;p&gt;Once we know what the user&amp;rsquo;s intention is, it&amp;rsquo;s time to build the real prompt that will give us a reply for the user.&lt;/p&gt;
&lt;p&gt;With the general intent identified, we can equip the LLM strictly with the tools and information that it needs to proceed. If the user is asking about repair shops in their area, we can provide the LLM with a tool to search repair shops by zip code, a tool that would be useless if the user was asking about a shipment or a promotional campaign. Same for the background information: we don&amp;rsquo;t need to tell the LLM that &amp;ldquo;you&amp;rsquo;re a customer support bot&amp;rdquo;, but we can narrow down its personality and background knowledge to make it focus a lot more on the task at hand, which is to help the user locating a suitable repair shop. And so on.&lt;/p&gt;
&lt;p&gt;This can be done by mapping each expected intent to a specific system prompt, pre-compiled to match the intent. At the prompt building stage we simply pick from our library of prompts and &lt;strong&gt;replace the system prompt&lt;/strong&gt; with the one that we just selected.&lt;/p&gt;
&lt;p&gt;For example, in our case the LLM selected intent n.4, &amp;ldquo;Get information about official repair centers&amp;rdquo;. This intent may correspond to a prompt like the following:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;You’re a helpful assistant helping a user finding the best
repair center for them.

You can use the tool `find_repair_center` to get a list of
centers that match your query. Before calling the tool,
make sure to ask them for their zip code. If they asked about
a specific opening time, you can also use the `get_datetime`
tool to translate relative time (such as &amp;#34;now&amp;#34; or &amp;#34;tomorrow&amp;#34;)
into a specific date and time (like 2024-01-24 10:24:32)
Don&amp;#39;t forget about timezones. ...
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;reply-generation&#34;&gt;
  Reply generation
  
&lt;/h3&gt;
&lt;p&gt;With the new, narrower system prompt in place at the head of the conversation, we can finally prompt the LLM again to generate a reply for the user. The LLM, following the instructions of the updated prompt, has an easier time following its instructions (because they&amp;rsquo;re simpler and more focused) and generated better quality answers for both the users and the developers.&lt;/p&gt;
&lt;p&gt;With a prompt like the above, the reply from the LLM is most likely going to be about the zipcode, something that normally an LLM would not attempt to ask for.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;assistant: Hello! How can I help you today?
user: Hello, can you tell me if there&amp;#39;s a repair shop for your product ABC open right now in Queens?

assistant: Sure, let me look it up! Can you please tell me your zipcode?
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;what-about-latency&#34;&gt;
  What about latency?
  
&lt;/h2&gt;
&lt;p&gt;With all these additional back-and-forth with the LLM, it&amp;rsquo;s easy to find ourselves into a situation where latency gets out of hand. With only half a second of time to spare, making sure the system works as efficiently as possible is crucial.&lt;/p&gt;
&lt;p&gt;With today&amp;rsquo;s models there are a few technical and non-technical ways to manage the latency of your bots and keep it under control.&lt;/p&gt;
&lt;h3 id=&#34;model-colocation&#34;&gt;
  Model colocation
  
&lt;/h3&gt;
&lt;p&gt;Colocating models means that, instead of hosting each model on a different server or SaaS provider, you host all of them on the same machine or server rack, very close together.&lt;/p&gt;
&lt;p&gt;Colocation can be helpful to reduce or remove entirely the overhead of network requests, which often is the largest source of latency in your bots. Colocation is very powerful for bringing latency down, however it&amp;rsquo;s not always feasible if you&amp;rsquo;re using proprietary models that don&amp;rsquo;t allow self-hosting.&lt;/p&gt;
&lt;p&gt;Keep in mind also that colocation can backfire if your hardware is not suitable for the needs of the models you&amp;rsquo;re running. If you don&amp;rsquo;t have GPUs available, or they don&amp;rsquo;t fit all the models you need to colocate, your latency might increase dramatically.&lt;/p&gt;
&lt;h3 id=&#34;io-streaming&#34;&gt;
  I/O streaming
  
&lt;/h3&gt;
&lt;p&gt;Modern LLMs and STT/TTS models are able to stream either their input or their output. The time it takes these models to generate the start of their output is often much faster than the time they take to generate the entire reply, so streaming the output of one into the input of the next will bring down the latency of the whole system by orders of magnitude.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Endpointing&lt;/strong&gt;, for example, is the technical term for the ability of a speech to text model to detect the end of a sentence and send it over to an LLM while it listens for the rest of the user&amp;rsquo;s message. LLMs, while unable to take token-by-token inputs, can stream out their replies in this way. Text to speech then can detect dots and commas in the output stream to aggregate the tokens into sentences or phrases and start reading them out long before the last token is produced by the LLM.&lt;/p&gt;
&lt;p&gt;This is exactly what frameworks like Pipecat enable for all their models, and it&amp;rsquo;s usually possible for all moderns LLMs.&lt;/p&gt;
&lt;h3 id=&#34;declaring-the-latency&#34;&gt;
  Declaring the latency
  
&lt;/h3&gt;
&lt;p&gt;If all technical solutions fails, one unconventional approach is to make the bot declare its own latency at the very start of the conversation. While it might sound silly, if a bot opens the chat saying &lt;code&gt;I might be a bit slow, so be patient with me&lt;/code&gt; users are automatically more keen to wait longer for the bot&amp;rsquo;s response instead of pinging it continuously. While this does not make for the best user experience, being honest about your bot&amp;rsquo;s capabilities is always appreciated.&lt;/p&gt;
&lt;p&gt;This technique, however, is not a band-aid for any sort of delay. Users won&amp;rsquo;t manage to talk to a bot if each reply takes more than one or two seconds to come back to them, regardless of how patient they might be.&lt;/p&gt;
&lt;h3 id=&#34;buying-time&#34;&gt;
  Buying time
  
&lt;/h3&gt;
&lt;p&gt;Last but not least, occasionally the bot might have a spike in latency due to the usage of a slow tool. When your bot knows that its reply is going to take longer than usual, it&amp;rsquo;s best, again, to warn the user by telling them what&amp;rsquo;s going on. Having the bot say something like &lt;code&gt;Ok, let me look it up, it will take a few seconds&lt;/code&gt; is a huge user experience improvement you should not underestimate.&lt;/p&gt;
&lt;h1 id=&#34;the-code&#34;&gt;
  The code
  
&lt;/h1&gt;
&lt;p&gt;Now that we&amp;rsquo;ve seen all the techniques that can make your bot effective, reliable and fast, it&amp;rsquo;s time to actually implement one!&lt;/p&gt;
&lt;p&gt;One of the best frameworks out there to build open-source voice bots right now is &lt;a href=&#34;http://www.pipecat.ai&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pipecat&lt;/a&gt;, a small library maintained by &lt;a href=&#34;https://www.daily.co/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Daily.co&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1CUX7JRYMU1MEJBZ6lWMg5EThPew19Zjs?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Here&lt;/a&gt; you can find a commented Colab notebook to learn how &lt;a href=&#34;https://www.pipecat.ai&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pipecat&lt;/a&gt; can help you build a very basic voice bots, how to implement the intent-detection system we&amp;rsquo;ve outlined above, and try such a bot yourself. Watch out: you&amp;rsquo;ll need a few API keys, but if you don&amp;rsquo;t have a specific one, often the Pipecat documentation can help you find a replacement component for any alternative model provider you may have access to.&lt;/p&gt;
&lt;p&gt;Have fun!&lt;/p&gt;
&lt;div style=&#34;display: flex; align-content: center;&#34;&gt;
  &lt;video style=&#34;margin:auto;&#34; height=&#34;100%&#34; width=&#34;100%&#34; controls &gt;
      &lt;source src=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools/notebook-presentation-clip.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;/video&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;The Pipecat bot in action (from &lt;a href=&#34;https://www.zansara.dev/talks/2024-10-29-odsc-west-voice-agents/&#34; &gt;my talk&lt;/a&gt; at ODSC West 2024, presenting &lt;a href=&#34;https://colab.research.google.com/drive/1CUX7JRYMU1MEJBZ6lWMg5EThPew19Zjs?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this same notebook&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;
&lt;p class=&#34;fleuron&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-05-06-teranoptia/&#34;&gt;WŽH&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to Build a Reliable Voice Bot</title>
      <link>https://www.zansara.dev/demos/2024-10-29-building-reliable-voice-bots-with-open-source-tools/</link>
      <pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/demos/2024-10-29-building-reliable-voice-bots-with-open-source-tools/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ODSC West: Building Reliable Voice Agents with Open Source tools</title>
      <link>https://www.zansara.dev/talks/2024-10-29-odsc-west-voice-agents/</link>
      <pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/talks/2024-10-29-odsc-west-voice-agents/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://odsc.com/speakers/building-reliable-voice-agents-with-open-source-tools-2/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Announcement&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/1H_8PKQY_kFIzwxakZc-RJU8Rtwnjajhl/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/drive/1CUX7JRYMU1MEJBZ6lWMg5EThPew19Zjs?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;notebook&lt;/a&gt;.
All resources can also be found in
&lt;a href=&#34;https://drive.google.com/drive/folders/1baO1Gv55CIjLT-KPWtSm2z4IOBdXkHjn?usp=drive_link&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my archive&lt;/a&gt;.
Did you miss the talk? Check out the write-up&amp;rsquo;s
&lt;a href=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/&#34; &gt;part 1&lt;/a&gt;
and &lt;a href=&#34;https://www.zansara.dev/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/&#34; &gt;part 2&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;hahahugoshortcode72s0hbhb&#34;&gt;
  &lt;div class=&#39;iframe-wrapper&#39;&gt;
  &lt;iframe src=&#34;https://drive.google.com/file/d/1uQTJFmHTinvUvCMHRq9rrbaClNw9I9i2/preview&#34; width=100% height=100% allow=&#34;autoplay&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

  
&lt;/h2&gt;
&lt;p&gt;At &lt;a href=&#34;https://odsc.com/california/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ODSC West 2024&lt;/a&gt; I talked about building modern and reliable voice bots using Pipecat,
a recently released open source tool. I gave an overview of the general structure of voice bots, of the improvements
their underlying tech recently saw, from the first Whisper release to GPT 4o Realtime, and the new challenges that
developers face when implementing one of these systems.&lt;/p&gt;
&lt;p&gt;The main highlight of the talk is the &lt;a href=&#34;https://colab.research.google.com/drive/1CUX7JRYMU1MEJBZ6lWMg5EThPew19Zjs?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;notebook&lt;/a&gt;
where I implement first a simple Pipecat bot from scratch, and then I give an overview of how to blend intent detection
and system prompt switching to improve our control of how LLM bots interact with users.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SNAIL Opening Day: Should I use an LLM Framework? (Private Event)</title>
      <link>https://www.zansara.dev/talks/2024-10-01-snail-opening-day-should-i-use-an-llm-framework/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/talks/2024-10-01-snail-opening-day-should-i-use-an-llm-framework/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1GQJ1qEY2hXQ6EBF-rtqzJqZzidfS7HfI/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides&lt;/a&gt;,
&lt;a href=&#34;https://colab.research.google.com/drive/11aOq-43wEWhSlxtkdXEAwPEarC0IQ3eN?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;notebook&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/ZanSara/seven-wonders&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RAG dataset&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/datasets/ZanSara/seven-wonders-eval&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;evaluation dataset&lt;/a&gt;
All resources can also be found in
&lt;a href=&#34;https://drive.google.com/drive/folders/1anl3adpxgbwq5nsFn8QXuofIWXX0jRKo?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my archive&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;div class=&#39;iframe-wrapper&#39;&gt;
  &lt;iframe src=&#34;https://drive.google.com/file/d/1AORVusaHVBqNvJ5OtctyB5TWQZSadoqT/preview&#34; width=100% height=100% allow=&#34;autoplay&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;Find the transcript &lt;a href=&#34;https://drive.google.com/file/d/1wwnTFmGOANVmxUaVd1PC3cfztzIfSCEa/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;For the &lt;a href=&#34;https://group.springernature.com/gp/group&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Springer Nature&lt;/a&gt; AI Lab Opening Day I talk about LLM frameworks: what they are, when they can be useful, and how to choose and compare one framework to the other.&lt;/p&gt;
&lt;p&gt;After an overview of six application frameworks (&lt;a href=&#34;https://www.langchain.com/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LangChain&lt;/a&gt;, &lt;a href=&#34;https://www.llamaindex.ai/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LlamaIndex&lt;/a&gt;, &lt;a href=&#34;https://haystack.deepset.ai/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haystack&lt;/a&gt;, &lt;a href=&#34;https://neuml.github.io/txtai/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;txtai&lt;/a&gt;, &lt;a href=&#34;https://dspy-docs.vercel.app/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DSPy&lt;/a&gt; and &lt;a href=&#34;https://www.crewai.com/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CrewAI&lt;/a&gt;), we run a notebook where we used &lt;a href=&#34;https://docs.ragas.io/en/latest/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RAGAS&lt;/a&gt; to compare four small RAG applications and see which one performs better.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Who has the best RAG?</title>
      <link>https://www.zansara.dev/demos/2024-10-01-who-has-the-best-rag/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/demos/2024-10-01-who-has-the-best-rag/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Building Reliable Voice Bots with Open Source Tools - Part 1</title>
      <link>https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/</link>
      <pubDate>Fri, 20 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;
  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools/Building%20Reliable%20Voice%20Bots%20with%20Open%20Source%20Tools%20-%20Part%201%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/b01db173-8803-4fb1-a1da-702c0cf5e451?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;
  &lt;audio controls style=&#34;width:100%;&#34;&gt;
    &lt;source src=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools/Building%20Reliable%20Voice%20Bots%20with%20Open%20Source%20Tools%20-%20Part%201%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt; 
  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;
    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;
    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;This is part one of the write-up of my talk at &lt;a href=&#34;https://www.zansara.dev/talks/2024-09-05-building-voice-agents-with-open-source-tools/&#34; &gt;ODSC Europe 2024&lt;/a&gt; and &lt;a href=&#34;https://www.zansara.dev/talks/2024-10-29-odsc-west-voice-agents/&#34; &gt;ODSC West 2024&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;In the last few years, the world of voice agents saw dramatic leaps forward in the state of the art of all its most basic components. Thanks mostly to OpenAI, bots are now able to understand human speech almost like a human would, they&amp;rsquo;re able to speak back with completely naturally sounding voices, and are able to hold a free conversation that feels extremely natural.&lt;/p&gt;
&lt;p&gt;But building voice bots is far from a solved problem. These improved capabilities are raising the bar, and even users accustomed to the simpler capabilities of old bots now expect a whole new level of quality when it comes to interacting with them.&lt;/p&gt;
&lt;p&gt;In this post we&amp;rsquo;re going to focus mostly on &lt;strong&gt;the challenges&lt;/strong&gt;: we&amp;rsquo;ll discuss the basic structure of most voice bots today, their shortcomings and the main issues that you may face on your journey to improve the quality of the conversation.&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;https://www.zansara.dev/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/&#34; &gt;Part 2&lt;/a&gt; we are going to focus on &lt;strong&gt;the solutions&lt;/strong&gt; that are available today, and we are going to build our own voice bot using &lt;a href=&#34;https://www.pipecat.ai&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pipecat&lt;/a&gt;, a recently released open-source library that makes building these bots a lot simpler.&lt;/p&gt;
&lt;h1 id=&#34;outline&#34;&gt;
  Outline
  
&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-a-voice-agent&#34; &gt;What is a voice agent?&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#speech-to-text-stt&#34; &gt;Speech-to-text (STT)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#text-to-speech-tts&#34; &gt;Text-to-speech (TTS)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logic-engine&#34; &gt;Logic engine&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#tree-based&#34; &gt;Tree-based&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#intent-based&#34; &gt;Intent-based&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#llm-based&#34; &gt;LLM-based&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#audio-to-audio&#34; &gt;Audio-to-audio models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#new-challenges&#34; &gt;New challenges&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#real-speech-is-not-turn-based&#34; &gt;Real speech is not turn-based&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#real-conversation-flows-are-not-predictable&#34; &gt;Real conversation flows are not predictable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#llms-bring-their-own-problems&#34; &gt;LLMs bring their own problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-context-window&#34; &gt;The context window&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#working-in-real-time&#34; &gt;Working in real time&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Continues in &lt;a href=&#34;https://www.zansara.dev/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/&#34; &gt;Part 2&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;what-is-a-voice-agent&#34;&gt;
  What is a voice agent?
  
&lt;/h1&gt;
&lt;p&gt;As the name says, voice agents are programs that are able to carry on a task and/or take actions and decisions on behalf of a user (&amp;ldquo;software agents&amp;rdquo;) by using voice as their primary mean of communication (as opposed to the much more common text chat format). Voice agents are inherently harder to build than their text based counterparts: computers operate primarily with text, and the art of making machines understand human voices has been an elusive problem for decades.&lt;/p&gt;
&lt;p&gt;Today, the basic architecture of a modern voice agent can be decomposed into three main fundamental building blocks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a &lt;strong&gt;speech-to-text (STT)&lt;/strong&gt; component, tasked to translate an audio stream into readable text,&lt;/li&gt;
&lt;li&gt;the agent&amp;rsquo;s &lt;strong&gt;logic engine&lt;/strong&gt;, which works entirely with text only,&lt;/li&gt;
&lt;li&gt;a &lt;strong&gt;text-to-speech (TTS)&lt;/strong&gt; component, which converts the bot&amp;rsquo;s text responses back into an audio stream of synthetic speech.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools/structure-of-a-voice-bot.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see the details of each.&lt;/p&gt;
&lt;h2 id=&#34;speech-to-text-stt&#34;&gt;
  Speech to text (STT)
  
&lt;/h2&gt;
&lt;p&gt;Speech-to-text software is able to convert the audio stream of a person saying something and produce a transcription of what the person said. Speech-to-text engines have a &lt;a href=&#34;https://en.wikipedia.org/wiki/Speech_recognition#History&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;long history&lt;/a&gt;, but their limitations have always been quite severe: they used to require fine-tuning on each individual speaker, have a rather high word error rate (WER) and they mainly worked strictly with native speakers of major languages, failing hard on foreign and uncommon accents and native speakers of less mainstream languages. These issues limited the adoption of this technology for anything else than niche software and research applications.&lt;/p&gt;
&lt;p&gt;With the &lt;a href=&#34;https://openai.com/index/whisper/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;first release of OpenAI&amp;rsquo;s Whisper models&lt;/a&gt; in late 2022, the state of the art improved dramatically. Whisper enabled transcription (and even direct translation) of speech from many languages with an impressively low WER, finally comparable to the performance of a human, all with relatively low resources, higher then realtime speed, and no finetuning required. Not only, but the model was free to use, as OpenAI &lt;a href=&#34;https://huggingface.co/openai&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;open-sourced it&lt;/a&gt; together with a &lt;a href=&#34;https://github.com/openai/whisper&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Python SDK&lt;/a&gt;, and the details of its architecture were &lt;a href=&#34;https://cdn.openai.com/papers/whisper.pdf&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;published&lt;/a&gt;, allowing the scientific community to improve on it.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools/whisper-wer.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The WER (word error rate) of Whisper was extremely impressive at the time of its publication (see the full diagram &lt;a href=&#34;https://github.com/openai/whisper/assets/266841/f4619d66-1058-4005-8f67-a9d811b77c62&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Since then, speech-to-text models kept improving at a steady pace. Nowadays the Whisper&amp;rsquo;s family of models sees some competition for the title of best STT model from  companies such as &lt;a href=&#34;https://deepgram.com/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deepgram&lt;/a&gt;, but it&amp;rsquo;s still one of the best options in terms of open-source models.&lt;/p&gt;
&lt;h2 id=&#34;text-to-speech-tts&#34;&gt;
  Text-to-speech (TTS)
  
&lt;/h2&gt;
&lt;p&gt;Text-to-speech model perform the exact opposite task than speech-to-text models: their goal is to convert written text into an audio stream of synthetic speech. Text-to-speech has &lt;a href=&#34;https://en.wikipedia.org/wiki/Speech_synthesis#History&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;historically been an easier feat&lt;/a&gt; than speech-to-text, but it also recently saw drastic improvements in the quality of the synthetic voices, to the point that it could nearly be considered a solved problem in its most basic form.&lt;/p&gt;
&lt;p&gt;Today many companies (such as OpenAI, &lt;a href=&#34;https://cartesia.ai/sonic&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cartesia&lt;/a&gt;, &lt;a href=&#34;https://elevenlabs.io/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ElevenLabs&lt;/a&gt;, Azure and many others) offer TTS software with voices that sound nearly indistinguishable to a human. They also have the capability to clone a specific human voice with remarkably little training data (just a few seconds of speech) and to tune accents, inflections, tone and even emotion.&lt;/p&gt;


&lt;div&gt;
&lt;audio controls src=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools/sonic-tts-sample.wav&#34; style=&#34;width: 100%&#34;&gt;&lt;/audio&gt;
&lt;/div&gt;


&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://cartesia.ai/sonic&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cartesia&amp;rsquo;s Sonic&lt;/a&gt; TTS example of a gaming NPC. Note how the model subtly reproduces the breathing in between sentences.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;TTS is still improving in quality by the day, but due to the incredibly high quality of the output competition now tends to focus on price and performance.&lt;/p&gt;
&lt;h2 id=&#34;logic-engine&#34;&gt;
  Logic engine
  
&lt;/h2&gt;
&lt;p&gt;Advancements in the agent&amp;rsquo;s ability to talk to users goes hand in hand with the progress of natural language understanding (NLU), another field with a &lt;a href=&#34;https://en.wikipedia.org/wiki/Natural_language_understanding#History&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;long and complicated history&lt;/a&gt;. Until recently, the bot&amp;rsquo;s ability to understand the user&amp;rsquo;s request has been severely limited and often available only for major languages.&lt;/p&gt;
&lt;p&gt;Based on the way their logic is implemented, today you may come across bots that rely on three different categories.&lt;/p&gt;
&lt;h3 id=&#34;tree-based&#34;&gt;
  Tree-based
  
&lt;/h3&gt;
&lt;p&gt;Tree-based (or rule-based) logic is one of the earliest method of implementing chatbot&amp;rsquo;s logic, still very popular today for its simplicity. Tree-based bots don&amp;rsquo;t really try to understand what the user is saying, but listen to the user looking for a keyword or key sentence that will trigger the next step. For example, a customer support chatbot may look for the keyword &amp;ldquo;refund&amp;rdquo; to give the user any information about how to perform a refund, or the name of a discount campaign to explain the user how to take advantage of that.&lt;/p&gt;
&lt;p&gt;Tree-based logic, while somewhat functional, doesn&amp;rsquo;t really resemble a conversation and can become very frustrating to the user when the conversation tree was not designed with care, because it&amp;rsquo;s difficult for the end user to understand which option or keyword they should use to achieve the desired outcome. It is also unsuitable to handle real questions and requests like a human would.&lt;/p&gt;
&lt;p&gt;One of its most effective usecases is as a first-line screening to triage incoming messages.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools/tree-based-logic.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example of a very simple decision tree for a chatbot. While rather minimal, this bot already has several flaws: there&amp;rsquo;s no way to correct the information you entered at a previous step, and it has no ability to recognize synonyms (&amp;ldquo;I want to buy an item&amp;rdquo; would trigger the fallback route.)&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;intent-based&#34;&gt;
  Intent-based
  
&lt;/h3&gt;
&lt;p&gt;In intent-based bots, &lt;strong&gt;intents&lt;/strong&gt; are defined roughly as &amp;ldquo;actions the users may want to do&amp;rdquo;. With respect to a strict, keyword-based tree structure, intent-based bots may switch from an intent to another much more easily (because they lack a strict tree-based routing) and may use advanced AI techniques to understand what the user is actually trying to accomplish and perform the required action.&lt;/p&gt;
&lt;p&gt;Advanced voice assistants such as Siri and Alexa use variations of this intent-based system. However, as their owners are usually familiar with, interacting with an intent-based bot doesn&amp;rsquo;t always feel natural, especially when the available intents don&amp;rsquo;t match the user&amp;rsquo;s expectation and the bot ends up triggering an unexpected action. In the long run, this ends with users carefully second-guessing what words and sentence structures activate the response they need and eventually leads to a sort of &amp;ldquo;magical incantation&amp;rdquo; style of prompting the agent, where the user has to learn what is the &amp;ldquo;magical sentence&amp;rdquo; that the bot will recognize to perform a specific intent without misunderstandings.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools/amazon-echo.webp&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Modern voice assistants like Alexa and Siri are often built on the concept of intent (image from Amazon).&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;llm-based&#34;&gt;
  LLM-based
  
&lt;/h3&gt;
&lt;p&gt;The introduction of instruction-tuned GPT models like ChatGPT revolutionized the field of natural language understanding and, with it, the way bots can be built today. LLMs are naturally good at conversation and can formulate natural replies to any sort of question, making the conversation feel much more natural than with any technique that was ever available earlier.&lt;/p&gt;
&lt;p&gt;However, LLMs tend to be harder to control. Their very ability of generating naturally sounding responses for anything makes them behave in ways that are often unexpected to the developer of the chatbot: for example, users can get the LLM-based bot to promise them anything they ask for, or they can be convinced to say something incorrect or even occasionally lie.&lt;/p&gt;
&lt;p&gt;The problem of controlling the conversation, one that traditionally was always on the user&amp;rsquo;s side, is now entirely on the shoulders of the developers and can easily backfire.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools/chatgpt-takesies-backsies.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In a rather &lt;a href=&#34;https://x.com/ChrisJBakke/status/1736533308849443121&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;famous instance&lt;/a&gt;, a user managed to convince a Chevrolet dealership chatbot to promise selling him a Chevy Tahoe for a single dollar.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;audio-to-audio-models&#34;&gt;
  Audio-to-audio models
  
&lt;/h2&gt;
&lt;p&gt;On top of all these changes, OpenAI recently made a step further. They latest flagship model, &lt;a href=&#34;https://openai.com/index/hello-gpt-4o/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPT 4o&lt;/a&gt;, was allegedly able to understand audio natively, taking away the need for a dedicated speech-to-text model, and to produce audio responses directly, making text-to-speech engines also redundant.&lt;/p&gt;
&lt;p&gt;For a long time these capabilities were heavily restricted to a limited number of partners, but as of the 1st of October 2024, they eventually made such capabilities generally available through their new &lt;a href=&#34;https://openai.com/index/introducing-the-realtime-api/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Realtime API&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;At a first glance, the release of such model seemed to shake the foundations of how we build voice bots today. However, at the time of writing, there are still a number of hurdles that prevents immediate adoption, the main one being &lt;strong&gt;price&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools/realtime-api-pricing.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://openai.com/api/pricing/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pricing of the Realtime API&lt;/a&gt; at the time of writing (October 2024)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The problem here is that voice bots built with the traditional stack can be more than sufficient to cover the most common usecases for a fraction of the price of GPT 4o, and while the latter can indeed handle cases that are impossible to address for a traditional voice bot, in most practical situations such capabilities are not necessary to achieve a smooth and effective interaction.&lt;/p&gt;
&lt;p&gt;However, GPT 4o is surely a step further on the evolutionary path of modern voice bots. With potential future price changes, a model like this could easily become a valid competitor to the architecture we&amp;rsquo;re going to explore in the rest of the post, with its own pros and cons.&lt;/p&gt;
&lt;h1 id=&#34;new-challenges&#34;&gt;
  New challenges
  
&lt;/h1&gt;
&lt;p&gt;Thanks to all these recent improvements, it would seem that making natural-sounding, smart bots is getting easier and easier. It is indeed much simpler to make a simple bot sound better, understand more and respond appropriately, but there&amp;rsquo;s still a long way to go before users can interact with these new bots as they would with a human.&lt;/p&gt;
&lt;p&gt;The issue lays in the fact that &lt;strong&gt;users expectations grow&lt;/strong&gt; with the quality of the bot. It&amp;rsquo;s not enough for the bot to have a voice that sounds human: users want to be able to interact with it in a way that it feels human too, which is far more rich and interactive than what the rigid tech of earlier chatbots allowed so far.&lt;/p&gt;
&lt;p&gt;What does this mean in practice? What are the expectations that users might have from our bots?&lt;/p&gt;
&lt;h2 id=&#34;real-speech-is-not-turn-based&#34;&gt;
  Real speech is not turn-based
  
&lt;/h2&gt;
&lt;p&gt;Traditional bots can only handle turn-based conversations: the user talks, then the bot talks as well, then the user talks some more, and so on. A conversation with another human, however, has no such limitation: people may talk over each other, give audible feedback without interrupting, and more.&lt;/p&gt;
&lt;p&gt;Here are some examples of this richer interaction style:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interruptions&lt;/strong&gt;. Interruptions occur when a person is talking and another one starts talking at the same time. It is expected that the first person stops talking, at least for a few seconds, to understand what the interruption was about, while the second person continue to talk.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Back-channeling&lt;/strong&gt;. Back-channeling is the practice of saying &amp;ldquo;ok&amp;rdquo;, &amp;ldquo;sure&amp;rdquo;, &amp;ldquo;right&amp;rdquo; while the other person is explaining something, to give them feedback and letting them know we&amp;rsquo;re paying attention to what is being said. The person that is talking is not supposed to stop: the aim of this sort of feedback is to let them know they are being heard.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Pinging&lt;/strong&gt;. This is the natural reaction a long silence, especially over a voice-only medium such as a phone call. When one of the two parties is supposed to speak but instead stays silent, the last one that talked might &amp;ldquo;ping&amp;rdquo; the silent party by asking &amp;ldquo;Are you there?&amp;rdquo;, &amp;ldquo;Did you hear?&amp;rdquo;, or even just &amp;ldquo;Hello?&amp;rdquo; to test whether they&amp;rsquo;re being heard. This behavior is especially difficult to handle for voice agents that have a significant delay, because it may trigger an ugly vicious cycle of repetitions and delayed replies.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Buying time&lt;/strong&gt;. When one of the parties know that it will stay silent for a while, a natural reaction is to notify the other party in advance by saying something like &amp;ldquo;Hold on&amp;hellip;&amp;rdquo;, &amp;ldquo;Wait a second&amp;hellip;&amp;rdquo;, &amp;ldquo;Let me check&amp;hellip;&amp;rdquo; and so on. This message has the benefit of preventing the &amp;ldquo;pinging&amp;rdquo; behavior we&amp;rsquo;ve seen before and can be very useful for voice bots that may need to carry on background work during the conversation, such as looking up information.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Audible clues&lt;/strong&gt;. Not everything can be transcribed by a speech-to-text model, but audio carries a lot of nuance that is often used by humans to communicate. A simple example is pitch: humans can often tell if they&amp;rsquo;re talking to a child, a woman or a man by the pitch of their voice, but STT engines don&amp;rsquo;t transcribe that information. So if a child picks up the phone, the model won&amp;rsquo;t pick up the obvious audible clue and will likely assume it is talking to an adult. Similar considerations should be made for tone (to detect mood, sarcasm, etc) or other sounds like laughter, sobs, and more. &lt;strong&gt;Audio-to-audio models&lt;/strong&gt; such as GPT 4o don&amp;rsquo;t have this intrinsic limitation, but while they surely can pick up these clues, their ability to use them effectively should not be taken for granted.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;real-conversation-flows-are-not-predictable&#34;&gt;
  Real conversation flows are not predictable
  
&lt;/h2&gt;
&lt;p&gt;Tree-based bots, and to some degree intent-based too, work on the implicit assumption that conversation flows are largely predictable. Once the user said something and the bot replied accordingly, they can only follow up with a fixed set of replies and nothing else.&lt;/p&gt;
&lt;p&gt;This is often a flawed assumption and the primary reason why talking to chatbots tends to be so frustrating.&lt;/p&gt;
&lt;p&gt;In reality, natural conversations are largely unpredictable. For example, they may feature:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sudden changes of topic&lt;/strong&gt;. Maybe user and bot were talking about making a refund, but then the user changes their mind and decides to ask for assistance finding a repair center for the product. Well designed intent-based bots can deal with that, but most bots are in practice unable to do so in a way that feels natural to the user.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unexpected, erratic phrasing&lt;/strong&gt;. This is common when users are nervous or in a bad mood for any reason. Erratic, convoluted phrasing, long sentences, rambling, are all very natural ways of expressing themselves, but such outbursts very often confuse bots completely.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Non native speakers&lt;/strong&gt;. Due to the nature la language learning, non native speakers may have trouble pronouncing words correctly, they may use highly unusual synonyms, or structure sentences in complicated ways. This is also difficult for bots to handle, because understanding the sentence is harder and transcription issues are far more likely.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Non sequitur&lt;/strong&gt;&lt;/em&gt;. &lt;em&gt;Non sequitur&lt;/em&gt; is an umbrella term for a sequence of sentences that bear no relation to each other in a conversation. A simple example is the user asking the bot &amp;ldquo;What&amp;rsquo;s the capital of France&amp;rdquo; and the bot replies &amp;ldquo;It&amp;rsquo;s raining now&amp;rdquo;. When done by the bot, this is often due to a severe transcription issue or a very flawed conversation design. When done by the user, it&amp;rsquo;s often a malicious intent to break the bot&amp;rsquo;s logic, so it should be handled with some care.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;llms-bring-their-own-problems&#34;&gt;
  LLMs bring their own problems
  
&lt;/h2&gt;
&lt;p&gt;It may seem that some of these issues, especially the ones related to conversation flow, could be easily solved with an LLM. These models, however, bring their own set of issues too:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hallucinations&lt;/strong&gt;. This is a technical term to say that LLMs can occasionally mis-remember information, or straight up lie. The problem is that they&amp;rsquo;re also very confident about their statements, sometimes to the point of trying to gaslight their users. Hallucinations are a major problem for all LLMs: although it may seem to get more manageable with larger and smarter models, the problem only gets more subtle and harder to spot.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Misunderstandings&lt;/strong&gt;. While LLMs are great at understanding what the user is trying to say, they&amp;rsquo;re not immune to misunderstandings. Unlike a human though, LLMs rarely suspect a misunderstanding and they rather make assumptions that ask for clarifications, resulting in surprising replies and behavior that are reminiscent of intent-based bots.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Lack of assertiveness&lt;/strong&gt;. LLMs are trained to listen to the user and do their best to be helpful. This means that LLMs are also not very good at taking the lead of the conversation when we would need them to, and are easily misled and distracted by a motivated user. Preventing your model to give your user&amp;rsquo;s a literary analysis of their unpublished poetry may sound silly, but it&amp;rsquo;s a lot harder than many suspect.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prompt hacking&lt;/strong&gt;. Often done with malicious intent by experienced users, prompt hacking is the practice of convincing an LLM to reveal its initial instructions, ignore them and perform actions they are explicitly forbidden from. This is especially dangerous and, while a lot of work has gone into this field, this is far from a solved problem.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-context-window&#34;&gt;
  The context window
  
&lt;/h2&gt;
&lt;p&gt;LLMs need to keep track of the whole conversation, or at least most of it, to be effective. However, they often have a limitation to the amount of text they can keep in mind at any given time: this limit is called &lt;strong&gt;context window&lt;/strong&gt; and for many models is still relatively low, at about 2000 tokens &lt;strong&gt;(between 1500-1800 words)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The problem is that this window also need to include all the instructions your bot needs for the conversation. This initial set of instructions is called &lt;strong&gt;system prompt&lt;/strong&gt;, and is slightly distinct from the other messages in the conversation to make the LLM understand that it&amp;rsquo;s not part of it, but it&amp;rsquo;s a set of instructions about how to handle the conversation.&lt;/p&gt;
&lt;p&gt;For example, a system prompt for a customer support bot may look like this:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;You&amp;#39;re a friendly customer support bot named VirtualAssistant. 
You are always kind to the customer and you must do your best 
to make them feel at ease and helped.

You may receive a set of different requests. If the users asks
you to do anything that is not in the list below, kindly refuse
to do so.

# Handle refunds

If the user asks you to handle a refund, perform these actions:
- Ask for their shipping code
- Ask for their last name
- Use the tool `get_shipping_info` to verify the shipping exists
...

# Handle subscriptions

If the user asks you to subscribe to a service, perform these actions:
- Ask what subscription are they interested in
- Ask if they have a promo code
- Ask for their username
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and so on.&lt;/p&gt;
&lt;p&gt;Although very effective, system prompts have a tendency to become huge in terms of tokens. Adding information to it makes the LLM behave much more like you expect (although it&amp;rsquo;s not infallible), hallucinate less, and can even shape its personality to some degree. But if the system prompt becomes too long (more than 1000 words), this means that the bot will only be able to exchange about 800 words worth of messages with the user before it starts to &lt;strong&gt;forget&lt;/strong&gt; either its instructions or the first messages of the conversation. For example, the bot will easily forget its own name and role, or it will forget the user&amp;rsquo;s name and initial demands, which can make the conversation drift completely.&lt;/p&gt;
&lt;h2 id=&#34;working-in-real-time&#34;&gt;
  Working in real time
  
&lt;/h2&gt;
&lt;p&gt;If all these issues weren&amp;rsquo;t enough, there&amp;rsquo;s also a fundamental issue related to voice interaction: &lt;strong&gt;latency&lt;/strong&gt;. Voice bots interact with their users in real time: this means that the whole pipeline of transcription, understanding, formulating a reply and synthesizing it back but be very fast.&lt;/p&gt;
&lt;p&gt;How fast? On average, people expect a reply from another person to arrive within &lt;strong&gt;300-500ms&lt;/strong&gt; to sound natural. They can normally wait for about 1-2 seconds. Any longer and they&amp;rsquo;ll likely ping the bot, breaking the flow.&lt;/p&gt;
&lt;p&gt;This means that, even if we had some solutions to all of the above problems (and we do have some), these solutions needs to operate at blazing fast speed. Considering that LLM inference alone can take the better part of a second to even start being generated, latency is often one of the major issues that voice bots face when deployed at scale.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools/ttft.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Time to First Token (TTFT) stats for several LLM inference providers running Llama 2 70B chat. From &lt;a href=&#34;https://github.com/ray-project/llmperf-leaderboard&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LLMPerf leaderboard&lt;/a&gt;. You can see how the time it takes for a reply to even start being produced is highly variable, going up to more than one second in some scenarios.&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;to-be-continued&#34;&gt;
  To be continued&amp;hellip;
  
&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Interested? Check out &lt;a href=&#34;https://www.zansara.dev/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2&#34; &gt;Part 2&lt;/a&gt;!&lt;/em&gt;&lt;/p&gt;
&lt;p class=&#34;fleuron&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-05-06-teranoptia/&#34;&gt;F]&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AMTA 2024 Virtual Tutorial Day: Controlling LLM Translations of Invariant Elements with RAG</title>
      <link>https://www.zansara.dev/talks/2024-09-18-amta-2024-controlling-invariants-rag/</link>
      <pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/talks/2024-09-18-amta-2024-controlling-invariants-rag/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://amtaweb.org/virtual-tutorial-day-program/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Announcement&lt;/a&gt;,
&lt;a href=&#34;https://colab.research.google.com/drive/1VMgK3DcVny_zTtAG_V3QSSdfSFBWAgmb?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;notebook&lt;/a&gt; and
&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1A1zk-u-RTSqBfE8LksZxihnp7KxWO7YK/edit?usp=sharing&amp;amp;ouid=102297935451395786183&amp;amp;rtpof=true&amp;amp;sd=true&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;glossary&lt;/a&gt;.
All resources can also be found in
&lt;a href=&#34;https://drive.google.com/drive/folders/1Tdq92P_E_77sErGjz7jSPfJ-or9UZXvn?usp=drive_link&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my archive&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;div class=&#39;iframe-wrapper&#39;&gt;
  &lt;iframe src=&#34;https://drive.google.com/file/d/1BvcNbsAGWp25EDpiQ5ljYos3_eneo3wu/preview&#34; width=100% height=100% allow=&#34;autoplay&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Note: this was a tutorial session co-presented with &lt;a href=&#34;https://www.linkedin.com/in/christian-lang-8942b0145/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Christian Lang&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/brunobitter/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bruno Bitter&lt;/a&gt;. My section starts at 01:08.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;At the &lt;a href=&#34;https://amtaweb.org/virtual-tutorial-day-program/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AMTA 2024 Virtual Tutorial Day&lt;/a&gt; I talked about controlling invariant translation elements with RAG. During the talk several speakers intervened on the topic, each bringing a different perspective of it.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/georgkirchner/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Georg Kirchner&lt;/a&gt; introduced the concept of invariant translation elements, such as brand names, UI elements, and corporate slogans. &lt;a href=&#34;https://www.linkedin.com/in/christian-lang-8942b0145/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Christian Lang&lt;/a&gt; gave a comprehensive overview of the challenges of handling invariant translation elements with existing tools and how LLMs can help at various stages of the translation, covering several approaches, including RAG. Building on his overview, I showed how to implement a simple RAG system to handle these invariants properly using &lt;a href=&#34;https://haystack.deepset.ai/?utm_campaign=amta-2024&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haystack&lt;/a&gt;: we run a &lt;a href=&#34;https://colab.research.google.com/drive/1VMgK3DcVny_zTtAG_V3QSSdfSFBWAgmb?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colab notebook&lt;/a&gt; live and checked how the translation changes by introducing context about the invariants to the LLM making the translation. Last, &lt;a href=&#34;https://www.linkedin.com/in/brunobitter/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bruno Bitter&lt;/a&gt; gave an overview of how you can use &lt;a href=&#34;https://www.blackbird.io/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blackbird&lt;/a&gt; to integrate a system like this with existing CAT tools and manage the whole lifecycle of content translation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Controlling LLM Translations of Invariant Elements with RAG</title>
      <link>https://www.zansara.dev/demos/2024-09-18-controlling-llm-translation-of-invariant-elements-with-rag/</link>
      <pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/demos/2024-09-18-controlling-llm-translation-of-invariant-elements-with-rag/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ODSC Europe: Building Reliable Voice Agents with Open Source tools</title>
      <link>https://www.zansara.dev/talks/2024-09-05-odsc-europe-voice-agents/</link>
      <pubDate>Fri, 06 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/talks/2024-09-05-odsc-europe-voice-agents/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://odsc.com/speakers/building-reliable-voice-agents-with-open-source-tools-2/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Announcement&lt;/a&gt;,
&lt;a href=&#34;https://drive.google.com/file/d/1ubk7Q_l9C7epQgYrMttHMjW1AVfdm-LT/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt; and
&lt;a href=&#34;https://colab.research.google.com/drive/1NCAAs8RB2FuqMChFKMIVWV0RiJr9O3IJ?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;notebook&lt;/a&gt;.
All resources can also be found on ODSC&amp;rsquo;s website and in
&lt;a href=&#34;https://drive.google.com/drive/folders/1rrXMTbfTZVuq9pMzneC8j-5GKdRQ6l2i?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my archive&lt;/a&gt;.
Did you miss the talk? Check out the write-up&amp;rsquo;s
&lt;a href=&#34;https://www.zansara.dev/posts/2024-09-05-building-voice-agents-with-open-source-tools-part-1/&#34; &gt;part 1&lt;/a&gt;
and &lt;a href=&#34;https://www.zansara.dev/posts/2024-10-30-building-voice-agents-with-open-source-tools-part-2/&#34; &gt;part 2&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;(Note: this is a recording of the notebook walkthrough only. The full recording will be shared soon).&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#39;iframe-wrapper&#39;&gt;
  &lt;iframe src=&#34;https://drive.google.com/file/d/15Kv8THmDsnnzfVBhHAf2O11RccpzAzYK/preview&#34; width=100% height=100% allow=&#34;autoplay&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;At &lt;a href=&#34;https://odsc.com/europe/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ODSC Europe 2024&lt;/a&gt; I talked about building modern and reliable voice bots using Pipecat,
a recently released open source tool. I gave an overview of the general structure of voice bots, of the improvements
their underlying tech recently saw, and the new challenges that developers face when implementing one of these systems.&lt;/p&gt;
&lt;p&gt;The main highlight of the talk is the &lt;a href=&#34;https://colab.research.google.com/drive/1NCAAs8RB2FuqMChFKMIVWV0RiJr9O3IJ?usp=drive_link&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;notebook&lt;/a&gt;
where I implement first a simple Pipecat bot from scratch, and then I give an overview of how to blend intent detection
and system prompt switching to improve our control of how LLM bots interact with users.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>EuroPython: Is RAG all you need? A look at the limits of retrieval augmented generation</title>
      <link>https://www.zansara.dev/talks/2024-07-10-europython-rag/</link>
      <pubDate>Wed, 10 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/talks/2024-07-10-europython-rag/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://ep2024.europython.eu/session/is-rag-all-you-need-a-look-at-the-limits-of-retrieval-augmented-generation&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Announcement&lt;/a&gt;,
&lt;a href=&#34;https://drive.google.com/file/d/13OXMLaBQr1I_za7sqVHJWxRj5xFAg7KV/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;.
Did you miss the talk? Check out the recording on &lt;a href=&#34;https://youtu.be/9wk7mGB_Gp4?feature=shared&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Youtube&lt;/a&gt;
or on my &lt;a href=&#34;https://drive.google.com/file/d/1OkYQ7WMt63QkdJTU3GIpSxBZmnLfZti6/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;backup&lt;/a&gt; (cut from the &lt;a href=&#34;https://www.youtube.com/watch?v=tcXmnCJIvFc&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;original stream&lt;/a&gt;),
or read the &lt;a href=&#34;https://www.zansara.dev/posts/2024-04-29-odsc-east-rag&#34; &gt;write-up&lt;/a&gt; of a previous edition of the same talk.&lt;/p&gt;
&lt;hr&gt;
&lt;div class=&#39;iframe-wrapper&#39;&gt;
  &lt;iframe src=&#34;https://drive.google.com/file/d/1OkYQ7WMt63QkdJTU3GIpSxBZmnLfZti6/preview&#34; width=100% height=100% allow=&#34;autoplay&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;At &lt;a href=&#34;https://ep2024.europython.eu/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EuroPython 2024&lt;/a&gt; I talked about RAG: how it works, how it fails, and how to evaluate its performance objectively. I gave an overview of some useful open-source tools for RAG evalution such as &lt;a href=&#34;https://docs.relari.ai/v0.3?utm_campaign=europython-2024&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;continuous-eval&lt;/a&gt; and how to use them with &lt;a href=&#34;https://haystack.deepset.ai/?utm_campaign=europython-2024&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haystack&lt;/a&gt;, and then offered some ideas on how to expand your RAG architecture further than a simple two-step process.&lt;/p&gt;
&lt;p&gt;Some resources mentioned in the talk:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://haystack.deepset.ai/?utm_campaign=europython-2024&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haystack&lt;/a&gt;: open-source LLM framework for RAG and beyond.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.relari.ai/v0.3?utm_campaign=europython-2024&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;continuous-eval&lt;/a&gt; by &lt;a href=&#34;https://www.relari.ai/?utm_campaign=europython-2024&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Relari AI&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Build and evaluate RAG with Haystack: &lt;a href=&#34;https://haystack.deepset.ai/tutorials/35_model_based_evaluation_of_rag_pipelines/?utm_campaign=europython-2024&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://haystack.deepset.ai/tutorials/35_model_based_evaluation_of_rag_pipelines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Use continuous-eval with Haystack: &lt;a href=&#34;https://github.com/relari-ai/examples/blob/main/examples/haystack/simple_rag/app.py&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/relari-ai/examples/blob/main/examples/haystack/simple_rag/app.py&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Perplexity.ai: &lt;a href=&#34;https://www.perplexity.ai/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.perplexity.ai/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The Agent Compass</title>
      <link>https://www.zansara.dev/posts/2024-06-10-the-agent-compass/</link>
      <pubDate>Mon, 10 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2024-06-10-the-agent-compass/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;
  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/The%20Agent%20Compass%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/317a5326-a8c5-418b-8ea1-5c7e664b8ea3?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;
  &lt;audio controls style=&#34;width:100%;&#34;&gt;
    &lt;source src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/The%20Agent%20Compass%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt; 
  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;
    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;
    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The concept of Agent is one of the vaguest out there in the post-ChatGPT landscape. The word has been used to identify systems that seem to have nothing in common with one another, from complex autonomous research systems down to a simple sequence of two predefined LLM calls. Even the distinction between Agents and techniques such as RAG and prompt engineering seems blurry at best.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try to shed some light on the topic by understanding just how much the term &amp;ldquo;AI Agent&amp;rdquo; covers and set some landmarks to better navigate the space.&lt;/p&gt;
&lt;h2 id=&#34;defining-agent&#34;&gt;
  Defining &amp;ldquo;Agent&amp;rdquo;
  
&lt;/h2&gt;
&lt;p&gt;The problem starts with the definition of &amp;ldquo;agent&amp;rdquo;. For example, &lt;a href=&#34;https://en.wikipedia.org/wiki/Software_agent&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia&lt;/a&gt; reports that a software agent is&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;a computer program that acts for a user or another program in a relationship of agency.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This definition is extremely high-level, to the point that it could be applied to systems ranging from ChatGPT to a thermostat. However, if we restrain our definition to &amp;ldquo;LLM-powered agents&amp;rdquo;, then it starts to mean something: an Agent is an LLM-powered application that is given some &lt;strong&gt;agency&lt;/strong&gt;, which means that it can take actions to accomplish the goals set by its user. Here we see the difference between an agent and a simple chatbot, because a chatbot can only talk to a user. but don&amp;rsquo;t have the agency to take any action on their behalf. Instead, an Agent is a system you can effectively delegate tasks to.&lt;/p&gt;
&lt;p&gt;In short, an LLM powered application can be called an Agent when&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;it can take decisions and choose to perform actions in order to achieve the goals set by the user.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;autonomous-vs-conversational&#34;&gt;
  Autonomous vs Conversational
  
&lt;/h2&gt;
&lt;p&gt;On top of this definition there&amp;rsquo;s an additional distinction to take into account, normally brought up by the terms &lt;strong&gt;autonomous&lt;/strong&gt; and &lt;strong&gt;conversational&lt;/strong&gt; agents.&lt;/p&gt;
&lt;p&gt;Autonomous Agents are applications that &lt;strong&gt;don&amp;rsquo;t use conversation as a tool&lt;/strong&gt; to accomplish their goal. They can use several tools several times, but they won&amp;rsquo;t produce an answer for the user until their goal is accomplished in full. These agents normally interact with a single user, the one that set their goal, and the whole result of their operations might be a simple notification that the task is done. The fact that they can understand language is rather a feature that lets them receive the user&amp;rsquo;s task in natural language, understand it, and then to navigate the material they need to use (emails, webpages, etc).&lt;/p&gt;
&lt;p&gt;An example of an autonomous agent is a &lt;strong&gt;virtual personal assistant&lt;/strong&gt;: an app that can read through your emails and, for example, pays the bills for you when they&amp;rsquo;re due. This is a system that the user sets up with a few credentials and then works autonomously, without the user&amp;rsquo;s supervision, on the user&amp;rsquo;s own behalf, possibly without bothering them at all.&lt;/p&gt;
&lt;p&gt;On the contrary, Conversational Agents &lt;strong&gt;use conversation as a tool&lt;/strong&gt;, often their primary one. This doesn&amp;rsquo;t have to be a conversation with the person that set them off: it&amp;rsquo;s usually a conversation with another party, that may or may not be aware that they&amp;rsquo;re talking to an autonomous system. Naturally, they behave like agents only from the perspective of the user that assigned them the task, while in many cases they have very limited or no agency from the perspective of the users that holds the conversation with them.&lt;/p&gt;
&lt;p&gt;An example of a conversational agent is a &lt;strong&gt;virtual salesman&lt;/strong&gt;: an app that takes a list of potential clients and calls them one by one, trying to persuade them to buy. From the perspective of the clients receiving the call this bot is not an agent: it can perform no actions on their behalf, in fact it may not be able to perform actions at all other than talking to them. But from the perspective of the salesman the bots are agents, because they&amp;rsquo;re calling people for them, saving a lot of their time.&lt;/p&gt;
&lt;p&gt;The distinction between these two categories is very blurry, and &lt;strong&gt;some systems may behave like both&lt;/strong&gt; depending on the circumnstances. For example, an autonomous agent might become a conversational one if it&amp;rsquo;s configured to reschedule appointments for you by calling people, or to reply to your emails to automatically challenge parking fines, and so on. Alternatively, an LLM that asks you if it&amp;rsquo;s appropriate to use a tool before using it is behaving a bit like a conversational agent, because it&amp;rsquo;s using the chat to improve its odds of providing you a better result.&lt;/p&gt;
&lt;h2 id=&#34;degrees-of-agency&#34;&gt;
  Degrees of agency
  
&lt;/h2&gt;
&lt;p&gt;All the distinctions we made above are best understood as a continuous spectrum rather than hard categories. Various AI systems may have more or less agency and may be tuned towards a more &amp;ldquo;autonomous&amp;rdquo; or &amp;ldquo;conversational&amp;rdquo; behavior.&lt;/p&gt;
&lt;p&gt;In order to understand this difference in practice, let&amp;rsquo;s try to categorize some well-known LLM techniques and apps to see how &amp;ldquo;agentic&amp;rdquo; they are. Having two axis to measure by, we can build a simple compass like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/empty-compass.png&#34; alt=&#34;a compass with two axis: no agency (left) to full agency (right) on the horizontal axis, and autonomous (bottom) to conversational (top) on the vertical axis.&#34;&gt;&lt;/p&gt;
&lt;div style=&#34;text-align:center;&#34;&gt;&lt;i&gt;Our Agent compass&lt;/i&gt;&lt;/div&gt;
&lt;h3 id=&#34;bare-llms&#34;&gt;
  Bare LLMs
  
&lt;/h3&gt;
&lt;p&gt;Many apps out there perform nothing more than direct calls to LLMs, such as ChatGPT&amp;rsquo;s free app and other similarly simple assistants and chatbots. There are no more components to this system other than the model itself and their mode of operation is very straightforward: a user asks a question to an LLM, and the LLM replies directly.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/direct-llm-call.png&#34; alt=&#34;Diagram of the operation of a direct LLM call: a user asks a question to an LLM and the LLM replies directly.&#34;&gt;&lt;/p&gt;
&lt;p&gt;This systems are not designed with the intent of accomplishing a goal, and neither can take any actions on the user&amp;rsquo;s behalf. They focus on talking with a user in a reactive way and can do nothing else than talk back. An LLM on its own has &lt;strong&gt;no agency at all&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;At this level it also makes very little sense to distinguish between autonomous or conversational agent behavior, because the entire app shows no degrees of autonomy. So we can place them at the very center-left of the diagram.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/direct-llm-call-compass.png&#34; alt=&#34;the updated compass&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;basic-rag&#34;&gt;
  Basic RAG
  
&lt;/h3&gt;
&lt;p&gt;Together with direct LLM calls and simple chatbots, basic RAG is also an example of an application that does not need any agency or goals to pursue in order to function. Simple RAG apps works in two stages: first the user question is sent to a retriever system, which fetches some additional data relevant to the question. Then, the question and the additional data is sent to the LLM to formulate an answer.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/basic-rag.png&#34; alt=&#34;Diagram of the operation of a RAG app: first the user question is sent to a retriever system, which fetches some additional data relevant to the question. Then, the question and the additional data is sent to the LLM to formulate an answer.&#34;&gt;&lt;/p&gt;
&lt;p&gt;This means that simple RAG is not an agent: the LLM has no role in the retrieval step and simply reacts to the RAG prompt, doing little more than what a direct LLM call does. &lt;strong&gt;The LLM is given no agency&lt;/strong&gt;, takes no decisions in order to accomplish its goals, and has no tools it can decide to use, or actions it can decide to take. It&amp;rsquo;s a fully pipelined, reactive system. However, we may rank basic RAG more on the autonomous side with respect to a direct LLM call, because there is one step that is done automonously (the retrieval).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/basic-rag-compass.png&#34; alt=&#34;the updated compass&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;agentic-rag&#34;&gt;
  Agentic RAG
  
&lt;/h3&gt;
&lt;p&gt;Agentic RAG is a slightly more advanced version of RAG that does not always perform the retrieval step. This helps the app produce better prompts for the LLM: for example, if the user is asking a question about trivia, retrieval is very important, while if they&amp;rsquo;re quizzing the LLM with some mathematical problem, retrieval might confuse the LLM by giving it examples of solutions to different puzzles, and therefore make hallucinations more likely.&lt;/p&gt;
&lt;p&gt;This means that an agentic RAG app works as follows: when the user asks a question, before calling the retriever the app checks whether the retrieval step is necessary at all. Most of the time the preliminary check is done by an LLM as well, but in theory the same check coould be done by a properly trained classifier model. Once the check is done, if retrieval was necessary it is run, otherwise the app skips directly to the LLM, which then replies to the user.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/agentic-rag.png&#34; alt=&#34;Diagram of the operation of an agentic RAG app: when the user asks a question, before calling the retriever the app checks whether the retrieval step is necessary at all. Once the check is done, if retrieval was necessary it is run, otherwise the app skips directly to the LLM, which then replies to the user.&#34;&gt;&lt;/p&gt;
&lt;p&gt;You can see immediately that there&amp;rsquo;s a fundamental difference between this type of RAG and the basic pipelined form: the app needs to &lt;strong&gt;take a decision&lt;/strong&gt; in order to accomplish the goal of answering the user. The goal is very limited (giving a correct answer to the user), and the decision very simple (use or not use a single tool), but this little bit of agency given to the LLM makes us place an application like this definitely more towards the Agent side of the diagram.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/agentic-rag-compass.png&#34; alt=&#34;the updated compass&#34;&gt;&lt;/p&gt;
&lt;p&gt;We keep Agentic RAG towards the Autonomous side because in the vast majority of cases the decision to invoke the retriever is kept hidden from the user.&lt;/p&gt;
&lt;h3 id=&#34;llms-with-function-calling&#34;&gt;
  LLMs with function calling
  
&lt;/h3&gt;
&lt;p&gt;Some LLM applications, such as ChatGPT with GPT4+ or Bing Chat, can make the LLM use some predefined tools: a web search, an image generator, and maybe a few more. The way they work is quite straightforward: when a user asks a question, the LLM first needs to decide whether it should use a tool to answer the question. If it decides that a tool is needed, it calls it, otherwise it skips directly to generating a reply, which is then sent back to the user.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/llm-with-function-calling.png&#34; alt=&#34;Diagram of the operation of an LLM with function calling: when a user asks a question, the LLM first needs to decide whether it should use a tool to answer the question. If it decides that a tool is needed, it calls it, otherwise it skips directly to generating a reply, which is then sent back to the user.&#34;&gt;&lt;/p&gt;
&lt;p&gt;You can see how this diagram resemble agentic RAG&amp;rsquo;s: before giving an answer to the user, the app needs to &lt;strong&gt;take a decision&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;With respect to Agentic RAG this decision is a lot more complex: it&amp;rsquo;s not a simple yes/no decision, but it involves choosing which tool to use and also generate the input parameters for the selected tool that will provide the desired output. In many cases the tool&amp;rsquo;s output will be given to the LLM to be re-elaborated (such as the output of a web search), while in some other it can go directly to the user (like in the case of image generators). This all implies that more agency is given to the system and, therefore, it can be placed more clearly towards the Agent end of the scale.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/llm-with-function-calling-compass.png&#34; alt=&#34;the updated compass&#34;&gt;&lt;/p&gt;
&lt;p&gt;We place LLMs with function calling in the middle between Conversational and Autonomous because the degree to which the user is aware of this decision can vary greatly between apps. For example, Bing Chat and ChatGPT normally notify the user that they&amp;rsquo;re going to use a tool when they do, and the user can instruct them to use them or not, so they&amp;rsquo;re slightly more conversational.&lt;/p&gt;
&lt;h3 id=&#34;self-correcting-rag&#34;&gt;
  Self correcting RAG
  
&lt;/h3&gt;
&lt;p&gt;Self-correcting RAG is a technique that improves on simple RAG by making the LLM double-check its replies before returning them to the user. It comes from an LLM evaluation technique called &amp;ldquo;LLM-as-a-judge&amp;rdquo;, because an LLM is used to judge the output of a different LLM or RAG pipeline.&lt;/p&gt;
&lt;p&gt;Self-correcting RAG starts as simple RAG: when the user asks a question, the retriever is called and the results are sent to the LLM to extract an answer from. However, before returning the answer to the user, another LLM is asked to judge whether in their opinion, the answer is correct. If the second LLM agrees, the answer is sent to the user. If not, the second LLM generates a new question for the retriever and runs it again, or in other cases, it simply integrates its opinion in the prompt and runs the first LLM again.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/self-correcting-rag.png&#34; alt=&#34;Diagram of the operation of self correcting RAG: when the user asks a question, the retriever is called and the results are sent to the LLM to extract an answer from. However, before returning the answer to the user, another LLM is asked to judge whether in their opinion, the answer is correct. If the second LLM agrees, the answer is sent to the user. If not, the second LLM generates a new question for the retriever and runs it again, or in other cases, it simply integrates its opinion in the prompt and runs the first LLM again.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Self-correcting RAG can be seen as &lt;strong&gt;one more step towards agentic behavior&lt;/strong&gt; because it unlocks a new possibility for the application: &lt;strong&gt;the ability to try again&lt;/strong&gt;. A self-correcting RAG app has a chance to detect its own mistakes and has the agency to decide that it&amp;rsquo;s better to try again, maybe with a slightly reworded question or different retrieval parameters, before answering the user. Given that this process is entirely autonomous, we&amp;rsquo;ll place this technique quite towards the Autonomous end of the scale.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/self-correcting-rag-compass.png&#34; alt=&#34;the updated compass&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;chain-of-thought&#34;&gt;
  Chain-of-thought
  
&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.11903&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chain-of-thought&lt;/a&gt; is a family of prompting techniques that makes the LLM &amp;ldquo;reason out loud&amp;rdquo;. It&amp;rsquo;s very useful when the model needs to process a very complicated question, such as a mathematical problem or a layered question like &amp;ldquo;When was the eldest sistem of the current King of Sweden born?&amp;rdquo; Assuming that the LLM knows these facts, in order to not hallucinate it&amp;rsquo;s best to ask the model to proceed &amp;ldquo;step-by-step&amp;rdquo; and find out, in order:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Who the current King of Sweden is,&lt;/li&gt;
&lt;li&gt;Whether he has an elder sister,&lt;/li&gt;
&lt;li&gt;If yes, who she is,&lt;/li&gt;
&lt;li&gt;The age of the person identified above.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The LLM might know the final fact in any case, but the probability of it giving the right answer increases noticeably if the LLM is prompted this way.&lt;/p&gt;
&lt;p&gt;Chain-of-thought prompts can also be seen as the LLM accomplishing the task of finding the correct answer in steps, which implies that there are two lines of thinking going on: on one side the LLM is answering the questions it&amp;rsquo;s posing to itself, while on the other it&amp;rsquo;s constantly re-assessing whether it has a final answer for the user.&lt;/p&gt;
&lt;p&gt;In the example above, the chain of thought might end at step 2 if the LLM realizes that the current King of Sweden has no elder sisters (he &lt;a href=&#34;https://en.wikipedia.org/wiki/Carl_XVI_Gustaf#Early_life&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doesn&amp;rsquo;t&lt;/a&gt;): the LLM needs to keep an eye of its own thought process and decide whether it needs to continue or not.&lt;/p&gt;
&lt;p&gt;We can summarize an app using chain-of-thought prompting like this: when a user asks a question, first of all the LLM reacts to the chain-of-thought prompt to lay out the sub-questions it needs to answer. Then it answers its own questions one by one, asking itself each time whether the final answer has already been found. When the LLM believes it has the final answer, it rewrites it for the user and returns it.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/chain-of-thought.png&#34; alt=&#34;Diagram of the operation of a chain-of-thought LLM app: when a user asks a question, first of all the LLM reacts to the chain-of-thought prompt to lay out the sub-questions it needs to answer. Then it answers its own questions one by one, asking itself each time whether the final answer has already been found. When the LLM believes it has the final answer, it rewrites it for the user and returns it &#34;&gt;&lt;/p&gt;
&lt;p&gt;This new prompting technique makes a big step towards full agency: the ability for the LLM to &lt;strong&gt;assess whether the goal has been achieved&lt;/strong&gt; before returning any answer to the user. While apps like Bing Chat iterate with the user and need their feedback to reach high-level goals, chain-of-thought gives the LLM the freedom to check its own answers before having the user judge them, which makes the loop much faster and can increase the output quality dramatically.&lt;/p&gt;
&lt;p&gt;This process is similar to what self-correcting RAG does, but has a wider scope, because the LLM does not only need to decide whether an answer is correct, it can also decide to continue reasoning in order to make it more complete, more detailed, to phrase it better, and so on.&lt;/p&gt;
&lt;p&gt;Another interesting trait of chain-of-thought apps is that they introduce the concept of &lt;strong&gt;inner monologue&lt;/strong&gt;. The inner monologue is a conversation that the LLM has with itself, a conversation buffer where it keeps adding messages as the reasoning develops. This monologue is not visible to the user, but helps the LLM deconstruct a complex reasoning line into a more manageable format, like a researcher that takes notes instead of keeping all their earlier reasoning inside their head all the times.&lt;/p&gt;
&lt;p&gt;Due to the wider scope of the decision-making that chain-of-thought apps are able to do, they also place in the middle of our compass They can be seen as slightly more autonomous than conversational due to the fact that they hide their internal monologue to the user.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/chain-of-thought-compass.png&#34; alt=&#34;the updated compass&#34;&gt;&lt;/p&gt;
&lt;p&gt;From here, the next step is straightforward: using tools.&lt;/p&gt;
&lt;h3 id=&#34;multi-hop-rag&#34;&gt;
  Multi-hop RAG
  
&lt;/h3&gt;
&lt;p&gt;Multi-hop RAG applications are nothing else than simple RAG apps that use chain-of-thought prompting and are free to invoke the retriever as many times as needed and only when needed.&lt;/p&gt;
&lt;p&gt;This is how it works. When the user makes a question, a chain of thought prompt is generated and sent to the LLM. The LLM assesses whether it knows the answer to the question and if not, asks itself whether a retrieval is necessary. If it decides that retrieval is necessary it calls it, otherwise it skips it and generates an answer directly. It then checks again whether the question is answered. Exiting the loop, the LLM produces a complete answer by re-reading its own inner monologue and returns this reply to the user.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/multi-hop-rag.png&#34; alt=&#34;Diagram of the operation of multi-hop RAG: when the user makes a question, a chain of thought prompt is generated and sent to the LLM. The LLM assesses whether it knows the answer to the question and if not, asks itself whether a retrieval is necessary. If it decides that retrieval is necessary it calls it, otherwise it skips it and generates an answer directly. It then checks again whether the question is answered. Exiting the loop, the LLM produces a complete answer by re-reading its own inner monologue and returns this reply to the user.&#34;&gt;&lt;/p&gt;
&lt;p&gt;An app like this is getting quite close to a proper autonomous agent, because it can &lt;strong&gt;perform its own research autonomously&lt;/strong&gt;. The LLM calls are made in such a way that the system is able to assess whether it knows enough to answer or whether it should do more research by formulating more questions for the retriever and then reasoning over the new collected data.&lt;/p&gt;
&lt;p&gt;Multi-hop RAG is a very powerful technique that shows a lot of agency and autonomy, and therefore can be placed in the lower-right quadrant of out compass. However, it is still limited with respect to a &amp;ldquo;true&amp;rdquo; autonomous agent, because the only action it can take is to invoke the retriever.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/multi-hop-rag-compass.png&#34; alt=&#34;the updated compass&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;react-agents&#34;&gt;
  ReAct Agents
  
&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now move onto apps that can be defined proper &amp;ldquo;agents&amp;rdquo;. One of the first flavor of agentic LLM apps, and still the most popular nowadays, is called &amp;ldquo;&lt;a href=&#34;https://arxiv.org/abs/2210.03629&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ReAct&lt;/a&gt;&amp;rdquo; Agents, which stands for &amp;ldquo;Reason + Act&amp;rdquo;. ReAct is a prompting technique that belongs to the chain-of-thought extended family: it makes the LLM reason step by step, decide whether to perform any action, and then observe the result of the actions it took before moving further.&lt;/p&gt;
&lt;p&gt;A ReAct agent works more or less like this: when user sets a goal, the app builds a ReAct prompt, which first of all asks the LLM whether the answer is already known. If the LLM says no, the prompt makes it select a tool. The tool returns some values which are added to the inner monologue of the application toghether with the invitation to re-assess whether the goal has been accomplished. The app loops over until the answer is found, and then the answer is returned to the user.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/react-agent.png&#34; alt=&#34;Diagram of the operation of a ReAct Agent: when user sets a goal, the app builds a ReAct prompt, which first of all asks the LLM whether the answer is already known. If the LLM says no, the prompt makes it select a tool. The tool returns some values which are added to the inner monologue of the application toghether with the invitation to re-assess whether the goal has been accomplished. The app loops over until the answer is found, and then the answer is returned to the user.&#34;&gt;&lt;/p&gt;
&lt;p&gt;As you can see, the structure is very similar to a multi-hop RAG, with an important difference: ReAct Agents normally have &lt;strong&gt;many tools to choose from&lt;/strong&gt; rather than a single retriever. This gives them the agency to take much more complex decisions and can be finally called &amp;ldquo;agents&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/react-agent-compass.png&#34; alt=&#34;the updated compass&#34;&gt;&lt;/p&gt;
&lt;p&gt;ReAct Agents are very autonomous in their tasks and rely on an inner monologue rather than a conversation with a user to achieve their goals. Therefore we place them very much on the Autonomous end of the spectrum.&lt;/p&gt;
&lt;h3 id=&#34;conversational-agents&#34;&gt;
  Conversational Agents
  
&lt;/h3&gt;
&lt;p&gt;Conversational Agents are a category of apps that can vary widely. As stated earlier, conversational agents focus on using the conversation itself as a tool to accomplish goals, so in order to understand them, one has to distinguish between the people that set the goal (let&amp;rsquo;s call them &lt;em&gt;owners&lt;/em&gt;) and those who talk with the bot (the &lt;em&gt;users&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Once this distinction is made, this is how the most basic conversational agents normally work. First, the owner sets a goal. The application then starts a conversation with a user and, right after the first message, starts asking itself if the given goal was accomplished. It then keeps talking to the target user until it believes the goal was attained and, once done, it returns back to its owner to report the outcome.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/basic-conversational-agent.png&#34; alt=&#34;Diagram of the operation of a Conversational Agent: first, the owner sets a goal. The application then starts a conversation with a user and, right after the first message, starts asking itself if the given goal was accomplished. It then keeps talking to the target user until it believes the goal was attained and, once done, it returns back to its owner to report the outcome.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Basic conversational agents are very agentic in the sense that they can take a task off the hands of their owners and keep working on them until the goal is achieved. However, &lt;strong&gt;they have varying degrees of agency&lt;/strong&gt; depending on how many tools they can use and how sophisticated is their ability to talk to their target users.&lt;/p&gt;
&lt;p&gt;For example, can the communication occurr over one single channel, be it email, chat, voice, or something else? Can the agent choose among different channels to reach the user? Can it perform side tasks to behalf of either party to work towards its task? There is a large variety of these agents available and no clear naming distinction between them, so depending on their abilities, their position on our compass might be very different. This is why we place them in the top center, spreading far out in both directions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/conversational-agent-compass.png&#34; alt=&#34;the updated compass&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;ai-crews&#34;&gt;
  AI Crews
  
&lt;/h3&gt;
&lt;p&gt;By far the most advanced agent implementation available right now is called AI Crew, such as the ones provided by &lt;a href=&#34;https://www.crewai.com/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CrewAI&lt;/a&gt;. These apps take the concept of autonomous agent to the next level by making several different agents work together.&lt;/p&gt;
&lt;p&gt;The way these apps works is very flexible. For example, let&amp;rsquo;s imagine we are making an AI application that can build a fully working mobile game from a simple description. This is an extremely complex task that, in real life, requires several developers. To achieve the same with an AI Crew, the crew needs to contain several agents, each one with their own special skills, tools, and background knowledge. There could be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a Designer Agent, that has all the tools to generate artwork and assets;&lt;/li&gt;
&lt;li&gt;a Writer Agent that writes the story, the copy, the dialogues, and most of the text;&lt;/li&gt;
&lt;li&gt;a Frontend Developer Agent that designs and implements the user interface;&lt;/li&gt;
&lt;li&gt;a Game Developer Agent that writes the code for the game itself;&lt;/li&gt;
&lt;li&gt;a Manager Agent, that coordinates the work of all the other agents, keeps them on track and eventually reports the results of their work to the user.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These agents interact with each other just like a team of humans would: by exchanging messages in a chat format, asking each other to perform actions for them, until their manager decides that the overall goal they were set to has been accomplished, and reports to the user.&lt;/p&gt;
&lt;p&gt;AI Crews are very advanced and dynamic systems that are still actively researched and explored. One thing that&amp;rsquo;s clear though is that they show the highest level of agency of any other LLM-based app, so we can place them right at the very bottom-right end of the scale.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-06-10-the-agent-compass/ai-crews-compass.png&#34; alt=&#34;the updated compass&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;
  Conclusion
  
&lt;/h2&gt;
&lt;p&gt;What we&amp;rsquo;ve seen here are just a few examples of LLM-powered applications and how close or far they are to the concept of a &amp;ldquo;real&amp;rdquo; AI agent. AI agents are still a very active area of research, and their effectiveness is getting more and more reasonable as LLMs become cheaper and more powerful.&lt;/p&gt;
&lt;p&gt;As matter of fact, with today&amp;rsquo;s LLMs true AI agents are possible, but in many cases they&amp;rsquo;re too brittle and expensive for real production use cases. Agentic systems today suffer from two main issues: they perform &lt;strong&gt;huge and frequent LLM calls&lt;/strong&gt; and they &lt;strong&gt;tolerate a very low error rate&lt;/strong&gt; in their decision making.&lt;/p&gt;
&lt;p&gt;Inner monologues can grow to an unbounded size during the agent&amp;rsquo;s operation, making the context window size a potential limitation. A single bad decision can send a chain-of-thought reasoning train in a completely wrong direction and many LLM calls will be performed before the system realizes its mistake, if it does at all. However, as LLMs become faster, cheaper and smarter, the day when AI Agent will become reliable and cheap enough is nearer than many think.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s be ready for it!&lt;/p&gt;
&lt;p class=&#34;fleuron&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-05-06-teranoptia/&#34;&gt;SDH&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Generating creatures with Teranoptia</title>
      <link>https://www.zansara.dev/posts/2024-05-06-teranoptia/</link>
      <pubDate>Mon, 06 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2024-05-06-teranoptia/</guid>
      <description>

&lt;style&gt;
    @font-face {
        font-family: teranoptia;
        src: url(&#34;/posts/2024-05-06-teranoptia/teranoptia/fonts/Teranoptia-Furiae.ttf&#34;);
    }

    .teranoptia {
        font-size: 5rem;
        font-family: teranoptia;
        hyphens: none!important;
        line-height: 70px;
    }

    .small {
        font-size:3rem;
        line-height: 40px;
    }

    .glyphset {
        display: flex;
        flex-wrap: wrap;
    }
    .glyphset div {
        margin: 3px;
    }
    .glyphset div p {
        text-align: center;
    }

&lt;/style&gt;


&lt;p&gt;Having fun with fonts doesn&amp;rsquo;t always mean obsessing over kerning and ligatures. Sometimes, writing text is not even the point!&lt;/p&gt;
&lt;p&gt;You don&amp;rsquo;t believe it? Type something in here.&lt;/p&gt;



&lt;textarea id=&#34;test-generated-animal&#34; class=&#34;teranoptia&#34; style=&#34;width: 100%; line-height: 50pt;&#34;&gt;&lt;/textarea&gt;

&lt;div style=&#34;display: flex; gap: 10px;&#34;&gt;
    Characters to generate:
    &lt;input id=&#34;test-glyph-count&#34; type=&#34;number&#34; value=10 &gt;&lt;/input&gt;
    &lt;button onclick=&#34;generateTest(document.getElementById(&#39;test-glyph-count&#39;).value);&#34;&gt;Generate!&lt;/button&gt;
&lt;/div&gt;

&lt;script&gt;
function makeBreakable(animal){
    // Line break trick - avoid hypens and allows wrapping
    const animalFragments = animal.split(/(?=[yvspmieaźACFILOSWŹv])/g);
    animal = animalFragments.join(&#34;&lt;wbr&gt;&#34;);
    return animal;
}

function generateTest(value){
    var newAnimal = &#39;&#39;;
    for (var i = 0; i &lt; value; i++) {
        newAnimal += randomFrom(validChars);
    }
    document.getElementById(&#34;test-generated-animal&#34;).value = newAnimal;
}

&lt;/script&gt;


&lt;p&gt;&lt;a href=&#34;https://www.tunera.xyz/fonts/teranoptia/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Teranoptia&lt;/a&gt; is a cool font that lets you build small creatures by mapping each letter (and a few other characters) to a piece of a creature like a head, a tail, a leg, a wing and so on. By typing words you can create strings of creatures.&lt;/p&gt;
&lt;p&gt;Here is the glyphset:&lt;/p&gt;


&lt;div class=&#34;glyphset&#34;&gt;
    &lt;div&gt;&lt;p&gt;A&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;A&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;B&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;B&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;C&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;C&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;D&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;D&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;E&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;E&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;F&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;F&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;G&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;G&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;H&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;H&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;I&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;I&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;J&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;J&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;K&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;K&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;L&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;L&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;M&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;M&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;N&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;N&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;O&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;O&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;P&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;P&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;Q&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;Q&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;R&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;R&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;S&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;S&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;T&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;T&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;U&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;U&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;V&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;V&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;W&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;W&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;X&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;X&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;Ẋ&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;Ẋ&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;Y&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;Y&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;Z&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;Z&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;Ź&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;Ź&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;Ž&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;Ž&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;Ż&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;Ż&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;a&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;a&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;b&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;b&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;ḅ&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;ḅ&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;c&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;c&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;d&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;d&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;e&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;e&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;f&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;f&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;g&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;g&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;h&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;h&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;i&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;i&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;j&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;j&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;k&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;k&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;l&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;l&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;m&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;m&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;n&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;n&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;o&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;o&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;p&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;p&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;q&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;q&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;r&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;r&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;s&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;s&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;t&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;t&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;u&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;u&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;v&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;v&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;w&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;w&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;x&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;x&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;y&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;y&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;z&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;z&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;ź&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;ź&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;ž&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;ž&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;ż&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;ż&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;,&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;,&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;*&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;*&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;(&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;(&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;)&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;)&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;{&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;{&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;}&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;}&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;[&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;[&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;]&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;]&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;‐&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;‐&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;“&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;“&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;”&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;”&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;‘&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;‘&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;’&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;’&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;«&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;«&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;»&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;»&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;‹&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;‹&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;›&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;›&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;$&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;$&lt;/p&gt;&lt;/div&gt;

    &lt;div&gt;&lt;p&gt;€&lt;/p&gt;&lt;p class=&#34;teranoptia&#34;&gt;€&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;

You&#39;ll notice that there&#39;s a lot you can do with it, from assembling simple creatures:

&lt;p class=&#34;teranoptia&#34;&gt;vTN&lt;/p&gt;

to more complex, multi-line designs:

&lt;p class=&#34;teranoptia&#34;&gt;&lt;wbr&gt; {Ž}&lt;/p&gt;
&lt;p class=&#34;teranoptia&#34;&gt;F] [Z&lt;/p&gt;



&lt;p&gt;Let&amp;rsquo;s play with it a bit and see how we can put together a few &amp;ldquo;correct&amp;rdquo; looking creatures.&lt;/p&gt;
&lt;div class=&#34;notice info&#34;&gt;
  &lt;div class=&#34;notice-content&#34;&gt;&lt;em&gt;As you&amp;rsquo;re about to notice, I&amp;rsquo;m no JavaScript developer. Don&amp;rsquo;t expect high-quality JS in this post.&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;mirroring-animals&#34;&gt;
  Mirroring animals
  
&lt;/h2&gt;
&lt;p&gt;To begin with, let&amp;rsquo;s start with a simple function: animal mirroring. The glyphset includes a mirrored version of each non-symmetric glyph, but the mapping is rather arbitrary, so we are going to need a map.&lt;/p&gt;
&lt;p&gt;Here are the pairs:&lt;/p&gt;
&lt;p class=&#34;small teranoptia&#34; style=&#34;letter-spacing: 5px;&#34;&gt; By Ev Hs Kp Nm Ri Ve Za Żź Az Cx Fu Ir Lo Ol Sh Wd Źż vE Dw Gt Jq Mn Pk Qj Tg Uf Xc Ẋḅ Yb Žž bY cX () [] {} &lt;/p&gt;
&lt;h3 id=&#34;animal-mirror&#34;&gt;
  Animal mirror
  
&lt;/h3&gt;



&lt;div style=&#34;display: flex; gap: 10px;&#34;&gt;
    &lt;input id=&#34;original-animal&#34; type=&#34;text&#34; class=&#34;teranoptia&#34; style=&#34;width: 50%; text-align:right;&#34; oninput=&#34;mirrorAnimal(this.value);&#34; value=&#34;WYZ*p»gh&#34;&gt;&lt;/input&gt;
    &lt;p id=&#34;mirrored-animal&#34; class=&#34;teranoptia&#34; style=&#34;line-height: 50pt;&#34;&gt;ST»K*abd&lt;/p&gt;
&lt;/div&gt;

&lt;script&gt;
const mirrorPairs = {&#34;B&#34;: &#34;y&#34;,  &#34;y&#34;: &#34;B&#34;, &#34;E&#34;: &#34;v&#34;,  &#34;v&#34;: &#34;E&#34;, &#34;H&#34;: &#34;s&#34;,  &#34;s&#34;: &#34;H&#34;, &#34;K&#34;: &#34;p&#34;,  &#34;p&#34;: &#34;K&#34;, &#34;N&#34;: &#34;m&#34;,  &#34;m&#34;: &#34;N&#34;, &#34;R&#34;: &#34;i&#34;,  &#34;i&#34;: &#34;R&#34;, &#34;V&#34;: &#34;e&#34;,  &#34;e&#34;: &#34;V&#34;, &#34;Z&#34;: &#34;a&#34;,  &#34;a&#34;: &#34;Z&#34;, &#34;Ż&#34;: &#34;ź&#34;,  &#34;ź&#34;: &#34;Ż&#34;, &#34;A&#34;: &#34;z&#34;,  &#34;z&#34;: &#34;A&#34;, &#34;C&#34;: &#34;x&#34;,  &#34;x&#34;: &#34;C&#34;, &#34;F&#34;: &#34;u&#34;,  &#34;u&#34;: &#34;F&#34;, &#34;I&#34;: &#34;r&#34;,  &#34;r&#34;: &#34;I&#34;, &#34;L&#34;: &#34;o&#34;,  &#34;o&#34;: &#34;L&#34;, &#34;O&#34;: &#34;l&#34;,  &#34;l&#34;: &#34;O&#34;, &#34;S&#34;: &#34;h&#34;,  &#34;h&#34;: &#34;S&#34;, &#34;W&#34;: &#34;d&#34;,  &#34;d&#34;: &#34;W&#34;, &#34;Ź&#34;: &#34;ż&#34;,  &#34;ż&#34;: &#34;Ź&#34;, &#34;v&#34;: &#34;E&#34;,  &#34;E&#34;: &#34;v&#34;, &#34;D&#34;: &#34;w&#34;,  &#34;w&#34;: &#34;D&#34;, &#34;G&#34;: &#34;t&#34;,  &#34;t&#34;: &#34;G&#34;, &#34;J&#34;: &#34;q&#34;,  &#34;q&#34;: &#34;J&#34;, &#34;M&#34;: &#34;n&#34;,  &#34;n&#34;: &#34;M&#34;, &#34;P&#34;: &#34;k&#34;,  &#34;k&#34;: &#34;P&#34;, &#34;Q&#34;: &#34;j&#34;,  &#34;j&#34;: &#34;Q&#34;, &#34;T&#34;: &#34;g&#34;,  &#34;g&#34;: &#34;T&#34;, &#34;U&#34;: &#34;f&#34;,  &#34;f&#34;: &#34;U&#34;, &#34;X&#34;: &#34;c&#34;,  &#34;c&#34;: &#34;X&#34;, &#34;Ẋ&#34;: &#34;ḅ&#34;,  &#34;ḅ&#34;: &#34;Ẋ&#34;, &#34;Y&#34;: &#34;b&#34;,  &#34;b&#34;: &#34;Y&#34;, &#34;Ž&#34;: &#34;ž&#34;,  &#34;ž&#34;: &#34;Ž&#34;, &#34;b&#34;: &#34;Y&#34;,  &#34;Y&#34;: &#34;b&#34;, &#34;c&#34;: &#34;X&#34;,  &#34;X&#34;: &#34;c&#34;, &#34;(&#34;: &#34;)&#34;,  &#34;)&#34;: &#34;(&#34;, &#34;[&#34;: &#34;]&#34;,  &#34;]&#34;: &#34;[&#34;, &#34;{&#34;: &#34;}&#34;, &#34;}&#34;: &#34;{&#34;};

function mirrorAnimal(original){
    var mirror = &#39;&#39;;
    for (i = original.length-1; i &gt;= 0; i--){
        newChar = mirrorPairs[original.charAt(i)];
        if (newChar){
            mirror += newChar;
        } else {
            mirror += original.charAt(i)
        }
        console.log(original, original.charAt(i), mirrorPairs[original.charAt(i)], mirror);
    }
    document.getElementById(&#34;mirrored-animal&#34;).innerHTML = mirror;
}
&lt;/script&gt;


&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-javascript&#34; data-lang=&#34;javascript&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; mirrorPairs &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;B&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;y&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;y&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;B&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;E&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;v&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;v&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;E&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;H&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;s&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;s&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;H&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;K&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;p&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;p&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;K&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;N&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;m&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;m&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;N&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;R&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;i&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;i&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;R&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;V&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;e&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;e&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;V&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Z&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;a&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Z&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Ż&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ź&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ź&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Ż&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;A&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;z&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;z&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;A&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;C&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;x&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;x&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;C&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;F&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;u&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;u&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;F&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;I&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;r&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;r&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;I&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;L&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;o&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;o&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;L&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;O&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;l&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;l&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;O&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;S&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;h&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;h&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;S&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;W&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;d&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;d&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;W&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Ź&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ż&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ż&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Ź&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;v&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;E&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;E&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;v&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;D&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;w&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;w&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;D&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;G&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;t&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;t&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;G&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;J&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;q&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;q&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;J&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;M&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;n&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;n&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;M&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;P&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;k&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;k&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;P&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Q&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;j&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;j&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Q&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;T&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;g&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;g&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;T&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;U&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;f&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;f&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;U&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;X&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;c&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;c&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;X&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Ẋ&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ḅ&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ḅ&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Ẋ&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Y&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;b&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;b&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Y&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Ž&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ž&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ž&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Ž&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;b&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Y&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Y&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;b&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;c&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;X&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;X&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;c&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;(&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;)&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;)&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;(&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;[&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;]&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;]&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;[&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;{&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;}&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;}&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;{&amp;#34;&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;function&lt;/span&gt; mirrorAnimal(original){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;var&lt;/span&gt; mirror &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;for&lt;/span&gt; (i &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; original.length&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;; i &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;0&lt;/span&gt;; i&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;--&lt;/span&gt;){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        newChar &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; mirrorPairs[original.charAt(i)];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; (newChar){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            mirror &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+=&lt;/span&gt; newChar;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        } &lt;span style=&#34;color:#ff7b72&#34;&gt;else&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            mirror &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+=&lt;/span&gt; original.charAt(i)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;return&lt;/span&gt; mirror;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;random-animal-generation&#34;&gt;
  Random animal generation
  
&lt;/h2&gt;
&lt;p&gt;While it&amp;rsquo;s fun to build complicated animals this way, you&amp;rsquo;ll notice something: it&amp;rsquo;s pretty hard to make them come out right by simply typing something. Most of the time you need quite careful planning. In addition there&amp;rsquo;s almost no meaningful (English) word that corresponds to a well-defined creature. Very often the characters don&amp;rsquo;t match, creating a sequence of &amp;ldquo;chopped&amp;rdquo; creatures.&lt;/p&gt;
&lt;p&gt;For example, &amp;ldquo;Hello&amp;rdquo; becomes:&lt;/p&gt;
&lt;p class=&#34;teranoptia&#34;&gt;Hello&lt;/p&gt;
&lt;p&gt;This is a problem if we want to make a parametric or random creature generator, because most of the random strings won&amp;rsquo;t look good.&lt;/p&gt;
&lt;h3 id=&#34;naive-random-generator&#34;&gt;
  Naive random generator
  
&lt;/h3&gt;


&lt;div style=&#34;display: flex; gap: 10px;&#34;&gt;
    Characters to generate:
    &lt;input id=&#34;naive-glyph-count&#34; type=&#34;number&#34; value=10&gt;&lt;/input&gt;
    &lt;button onclick=&#34;generateNaive(document.getElementById(&#39;naive-glyph-count&#39;).value);&#34;&gt;Generate!&lt;/button&gt;
&lt;/div&gt;

&lt;p id=&#34;naive-generated-animal&#34; class=&#34;teranoptia&#34; style=&#34;line-height: 50pt;&#34;&gt;n]Zgameź)‐&lt;/p&gt;

&lt;script&gt;
const validChars = &#34;ABCDEFGHIJKLMNOPQRSTUVWXẊYZŹŽŻabḅcdefghijklmnopqrstuvwxyzźžż,*(){}[]‐“”«»$&#34;; //‘’‹›€

function randomFrom(list){
    return list[Math.floor(Math.random() * list.length)];
}

function generateNaive(value){
    var newAnimal = &#39;&#39;;
    for (var i = 0; i &lt; value; i++) {
        newAnimal += randomFrom(validChars);
    }
    
    // Line break trick - helps with wrapping
    const animalFragments = newAnimal.split(&#39;&#39;);
    newAnimal = animalFragments.join(&#34;&lt;wbr&gt;&#34;);

    document.getElementById(&#34;naive-generated-animal&#34;).innerHTML = newAnimal;
}
generateNaive(document.getElementById(&#39;naive-glyph-count&#39;).value);

&lt;/script&gt;


&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-javascript&#34; data-lang=&#34;javascript&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; validChars &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ABCDEFGHIJKLMNOPQRSTUVWXẊYZŹŽŻabḅcdefghijklmnopqrstuvwxyzźžż,*(){}[]‐“”«»$&amp;#34;&lt;/span&gt;; &lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;// ‘’‹›€ excluded because they&amp;#39;re mostly vertical
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;function&lt;/span&gt; randomFrom(list){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;return&lt;/span&gt; list[Math.floor(Math.random() &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;*&lt;/span&gt; list.length)];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;function&lt;/span&gt; generateNaive(value){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;var&lt;/span&gt; newAnimal &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#ff7b72&#34;&gt;var&lt;/span&gt; i &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;0&lt;/span&gt;; i &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;lt;&lt;/span&gt; value; i&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        newAnimal &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+=&lt;/span&gt; randomFrom(validChars);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;return&lt;/span&gt; newAnimal;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Can we do better than this?&lt;/p&gt;
&lt;h2 id=&#34;generating-good-animals&#34;&gt;
  Generating &amp;ldquo;good&amp;rdquo; animals
  
&lt;/h2&gt;
&lt;p&gt;There are many ways to define &amp;ldquo;good&amp;rdquo; or &amp;ldquo;well-formed&amp;rdquo; creatures. One of the first rules we can introduce is that we don&amp;rsquo;t want chopped body parts to float alone.&lt;/p&gt;
&lt;p&gt;Translating it into a rule we can implement: a character that is &amp;ldquo;open&amp;rdquo; on the right must be followed by a character that is open on the left, and a character that is &lt;em&gt;not&lt;/em&gt; open on the right must be followed by another character that is &lt;em&gt;not&lt;/em&gt; open on the left.&lt;/p&gt;
&lt;p&gt;For example, &lt;span class=&#34;small teranoptia&#34;&gt;A&lt;/span&gt; may be followed by &lt;span class=&#34;small teranoptia&#34;&gt;B&lt;/span&gt; to make &lt;span class=&#34;small teranoptia&#34;&gt;AB&lt;/span&gt;, but &lt;span class=&#34;small teranoptia&#34;&gt;A&lt;/span&gt; cannot be followed by &lt;span class=&#34;small teranoptia&#34;&gt;C&lt;/span&gt; to make &lt;span class=&#34;small teranoptia&#34;&gt;AC&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the same way, &lt;span class=&#34;small teranoptia&#34;&gt;Z&lt;/span&gt; may be followed by &lt;span class=&#34;small teranoptia&#34;&gt;A&lt;/span&gt; to make &lt;span class=&#34;small teranoptia&#34;&gt;ZA&lt;/span&gt;, but &lt;span class=&#34;small teranoptia&#34;&gt;Z&lt;/span&gt; cannot be followed by &lt;span class=&#34;small teranoptia&#34;&gt;ż&lt;/span&gt; to make &lt;span class=&#34;small teranoptia&#34;&gt;Zż&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This way we will get rid of all those &amp;ldquo;chopped&amp;rdquo; monsters that make up most of the randomly generated string.&lt;/p&gt;
&lt;p&gt;To summarize, the rules we have to implement are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Any character that is open on the right must be followed by another character that is open on the left.&lt;/li&gt;
&lt;li&gt;Any character that is closed on the right must be followed by another character that is closed on the left.&lt;/li&gt;
&lt;li&gt;The first character must not be open on the left.&lt;/li&gt;
&lt;li&gt;The last character must not be open on the right.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;non-chopped-animals-generator&#34;&gt;
  Non-chopped animals generator
  
&lt;/h3&gt;


&lt;div style=&#34;display: flex; gap: 10px;&#34;&gt;
    Characters to generate:
    &lt;input id=&#34;nochop-glyph-count&#34; type=&#34;number&#34; value=10&gt;&lt;/input&gt;
    &lt;button onclick=&#34;generateNoChop(document.getElementById(&#39;nochop-glyph-count&#39;).value);&#34;&gt;Generate!&lt;/button&gt;
&lt;/div&gt;

&lt;p id=&#34;nochop-generated-animal&#34; class=&#34;teranoptia&#34; style=&#34;line-height: 50pt;&#34;&gt;suSHebQ«EIl&lt;/p&gt;

&lt;script&gt;
const charsOpenOnTheRightOnly = &#34;yvspmieaźACFILOSWŹ({[&#34;;
const charsOpenOnTheLeftOnly =  &#34;BEHKNRVZŻzxurolhdż)]}&#34;;
const charsOpenOnBothSides = &#34;DGJMPQTUXẊYŽbcwtqnkjgfcḅbžYX«»&#34;;
const charsOpenOnNoSides = &#34;,*-“”&#34;;

const charsOpenOnTheRight = charsOpenOnTheRightOnly + charsOpenOnBothSides;
const charsOpenOnTheLeft = charsOpenOnTheLeftOnly + charsOpenOnBothSides;
const validInitialChars = charsOpenOnTheRightOnly + charsOpenOnNoSides;

function generateNoChop(value){
    document.getElementById(&#34;nochop-generated-animal&#34;).innerHTML = &#34;&#34;;
    var newAnimal = &#39;&#39; + randomFrom(validInitialChars);
    for (var i = 0; i &lt; value-1; i++) {
        if (charsOpenOnTheRight.indexOf(newAnimal[i]) &gt; -1){
            newAnimal += randomFrom(charsOpenOnTheLeft);

        } else if (charsOpenOnTheLeftOnly.indexOf(newAnimal[i]) &gt; -1){
            newAnimal += randomFrom(charsOpenOnTheRightOnly);
        
        } else if (charsOpenOnNoSides.indexOf(newAnimal[i]) &gt; -1){
            newAnimal += randomFrom(validInitialChars);
        }
    }
    // Final character
    if (charsOpenOnTheRight.indexOf(newAnimal[i]) &gt; -1){
        newAnimal += randomFrom(charsOpenOnTheLeftOnly);
    } else {
        newAnimal += randomFrom(charsOpenOnNoSides);
    }
    document.getElementById(&#34;nochop-generated-animal&#34;).innerHTML = makeBreakable(newAnimal);
}
generateNoChop(document.getElementById(&#34;nochop-glyph-count&#34;).value);

&lt;/script&gt;


&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-javascript&#34; data-lang=&#34;javascript&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; charsOpenOnTheRightOnly &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;yvspmieaźACFILOSWŹ({[&amp;#34;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; charsOpenOnTheLeftOnly &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;  &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;BEHKNRVZŻzxurolhdż)]}&amp;#34;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; charsOpenOnBothSides &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DGJMPQTUXẊYŽbcwtqnkjgfcḅbžYX«»&amp;#34;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; charsOpenOnNoSides &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;,*-“”&amp;#34;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; charsOpenOnTheRight &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; charsOpenOnTheRightOnly &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; charsOpenOnBothSides;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; charsOpenOnTheLeft &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; charsOpenOnTheLeftOnly &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; charsOpenOnBothSides;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; validInitialChars &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; charsOpenOnTheRightOnly &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; charsOpenOnNoSides;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;function&lt;/span&gt; generateNoChop(value){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;var&lt;/span&gt; newAnimal &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; randomFrom(validInitialChars);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#ff7b72&#34;&gt;var&lt;/span&gt; i &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;0&lt;/span&gt;; i &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;lt;&lt;/span&gt; value&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;; i&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; (charsOpenOnTheRight.indexOf(newAnimal[i]) &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            newAnimal &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+=&lt;/span&gt; randomFrom(charsOpenOnTheLeft);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        } &lt;span style=&#34;color:#ff7b72&#34;&gt;else&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; (charsOpenOnTheLeftOnly.indexOf(newAnimal[i]) &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            newAnimal &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+=&lt;/span&gt; randomFrom(charsOpenOnTheRightOnly);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        } &lt;span style=&#34;color:#ff7b72&#34;&gt;else&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; (charsOpenOnNoSides.indexOf(newAnimal[i]) &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            newAnimal &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+=&lt;/span&gt; randomFrom(validInitialChars);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;// Final character
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; (charsOpenOnTheRight.indexOf(newAnimal[i]) &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        newAnimal &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+=&lt;/span&gt; randomFrom(charsOpenOnTheLeftOnly);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    } &lt;span style=&#34;color:#ff7b72&#34;&gt;else&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        newAnimal &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+=&lt;/span&gt; randomFrom(charsOpenOnNoSides);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;return&lt;/span&gt; newAnimal;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The resulting animals are already quite better!&lt;/p&gt;
&lt;p&gt;There are still a few things we may want to fix. For example, some animals end up being just a pair of heads (such as &lt;span class=&#34;small teranoptia&#34;&gt;sN&lt;/span&gt;); others instead have their bodies oriented in the wrong direction (like &lt;span class=&#34;small teranoptia&#34;&gt;IgV&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try to get rid of those too.&lt;/p&gt;
&lt;p&gt;The trick here is to separate the characters into two groups: elements that are &amp;ldquo;facing left&amp;rdquo;, elements that are &amp;ldquo;facing right&amp;rdquo;, and symmetric ones. At this point, it&amp;rsquo;s convenient to call them &amp;ldquo;heads&amp;rdquo;, &amp;ldquo;bodies&amp;rdquo; and &amp;ldquo;tails&amp;rdquo; to make the code more understandable, like the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Right heads: &lt;span class=&#34;small teranoptia&#34;&gt;BEHKNRVZŻ&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Left heads: &lt;span class=&#34;small teranoptia&#34;&gt;yvspmieaź&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Right tails: &lt;span class=&#34;small teranoptia&#34;&gt;ACFILOSWŹv&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Left tails: &lt;span class=&#34;small teranoptia&#34;&gt;zxurolhdżE&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Right bodies: &lt;span class=&#34;small teranoptia&#34; style=&#34;letter-spacing: 5px;&#34;&gt;DGJMPQTUẊŽ&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Left bodies: &lt;span class=&#34;small teranoptia&#34; style=&#34;letter-spacing: 5px;&#34;&gt;wtqnkjgfḅž&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Entering hole: &lt;span class=&#34;small teranoptia&#34; style=&#34;letter-spacing: 5px;&#34;&gt;)]}&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Exiting hole: &lt;span class=&#34;small teranoptia&#34; style=&#34;letter-spacing: 5px;&#34;&gt;([{&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bounce &amp;amp; symmetric bodies: &lt;span class=&#34;small teranoptia&#34; style=&#34;letter-spacing: 5px;&#34;&gt;«»$bcXY&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Singletons: &lt;span class=&#34;small teranoptia&#34; style=&#34;letter-spacing: 5px;&#34;&gt;,*-&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s put this all together!&lt;/p&gt;
&lt;h3 id=&#34;oriented-animals-generator&#34;&gt;
  Oriented animals generator
  
&lt;/h3&gt;


&lt;div style=&#34;display: flex; gap: 10px;&#34;&gt;
    Characters to generate:
    &lt;input id=&#34;oriented-glyph-count&#34; type=&#34;number&#34; value=10&gt;&lt;/input&gt;
    &lt;button onclick=&#34;generateOriented(document.getElementById(&#39;oriented-glyph-count&#39;).value);&#34;&gt;Generate!&lt;/button&gt;
&lt;/div&gt;

&lt;p id=&#34;oriented-generated-animal&#34; class=&#34;teranoptia&#34; style=&#34;line-height: 50pt;&#34;&gt;suSHebQ«EIl&lt;/p&gt;

&lt;script&gt;

const rightAnimalHeads = &#34;BEHKNRVZŻ&#34;;
const leftAnimalHeads = &#34;yvspmieaź&#34;;
const rightAnimalTails = &#34;ACFILOSWŹv&#34;;
const leftAnimalTails = &#34;zxurolhdżE&#34;;
const rightAnimalBodies = &#34;DGJMPQTUẊŽ&#34;;
const leftAnimalBodies = &#34;wtqnkjgfḅž&#34;;
const singletons = &#34;,*‐&#34;;
const exitingHole = &#34;([{&#34;;
const enteringHole = &#34;)]}&#34;;
const bounce = &#34;«»$bcXY&#34;;

const validStarts = leftAnimalHeads + rightAnimalTails + exitingHole;
const validSuccessors = {
    [exitingHole + bounce]: rightAnimalHeads + rightAnimalBodies + leftAnimalBodies + leftAnimalTails + enteringHole + bounce,
    [enteringHole]: rightAnimalTails + leftAnimalHeads + exitingHole + singletons,
    [rightAnimalHeads + leftAnimalTails + singletons]: rightAnimalTails + leftAnimalHeads + exitingHole + singletons,
    [leftAnimalHeads]: leftAnimalBodies + leftAnimalBodies + leftAnimalBodies + leftAnimalTails + enteringHole + bounce,
    [rightAnimalTails]: rightAnimalBodies + rightAnimalBodies + rightAnimalBodies + rightAnimalHeads + enteringHole + bounce,
    [rightAnimalBodies]: rightAnimalBodies + rightAnimalBodies + rightAnimalBodies + rightAnimalHeads + enteringHole + bounce,
    [leftAnimalBodies]: leftAnimalBodies + leftAnimalBodies + leftAnimalBodies + leftAnimalTails + enteringHole + bounce,
};
const validEnds = {
    [exitingHole + bounce]: leftAnimalTails + rightAnimalHeads + enteringHole,
    [rightAnimalHeads + leftAnimalTails + enteringHole]: singletons,
    [leftAnimalHeads]: leftAnimalTails + enteringHole,
    [rightAnimalTails]: rightAnimalHeads + enteringHole,
    [rightAnimalBodies]: rightAnimalHeads,
    [leftAnimalBodies]: leftAnimalTails,
};

function generateOriented(value){

    var newAnimal = &#39;&#39; + randomFrom(validStarts);
    for (var i = 0; i &lt; value-1; i++) {
        last_char = newAnimal[i-1];
        for (const [predecessor, successor] of Object.entries(validSuccessors)) {
            if (predecessor.indexOf(last_char) &gt; -1){
                newAnimal += randomFrom(successor);
                break;
            }
        }
    }
    last_char = newAnimal[i-1];
    for (const [predecessor, successor] of Object.entries(validEnds)) {
        if (predecessor.indexOf(last_char) &gt; -1){
            newAnimal += randomFrom(successor);
            break;
        }
    }
    document.getElementById(&#34;oriented-generated-animal&#34;).innerHTML = makeBreakable(newAnimal);
}
generateOriented(document.getElementById(&#34;oriented-glyph-count&#34;).value);

&lt;/script&gt;


&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-javascript&#34; data-lang=&#34;javascript&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; rightAnimalHeads &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;BEHKNRVZŻ&amp;#34;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; leftAnimalHeads &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;yvspmieaź&amp;#34;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; rightAnimalTails &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ACFILOSWŹv&amp;#34;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; leftAnimalTails &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;zxurolhdżE&amp;#34;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; rightAnimalBodies &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DGJMPQTUẊŽ&amp;#34;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; leftAnimalBodies &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;wtqnkjgfḅž&amp;#34;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; singletons &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;,*‐&amp;#34;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; exitingHole &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;([{&amp;#34;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; enteringHole &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;)]}&amp;#34;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; bounce &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;«»$bcXY&amp;#34;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; validStarts &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; leftAnimalHeads &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; rightAnimalTails &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; exitingHole;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; validSuccessors &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    [exitingHole &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; bounce]&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; rightAnimalHeads &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; rightAnimalBodies &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; leftAnimalBodies &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; leftAnimalTails &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; enteringHole &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; bounce,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    [enteringHole]&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; rightAnimalTails &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; leftAnimalHeads &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; exitingHole &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; singletons,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    [rightAnimalHeads &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; leftAnimalTails &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; singletons]&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; rightAnimalTails &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; leftAnimalHeads &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; exitingHole &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; singletons,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    [leftAnimalHeads]&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; leftAnimalBodies &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; leftAnimalBodies &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; leftAnimalBodies &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; leftAnimalTails &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; enteringHole &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; bounce,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    [rightAnimalTails]&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; rightAnimalBodies &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; rightAnimalBodies &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; rightAnimalBodies &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; rightAnimalHeads &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; enteringHole &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; bounce,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    [rightAnimalBodies]&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; rightAnimalBodies &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; rightAnimalBodies &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; rightAnimalBodies &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; rightAnimalHeads &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; enteringHole &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; bounce,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    [leftAnimalBodies]&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; leftAnimalBodies &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; leftAnimalBodies &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; leftAnimalBodies &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; leftAnimalTails &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; enteringHole &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; bounce,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; validEnds &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    [exitingHole &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; bounce]&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; leftAnimalTails &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; rightAnimalHeads &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; enteringHole,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    [rightAnimalHeads &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; leftAnimalTails &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; enteringHole]&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; singletons,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    [leftAnimalHeads]&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; leftAnimalTails &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; enteringHole,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    [rightAnimalTails]&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; rightAnimalHeads &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; enteringHole,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    [rightAnimalBodies]&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; rightAnimalHeads,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    [leftAnimalBodies]&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;:&lt;/span&gt; leftAnimalTails,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;function&lt;/span&gt; generateOriented(value){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;var&lt;/span&gt; newAnimal &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; randomFrom(validStarts);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#ff7b72&#34;&gt;var&lt;/span&gt; i &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;0&lt;/span&gt;; i &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;lt;&lt;/span&gt; value&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;; i&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        last_char &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; newAnimal[i&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff7b72&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; [predecessor, successor] &lt;span style=&#34;color:#ff7b72&#34;&gt;of&lt;/span&gt; Object.entries(validSuccessors)) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; (predecessor.indexOf(last_char) &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                newAnimal &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+=&lt;/span&gt; randomFrom(successor);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#ff7b72&#34;&gt;break&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    last_char &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; newAnimal[i&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; [predecessor, successor] &lt;span style=&#34;color:#ff7b72&#34;&gt;of&lt;/span&gt; Object.entries(validEnds)) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; (predecessor.indexOf(last_char) &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            newAnimal &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+=&lt;/span&gt; randomFrom(successor);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#ff7b72&#34;&gt;break&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;return&lt;/span&gt; newAnimal;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;a-regular-grammar&#34;&gt;
  A regular grammar
  
&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s move up a level now.&lt;/p&gt;
&lt;p&gt;What we&amp;rsquo;ve defined up to this point is a set of rules that, given a string, determine what characters are allowed next. This is called a &lt;a href=&#34;https://en.wikipedia.org/wiki/Formal_grammar&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;formal grammar&lt;/strong&gt;&lt;/a&gt; in Computer Science.&lt;/p&gt;
&lt;p&gt;A grammar is defined primarily by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;an &lt;strong&gt;alphabet&lt;/strong&gt; of symbols (our Teranoptia font).&lt;/li&gt;
&lt;li&gt;a set of &lt;strong&gt;starting characters&lt;/strong&gt;: all the characters that can be used at the start of the string (such as &lt;span class=&#34;small teranoptia&#34;&gt;a&lt;/span&gt; or &lt;span class=&#34;small teranoptia&#34;&gt;*&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;a set of &lt;strong&gt;terminating character&lt;/strong&gt;: all the characters that can be used to terminate the string (such as &lt;span class=&#34;small teranoptia&#34;&gt;d&lt;/span&gt; or &lt;span class=&#34;small teranoptia&#34;&gt;-&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;a set of &lt;strong&gt;production rules&lt;/strong&gt;: the rules needed to generate valid strings in that grammar.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In our case, we&amp;rsquo;re looking for a grammar that defines &amp;ldquo;well formed&amp;rdquo; animals. For example, our production rules might look like this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;S (the start of the string) → a (&lt;span class=&#34;small teranoptia&#34;&gt;a&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;a (&lt;span class=&#34;small teranoptia&#34;&gt;a&lt;/span&gt;) → ad (&lt;span class=&#34;small teranoptia&#34;&gt;ad&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;a (&lt;span class=&#34;small teranoptia&#34;&gt;a&lt;/span&gt;) → ab (&lt;span class=&#34;small teranoptia&#34;&gt;ab&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;b (&lt;span class=&#34;small teranoptia&#34;&gt;b&lt;/span&gt;) → bb (&lt;span class=&#34;small teranoptia&#34;&gt;bb&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;b (&lt;span class=&#34;small teranoptia&#34;&gt;b&lt;/span&gt;) → bd (&lt;span class=&#34;small teranoptia&#34;&gt;bd&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;d (&lt;span class=&#34;small teranoptia&#34;&gt;d&lt;/span&gt;) → E (the end of the string)&lt;/li&gt;
&lt;li&gt;, (&lt;span class=&#34;small teranoptia&#34;&gt;,&lt;/span&gt;) → E (the end of the string)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and so on. Each combination would have its own rule.&lt;/p&gt;
&lt;p&gt;There are three main types of grammars according to Chomsky&amp;rsquo;s hierarchy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Regular grammars&lt;/strong&gt;: in all rules, the left-hand side is only a single nonterminal symbol and right-hand side may be the empty string, or a single terminal symbol, or a single terminal symbol followed by a nonterminal symbol, but nothing else.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Context-free grammars&lt;/strong&gt;: in all rules, the left-hand side of each production rule consists of only a single nonterminal symbol, while the right-hand side may contain any number of terminal and non-terminal symbols.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Context-sensitive grammars&lt;/strong&gt;: rules can contain many terminal and non-terminal characters on both sides.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In our case, all the production rules look very much like the examples we defined above: one character on the left-hand side, at most two on the right-hand side. This means we&amp;rsquo;re dealing with a regular grammar. And this is good news, because it means that this language can be encoded into a &lt;strong&gt;regular expression&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;building-the-regex&#34;&gt;
  Building the regex
  
&lt;/h2&gt;
&lt;p&gt;Regular expressions are a very powerful tool, one that needs to be used with care. They&amp;rsquo;re best used for string validation: given an arbitrary string, they are going to check whether it respects the grammar, i.e. whether the string it could have been generated by applying the rules above.&lt;/p&gt;
&lt;p&gt;Having a regex for our Teranoptia animals will allow us to search for valid animals in long lists of stroings, for example an English dictionary. Such a search would have been prohibitively expensive without a regular expression: using one, while still quite costly, is orders of magnitude more efficient.&lt;/p&gt;
&lt;p&gt;In order to build this complex regex, let&amp;rsquo;s start with a very limited example: a regex that matches left-facing snakes.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-regex&#34; data-lang=&#34;regex&#34;&gt;^(a(b|c|X|Y)*d)+$
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This regex is fairly straightforward: the string must start with a (&lt;span class=&#34;small teranoptia&#34;&gt;a&lt;/span&gt;), can contain any number of b (&lt;span class=&#34;small teranoptia&#34;&gt;b&lt;/span&gt;), c (&lt;span class=&#34;small teranoptia&#34;&gt;c&lt;/span&gt;), X (&lt;span class=&#34;small teranoptia&#34;&gt;X&lt;/span&gt;) and Y (&lt;span class=&#34;small teranoptia&#34;&gt;Y&lt;/span&gt;), and must end with d (&lt;span class=&#34;small teranoptia&#34;&gt;d&lt;/span&gt;). While we&amp;rsquo;re at it, let&amp;rsquo;s add a + to the end, meaning that this pattern can repeat multiple times: the string will simply contain many snakes.&lt;/p&gt;
&lt;h3 id=&#34;left-facing-snakes-regex&#34;&gt;
  Left-facing snakes regex
  
&lt;/h3&gt;


&lt;div style=&#34;display: flex; gap: 10px;&#34;&gt;
    &lt;input id=&#34;left-facing-snakes-input&#34; type=&#34;string&#34; class=&#34;teranoptia&#34; value=&#34;abd&#34; oninput=&#34;validateLeftFacingSnake();&#34;&gt;&lt;/input&gt;
    &lt;p id=&#34;left-facing-snakes-result&#34;&gt;Valid&lt;/p&gt;
&lt;/div&gt;

&lt;script&gt;
var leftFacingSnake = new RegExp(&#34;^(a(b|c|X|Y)*d)+$&#34;);

function validateLeftFacingSnake(){
    const candidate = document.getElementById(&#39;left-facing-snakes-input&#39;).value;
    if (leftFacingSnake.test(candidate)){
        document.getElementById(&#39;left-facing-snakes-input&#39;).style.color = &#34;green&#34;;
        document.getElementById(&#39;left-facing-snakes-result&#39;).innerHTML = &#34;Valid!&#34;;
    } else {
        document.getElementById(&#39;left-facing-snakes-input&#39;).style.color = &#34;red&#34;;
        document.getElementById(&#39;left-facing-snakes-result&#39;).innerHTML = &#34;NOT valid!&#34;;
    }
}
validateLeftFacingSnake()
&lt;/script&gt;


&lt;p&gt;What would it take to extend it to snakes that face either side? Luckily, snake bodies are symmetrical, so we can take advantage of that and write:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-regex&#34; data-lang=&#34;regex&#34;&gt;^((a|W)(b|c|X|Y)*(d|Z))+$
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;naive-snakes&#34;&gt;
  Naive snakes
  
&lt;/h3&gt;


&lt;div style=&#34;display: flex; gap: 10px;&#34;&gt;
    &lt;input id=&#34;naive-snakes-input&#34; type=&#34;string&#34; class=&#34;teranoptia&#34; value=&#34;abdWXZ&#34; oninput=&#34;validateNaiveSnake();&#34;&gt;&lt;/input&gt;
    &lt;p id=&#34;naive-snakes-result&#34;&gt;Valid&lt;/p&gt;
&lt;/div&gt;

&lt;script&gt;
var naiveSnake = new RegExp(&#34;^((a|W)(b|c|X|Y)*(d|Z))+$&#34;);

function validateNaiveSnake(){
    const candidate = document.getElementById(&#39;naive-snakes-input&#39;).value;
    if (naiveSnake.test(candidate)){
        document.getElementById(&#39;naive-snakes-input&#39;).style.color = &#34;green&#34;;
        document.getElementById(&#39;naive-snakes-result&#39;).innerHTML = &#34;Valid!&#34;;
    } else {
        document.getElementById(&#39;naive-snakes-input&#39;).style.color = &#34;red&#34;;
        document.getElementById(&#39;naive-snakes-result&#39;).innerHTML = &#34;NOT valid!&#34;;
    }
}
validateNaiveSnake();
&lt;/script&gt;


&lt;p&gt;That looks super-promising until we realize that there&amp;rsquo;s a problem: this &amp;ldquo;snake&amp;rdquo; &lt;span class=&#34;small teranoptia&#34;&gt;aZ&lt;/span&gt; also matches the regex. To generate well-formed animals we need to keep heads and tails separate. In the regex, it would look like:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-regex&#34; data-lang=&#34;regex&#34;&gt;^(
    (a)(b|c|X|Y)*(d) |
    (W)(b|c|X|Y)*(Z)
)+$
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;correct-snakes&#34;&gt;
  Correct snakes
  
&lt;/h3&gt;


&lt;div style=&#34;display: flex; gap: 10px;&#34;&gt;
    &lt;input id=&#34;correct-snakes-input&#34; type=&#34;string&#34; class=&#34;teranoptia&#34; value=&#34;abdWXZ&#34; oninput=&#34;validateCorrectSnake();&#34;&gt;&lt;/input&gt;
    &lt;p id=&#34;correct-snakes-result&#34;&gt;Valid&lt;/p&gt;
&lt;/div&gt;

&lt;script&gt;
var correctSnake = new RegExp(&#34;^(((a)(b|c|X|Y)*(d))|((W)(b|c|X|Y)*(Z)))+$&#34;);

function validateCorrectSnake(){
    const candidate = document.getElementById(&#39;correct-snakes-input&#39;).value;
    if (correctSnake.test(candidate)){
        document.getElementById(&#39;correct-snakes-input&#39;).style.color = &#34;green&#34;;
        document.getElementById(&#39;correct-snakes-result&#39;).innerHTML = &#34;Valid!&#34;;
    } else {
        document.getElementById(&#39;correct-snakes-input&#39;).style.color = &#34;red&#34;;
        document.getElementById(&#39;correct-snakes-result&#39;).innerHTML = &#34;NOT valid!&#34;;
    }
}
validateCorrectSnake()
&lt;/script&gt;


&lt;p&gt;Once here, building the rest of the regex is simply matter of adding the correct characters to each group. We&amp;rsquo;re gonna trade some extra characters for an easier structure by duplicating the symmetric characters when needed.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-regex&#34; data-lang=&#34;regex&#34;&gt;^(
    // Left-facing animals
    (
        y|v|s|p|m|i|e|a|ź|(|[|{   // Left heads &amp;amp; exiting holes
    )(
        w|t|q|n|k|j|g|f|ḅ|ž|X|Y|b|c|$|«|»  // Left &amp;amp; symmetric bodies
    )*(
        z|x|u|r|o|l|h|d|ż|E|)|]|}  // Left tails &amp;amp; entering holes
    ) |

    // Right facing animals
    (
        A|C|F|I|L|O|S|W|Ź|v|(|[|{   // right tails &amp;amp; exiting holes
    )(
        D|G|J|M|P|Q|T|U|Ẋ|Ž|b|c|X|Y|$|«|»  // right &amp;amp; symmetric bodies  
    )*(
        B|E|H|K|N|R|V|Z|Ż|)|]|}   // right heads &amp;amp; entering holes
    ) |

    // Singletons
    (,|-|*)
)+$
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;well-formed-animals-regex&#34;&gt;
  Well-formed animals regex
  
&lt;/h3&gt;


&lt;div style=&#34;display: flex; gap: 10px;&#34;&gt;
    &lt;input id=&#34;correct-animal-input&#34; type=&#34;string&#34; class=&#34;teranoptia&#34; value=&#34;abu*W«XZ&#34; oninput=&#34;validateCorrectAnimal();&#34;&gt;&lt;/input&gt;
    &lt;p id=&#34;correct-animal-result&#34;&gt;Valid&lt;/p&gt;
&lt;/div&gt;

&lt;script&gt;
var correctAnimal = new RegExp(&#34;^((y|v|s|p|m|i|e|a|ź|\\(|\\[|\\{)(w|t|q|n|k|j|g|f|ḅ|ž|b|c|X|Y|\\$|«|»)*(z|x|u|r|o|l|h|d|ż|E|\\)|\\]|\\})|(A|C|F|I|L|O|S|W|Ź|v|\\(|\\[|\\{)(D|G|J|M|P|Q|T|U|Ẋ|Ž|b|c|X|Y|\\$|«|»)*(B|E|H|K|N|R|V|Z|Ż|\\)|\\]|\\})|(-|\\*|,))+$&#34;);

function validateCorrectAnimal(){
    const candidate = document.getElementById(&#39;correct-animal-input&#39;).value;
    if (correctAnimal.test(candidate)){
        document.getElementById(&#39;correct-animal-input&#39;).style.color = &#34;green&#34;;
        document.getElementById(&#39;correct-animal-result&#39;).innerHTML = &#34;Valid!&#34;;
    } else {
        document.getElementById(&#39;correct-animal-input&#39;).style.color = &#34;red&#34;;
        document.getElementById(&#39;correct-animal-result&#39;).innerHTML = &#34;NOT valid!&#34;;
    }
}
validateCorrectAnimal();
&lt;/script&gt;


&lt;p&gt;If you play with the above regex, you&amp;rsquo;ll notice a slight discrepancy with what our well-formed animal generator creates. The generator can create &amp;ldquo;double-headed&amp;rdquo; monsters where a symmetric body part is inserted, like &lt;span class=&#34;small teranoptia&#34;&gt;a«Z&lt;/span&gt;. However, the regex does not allow it. Extending it to account for these scenarios would make it even more unreadable, so this is left as an exercise for the reader.&lt;/p&gt;
&lt;h2 id=&#34;searching-for-monstrous-words&#34;&gt;
  Searching for &amp;ldquo;monstrous&amp;rdquo; words
  
&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s put the regex to use! There must be some English words that match the regex, right?&lt;/p&gt;
&lt;p&gt;Google helpfully compiled a text file with the most frequent 10.000 English words by frequency. Let&amp;rsquo;s load it up and match every line with our brand-new regex. Unfortunately Teranoptia is case-sensitive and uses quite a few odd letters and special characters, so it&amp;rsquo;s unlikely we&amp;rsquo;re going to find many interesting creatures. Still worth an attempt.&lt;/p&gt;
&lt;h3 id=&#34;monster-search&#34;&gt;
  Monster search
  
&lt;/h3&gt;


&lt;div style=&#34;display: flex; gap: 10px;&#34;&gt;
    &lt;input id=&#34;file-url&#34; type=&#34;url&#34; value=&#34;https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english.txt&#34; style=&#34;width: 100%;&#34;&gt;&lt;/input&gt;
    &lt;button onclick=&#34;searchFile();&#34;&gt;Search&lt;/button&gt;
&lt;/div&gt;
&lt;p id=&#34;search-result&#34;&gt;&lt;/p&gt;
&lt;div id=&#34;words-found&#34;&gt;&lt;/div&gt;

&lt;script&gt;
var correctAnimal = new RegExp(&#34;^((y|v|s|p|m|i|e|a|ź|\\(|\\[|\\{)(w|t|q|n|k|j|g|f|ḅ|ž|b|c|X|Y|\\$|«|»)*(z|x|u|r|o|l|h|d|ż|E|\\)|\\]|\\})|(A|C|F|I|L|O|S|W|Ź|v|\\(|\\[|\\{)(D|G|J|M|P|Q|T|U|Ẋ|Ž|b|c|X|Y|\\$|«|»)*(B|E|H|K|N|R|V|Z|Ż|\\)|\\]|\\})|(-|\\*|,))+$&#34;);

function searchFile(){
    document.getElementById(&#39;search-result&#39;).innerHTML = &#34;Loading...&#34;;

    fetch(document.getElementById(&#39;file-url&#39;).value)
    .then((response) =&gt; {
        if (!response.ok) {
            throw new Error(`HTTP error: ${response.status}`);
        }
        return response.text();
    })
    .then((text) =&gt; {
        lines = text.split(&#39;\n&#39;);
        counter = 0;

        for (i = 0; i &lt; lines.length; i++){
            var candidate = lines[i];
            document.getElementById(&#39;search-result&#39;).innerHTML = &#34;Checking &#34; + candidate;
            if (correctAnimal.test(candidate)){
                document.getElementById(&#39;words-found&#39;).innerHTML += &#34;&lt;p&gt;&#34;+candidate+&#34;&lt;span class=&#39;teranoptia&#39;&gt; &#34;+candidate+&#34;&lt;/span&gt;&lt;/p&gt;&#34;;
                counter++;
            }
        }
        document.getElementById(&#39;search-result&#39;).innerHTML = &#34;Done! Found &#34;+ counter +&#34; animals over &#34;+lines.length+&#34; words tested.&#34;;        
    })
    .catch((error) =&gt; {
        document.getElementById(&#39;search-result&#39;).innerHTML = &#34;Failed to fetch file :(&#34;;
    });
}
&lt;/script&gt;


&lt;p&gt;Go ahead and put your own vocabulary file to see if your language contains more animals!&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;
  Conclusion
  
&lt;/h2&gt;
&lt;p&gt;In this post I&amp;rsquo;ve just put together a few exercises for fun, but these tools can be great for teaching purposes: the output is very easy to validate visually, and the grammar involved, while not trivial, is not as complex as natural language or as dry as numerical sequences. If you need something to keep your students engaged, this might be a simple trick to help them visualize the concepts better.&lt;/p&gt;
&lt;p&gt;On my side, I think I&amp;rsquo;m going to use these neat little monsters as weird &lt;a href=&#34;https://en.wikipedia.org/wiki/Fleuron_%28typography%29&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fleurons&lt;/a&gt; :)&lt;/p&gt;
&lt;p class=&#34;fleuron&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-05-06-teranoptia/&#34;&gt;su&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Download Teranoptia at this link: &lt;a href=&#34;https://www.tunera.xyz/fonts/teranoptia/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.tunera.xyz/fonts/teranoptia/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>RAG, the bad parts (and the good!)</title>
      <link>https://www.zansara.dev/posts/2024-04-29-odsc-east-rag/</link>
      <pubDate>Mon, 29 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2024-04-29-odsc-east-rag/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;
  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-04-29-odsc-east-rag/RAG,%20the%20bad%20parts%20%28and%20the%20good!%29%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/1e960694-4227-4da4-9a79-158d4ab1fd35?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;
  &lt;audio controls style=&#34;width:100%;&#34;&gt;
    &lt;source src=&#34;https://www.zansara.dev/posts/2024-04-29-odsc-east-rag/RAG,%20the%20bad%20parts%20%28and%20the%20good!%29%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt; 
  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;
    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;
    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;This is a writeup of my talk at &lt;a href=&#34;https://www.zansara.dev/talks/2024-04-25-odsc-east-rag/&#34; &gt;ODSC East 2024&lt;/a&gt; and &lt;a href=&#34;https://www.zansara.dev/talks/2024-07-10-europython-rag/&#34; &gt;EuroPython 2024&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;If you&amp;rsquo;ve been at any AI or Python conference this year, there&amp;rsquo;s one acronym that you&amp;rsquo;ve probably heard in nearly every talk: it&amp;rsquo;s RAG. RAG is one of the most used techniques to enhance LLMs in production, but why is it so? And what are its weak points?&lt;/p&gt;
&lt;p&gt;In this post, we will first describe what RAG is and how it works at a high level. We will then see what type of failures we may encounter, how they happen, and a few reasons that may trigger these issues. Next, we will look at a few tools to help us evaluate a RAG application in production. Last, we&amp;rsquo;re going to list a few techniques to enhance your RAG app and make it more capable in a variety of scenarios.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s dive in.&lt;/p&gt;
&lt;h1 id=&#34;outline&#34;&gt;
  Outline
  
&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-rag&#34; &gt;What is RAG?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#why-should-i-use-it&#34; &gt;Why should I use it?&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#a-weather-chatbot&#34; &gt;A weather chatbot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-real-world-example&#34; &gt;A real-world example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#failure-modes&#34; &gt;Failure modes&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#retrieval-failure&#34; &gt;Retrieval failure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#generation-failure&#34; &gt;Generation failure&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#evaluation-strategies&#34; &gt;Evaluation strategies&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#evaluating-retrieval&#34; &gt;Evaluating Retrieval&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#evaluating-generation&#34; &gt;Evaluating Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#end-to-end-evaluation&#34; &gt;End-to-end evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#putting-it-all-together&#34; &gt;Putting it all together&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#advanced-flavors-of-rag&#34; &gt;Advanced flavors of RAG&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#use-multiple-retrievers&#34; &gt;Use multiple retrievers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#self-correction&#34; &gt;Self-correction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#agentic-rag&#34; &gt;Agentic RAG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multihop-rag&#34; &gt;Multihop RAG&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-word-on-finetuning&#34; &gt;A word on finetuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34; &gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;what-is-rag&#34;&gt;
  What is RAG?
  
&lt;/h1&gt;
&lt;p&gt;RAG stands for &lt;strong&gt;R&lt;/strong&gt;etrieval &lt;strong&gt;A&lt;/strong&gt;ugmented &lt;strong&gt;G&lt;/strong&gt;eneration, which can be explained as: &amp;ldquo;A technique to &lt;strong&gt;augment&lt;/strong&gt; LLM’s knowledge beyond its training data by &lt;strong&gt;retrieving&lt;/strong&gt; contextual information before a &lt;strong&gt;generating&lt;/strong&gt; an answer.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-04-29-odsc-east-rag/rag-diagram.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;RAG is a technique that works best for question-answering tasks, such as chatbots or similar knowledge extraction applications. This means that the user of a RAG app is a user who needs an answer to a question.&lt;/p&gt;
&lt;p&gt;The first step of RAG is to take the question and hand it over to a component called &lt;a href=&#34;https://docs.haystack.deepset.ai/docs/retrievers?utm_campaign=odsc-east&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;retriever&lt;/strong&gt;&lt;/a&gt;. A retriever is any system that, given a question, can find data relevant to the question within a vast dataset, be it text, images, rows in a DB, or anything else.&lt;/p&gt;
&lt;p&gt;When implementing RAG, many developers think immediately that a vector database is necessary for retrieval. While vector databases such as &lt;a href=&#34;https://haystack.deepset.ai/integrations/qdrant-document-store?utm_campaign=odsc-east&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Qdrant&lt;/a&gt;, &lt;a href=&#34;https://haystack.deepset.ai/integrations/chroma-documentstore?utm_campaign=odsc-east&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ChromaDB&lt;/a&gt;, &lt;a href=&#34;https://haystack.deepset.ai/integrations/weaviate-document-store?utm_campaign=odsc-east&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Weaviate&lt;/a&gt; and so on, are great for retrieval in some applications, they&amp;rsquo;re not the only option. Keyword-based algorithms such as &lt;a href=&#34;https://haystack.deepset.ai/integrations/elasticsearch-document-store?utm_campaign=odsc-east&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Elasticsearch BM25&lt;/a&gt; or TF-IDF can be used as retrievers in a RAG application, and you can even go as far as using a &lt;a href=&#34;https://docs.haystack.deepset.ai/docs/websearch?utm_campaign=odsc-east&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;web search engine API&lt;/a&gt;, such as Google or Bing. Anything that is given a question and can return information relevant to the question can be used here.&lt;/p&gt;
&lt;p&gt;Once our retriever sifted through all the data and returned a few relevant snippets of context, the question and the context are assembled into a &lt;strong&gt;RAG prompt&lt;/strong&gt;. It looks like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Read the text below and answer the question at the bottom.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Text: [all the text found by the retriever]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Question: [the user&amp;#39;s question]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This prompt is then fed to the last component, called a &lt;a href=&#34;https://docs.haystack.deepset.ai/docs/components_overview#generators?utm_campaign=odsc-east&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;generator&lt;/strong&gt;&lt;/a&gt;. A generator is any system that, given a prompt, can answer the question that it contains. In practice, &amp;ldquo;generator&amp;rdquo; is an umbrella term for any LLM, be it behind an API like GPT-3.5 or running locally, such as a Llama model. The generator receives the prompt, reads and understands it, and then writes down an answer that can be given back to the user, closing the loop.&lt;/p&gt;
&lt;h1 id=&#34;why-should-i-use-it&#34;&gt;
  Why should I use it?
  
&lt;/h1&gt;
&lt;p&gt;There are three main benefits of using a RAG architecture for your LLM apps instead of querying the LLM directly.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reduces hallucinations&lt;/strong&gt;. The RAG prompt contains the answer to the user&amp;rsquo;s question together with the question, so the LLM doesn&amp;rsquo;t need to &lt;em&gt;know&lt;/em&gt; the answer, but it only needs to read the prompt and rephrase a bit of its content.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Allows access to fresh data&lt;/strong&gt;. RAG makes LLMs capable of reasoning about data that wasn&amp;rsquo;t present in their training set, such as highly volatile figures, news, forecasts, and so on.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Increases transparency&lt;/strong&gt;. The retrieval step is much easier to inspect than LLM&amp;rsquo;s inference process, so it&amp;rsquo;s far easier to spot and fact-check any answer the LLM provides.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To understand these points better, let&amp;rsquo;s see an example.&lt;/p&gt;
&lt;h2 id=&#34;a-weather-chatbot&#34;&gt;
  A weather chatbot
  
&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;re making a chatbot for a weather forecast app. Suppose the user asks an LLM directly, &amp;ldquo;Is it going to rain in Lisbon tomorrow morning?&amp;rdquo;. In that case, the LLM will make up a random answer because it obviously didn&amp;rsquo;t have tomorrow&amp;rsquo;s weather forecast for Lisbon in its training set and knows nothing about it.&lt;/p&gt;
&lt;p&gt;When an LLM is queried with a direct question, it will use its internal knowledge to answer it. LLMs have read the entire Internet during their training phase, so they learned that whenever they saw a line such as &amp;ldquo;What&amp;rsquo;s the capital of France?&amp;rdquo;, the string &amp;ldquo;Paris&amp;rdquo; always appeared among the following few words. So when a user asks the same question, the answer will likely be &amp;ldquo;Paris&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;This &amp;ldquo;recalling from memory&amp;rdquo; process works for well-known facts but is not always practical. For more nuanced questions or something that the LLM hasn&amp;rsquo;t seen during training, it often fails: in an attempt to answer the question, the LLM will make up a response that is not based on any real source. This is called a &lt;strong&gt;hallucination&lt;/strong&gt;, one of LLMs&amp;rsquo; most common and feared failure modes.&lt;/p&gt;
&lt;p&gt;RAG helps prevent hallucinations because, in the RAG prompt, the question and all the data needed to answer it are explicitly given to the LLM. For our weather chatbot, the retriever will first do a Google search and find some data. Then, we will put together the RAG prompt. The result will look like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Read the text below and answer the question at the bottom.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Text: According to the weather forecast, the weather in Lisbon tomorrow 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;is expected to be mostly sunny, with a high of 18°C and a low of 11°C. 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;There is a 25% chance of showers in the evening.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Question: Is it going to rain in Lisbon tomorrow morning?
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now, it&amp;rsquo;s clear that the LLM doesn&amp;rsquo;t have to recall anything about the weather in Lisbon from its memory because the prompt already contains the answer. The LLM only needs to rephrase the context. This makes the task much simpler and drastically reduces the chances of hallucinations.&lt;/p&gt;
&lt;p&gt;In fact, RAG is the only way to build an LLM-powered system that can answer a question like this with any confidence at all. Retraining an LLM every morning with the forecast for the day would be a lot more wasteful, require a ton of data, and won&amp;rsquo;t return consistent results. Imagine if we were making a chatbot that gives you figures from the stock market!&lt;/p&gt;
&lt;p&gt;In addition, a weather chatbot built with RAG &lt;strong&gt;can be fact-checked&lt;/strong&gt;. If users have access to the web pages that the retriever found, they can check the pages directly when the results are not convincing, which helps build trust in the application.&lt;/p&gt;
&lt;h2 id=&#34;a-real-world-example&#34;&gt;
  A real-world example
  
&lt;/h2&gt;
&lt;p&gt;If you want to compare a well-implemented RAG system with a plain LLM, you can put &lt;a href=&#34;https://chat.openai.com/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ChatGPT&lt;/a&gt; (the free version, powered by GPT-3.5) and &lt;a href=&#34;https://www.perplexity.ai/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perplexity&lt;/a&gt; to the test. ChatGPT does not implement RAG, while Perplexity is one of the most effective implementations existing today.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s ask both: &amp;ldquo;Where does ODSC East 2024 take place?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;ChatGPT says:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-04-29-odsc-east-rag/chatgpt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;While Perplexity says:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-04-29-odsc-east-rag/perplexity-ai.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Note how ChatGPT clearly says that it doesn&amp;rsquo;t know: this is better than many other LLMs, which would just make up a place and date. On the contrary, Perplexity states some specific facts, and in case of doubt it&amp;rsquo;s easy to verify that it&amp;rsquo;s right by simply checking the sources above. Even just looking at the source&amp;rsquo;s URL can give users a lot more confidence in whether the answer is grounded.&lt;/p&gt;
&lt;h1 id=&#34;failure-modes&#34;&gt;
  Failure modes
  
&lt;/h1&gt;
&lt;p&gt;Now that we understand how RAG works, let&amp;rsquo;s see what can go wrong in the process.&lt;/p&gt;
&lt;p&gt;As we&amp;rsquo;ve just described, a RAG app goes in two steps &amp;ndash; retrieval and generation. Therefore, we can classify RAG failures into two broad categories:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Retrieval failures&lt;/strong&gt;: The retriever component fails to find the correct context for the given question. The RAG prompt injects irrelevant noise into the prompt, which confuses the LLM and results in a wrong or unrelated answer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generation failures&lt;/strong&gt;: The LLM fails to produce a correct answer even with a proper RAG prompt containing a question and all the data needed to answer it.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To understand them better, let&amp;rsquo;s pretend an imaginary user poses our application the following question about a &lt;a href=&#34;https://en.wikipedia.org/wiki/Republic_of_Rose_Island&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;little-known European microstate&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;What was the official language of the Republic of Rose Island?
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here is what would happen in an ideal case:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-04-29-odsc-east-rag/successful-query.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;First, the retriever searches the dataset (let&amp;rsquo;s imagine, in this case, Wikipedia) and returns a few snippets. The retriever did a good job here, and the snippets contain clearly stated information about the official language of Rose Island. The LLM reads these snippets, understands them, and replies to the user (correctly):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;The official language of the Republic of Rose Island was Esperanto.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;retrieval-failure&#34;&gt;
  Retrieval failure
  
&lt;/h2&gt;
&lt;p&gt;What would happen if the retrieval step didn&amp;rsquo;t go as planned?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-04-29-odsc-east-rag/retrieval-failure.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here, the retriever finds some information about Rose Island, but none of the snippets contain any information about the official language. They only say where it was located, what happened to it, and so on. So the LLM, which knows nothing about this nation except what the prompt says, takes an educated guess and replies:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;The official language of the Republic of Rose Island was Italian.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The wrong answer here is none of the LLM&amp;rsquo;s fault: the retriever is the component to blame.&lt;/p&gt;
&lt;p&gt;When and why can retrieval fail? There are as many answers to this question as retrieval methods, so each should be inspected for its strengths and weaknesses. However there are a few reasons that are common to most of them.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The relevant data does not exist in the database&lt;/strong&gt;. When the data does not exist, it&amp;rsquo;s impossible to retrieve it. Many retrieval techniques, however, give a relevance score to each result that they return, so filtering out low-relevance snippets may help mitigate the issue.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The retrieval algorithm is too naive to match a question with its relevant context&lt;/strong&gt;. This is a common issue for keyword-based retrieval methods such as TF-IDF or BM25 (Elasticsearch). These algorithms can&amp;rsquo;t deal with synonims or resolve acronyms, so if the question and the relevant context don&amp;rsquo;t share the exact same words, the retrieval won&amp;rsquo;t work.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Embedding model (if used) is too small or unsuitable for the data&lt;/strong&gt;. The data must be embedded before being searchable when doing a vector-based search. &amp;ldquo;Embedded&amp;rdquo; means that every snippet of context is associated with a list of numbers called an &lt;strong&gt;embedding&lt;/strong&gt;. The quality of the embedding then determines the quality of the retrieval. If you embed your documents with a naive embedding model, or if you are dealing with a very specific domain such as narrow medical and legal niches, the embedding of your data won&amp;rsquo;t be able to represent their content precisely enough for the retrieval to be successful.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The data is not chunked properly (too big or too small chunks)&lt;/strong&gt;. Retrievers thrive on data that is chunked properly. Huge blocks of text will be found relevant to almost any question and will drown the LLM with information. Too small sentences or sentence fragments won&amp;rsquo;t carry enough context for the LLM to benefit from the retriever&amp;rsquo;s output. Proper chunking can be a huge lever to improve the quality of your retrieval.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The data and the question are in different languages&lt;/strong&gt;. Keyword-based retrieval algorithms suffer from this issue the most because keywords in different languages rarely match. If you expect questions to come in a different language than the data you are retrieving from, consider adding a translation step or performing retrieval with a multilingual embedder instead.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One caveat with retrieval failures is that if you&amp;rsquo;re using a very powerful LLM such as GPT-4, sometimes your LLM is smart enough to understand that the retrieved context is incorrect and will discard it, &lt;strong&gt;hiding the failure&lt;/strong&gt;. This means that it&amp;rsquo;s even more important to make sure retrieval is working well in isolation, something we will see in a moment.&lt;/p&gt;
&lt;h2 id=&#34;generation-failure&#34;&gt;
  Generation failure
  
&lt;/h2&gt;
&lt;p&gt;Assuming that retrieval was successful, what would happen if the LLM still hallucinated?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-04-29-odsc-east-rag/generation-failure.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This is clearly an issue with our LLM: even when given all the correct data, the LLM still generated a wrong answer. Maybe our LLM doesn&amp;rsquo;t know that Esperanto is even a language? Or perhaps we&amp;rsquo;re using an LLM that doesn&amp;rsquo;t understand English well?&lt;/p&gt;
&lt;p&gt;Naturally, each LLM will have different weak points that can trigger issues like these. Here are some common reasons why you may be getting generation failures.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The model is too small and can’t follow instructions well&lt;/strong&gt;. When building in a resource-constrained environment (such as local smartphone apps or IoT), the choice of LLMs shrinks to just a few tiny models. However, the smaller the model, the less it will be able to understand natural language, and even when it does, it limits its ability to follow instructions. If you notice that your model consistently doesn&amp;rsquo;t pay enough attention to the question when answering it, consider switching to a larger or newer LLM.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The model knows too little about the domain to even understand the question&lt;/strong&gt;. This can happen if your domain is highly specific, uses specific terminology, or relies on uncommon acronyms. Models are trained on general-purpose text, so they might not understand some questions without finetuning, which helps specify the meaning of the most critical key terms and acronyms. When the answers given by your model somewhat address the question but miss the point entirely and stay generic or hand-wavy, this is likely the case.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The model is not multilingual, but the questions and context may be&lt;/strong&gt;. It&amp;rsquo;s essential that the model understands the question being asked in order to be able to answer it. The same is true for context: if the data found by the retriever is in a language that the LLM cannot understand, it won&amp;rsquo;t help it answer and might even confuse it further. Always make sure that your LLM understands the languages your users use.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The RAG prompt is not built correctly&lt;/strong&gt;. Some LLMs, especially older or smaller ones, may be very sensitive to how the prompt is built. If your model ignores part of the context or misses the question, the prompt might contain contradicting information, or it might be simply too large. LLMs are not always great at &lt;a href=&#34;https://cs.stanford.edu/~nfliu/papers/lost-in-the-middle.arxiv2023.pdf&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;finding a needle in the haystack&lt;/a&gt;: if you are consistently building huge RAG prompts and you observe generation issues, consider cutting it back to help the LLM focus on the data that actually contains the answer.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-04-29-odsc-east-rag/lost-in-the-middle.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;evaluation-strategies&#34;&gt;
  Evaluation strategies
  
&lt;/h1&gt;
&lt;p&gt;Once we put our RAG system in production, we should keep an eye on its performance at scale. This is where evaluation frameworks come into play.&lt;/p&gt;
&lt;p&gt;To properly evaluate the performance of RAG, it&amp;rsquo;s best to perform two evaluation steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Isolated Evaluation&lt;/strong&gt;. Being a two-step process, failures at one stage can hide or mask the other, so it&amp;rsquo;s hard to understand where the failures originate from. To address this issue, evaluate the retrieval and generation separately: both must work well in isolation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;End to end evaluation&lt;/strong&gt;. To ensure the system works well from start to finish, it&amp;rsquo;s best to evaluate it as a whole. End-to-end evaluation brings its own set of challenges, but it correlates more directly to the quality of the overall app.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;evaluating-retrieval&#34;&gt;
  Evaluating Retrieval
  
&lt;/h2&gt;
&lt;p&gt;Each retrieval method has its own state-of-the-art evaluation method and framework, so it&amp;rsquo;s usually best to refer to those.&lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;keyword-based&lt;/strong&gt; retrieval algorithms such as TD-IDF, BM25, PageRank, and so on, evaluation is often done by checking the keywords match well. For this, you can use &lt;a href=&#34;https://en.wikipedia.org/wiki/Evaluation_measures_%28information_retrieval%29&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;one of the many metrics&lt;/a&gt; used for this purpose: &lt;a href=&#34;https://en.wikipedia.org/wiki/Precision_and_recall&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recall, precision&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/F-score&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;F1&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Mean_reciprocal_rank&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MRR&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Evaluation_measures_%28information_retrieval%29#Mean_average_precision&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MAP&lt;/a&gt;, …&lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;vector-based&lt;/strong&gt; retrievers like vector DBs, the evaluation is more tricky because checking for matching keywords is not sufficient: the semantics of the question and the answer must evaluated for similarity. We are going to see some libraries that help with this when evaluating generation: in short, they use another LLM to judge the similarity or compute metrics like &lt;a href=&#34;https://docs.ragas.io/en/latest/concepts/metrics/semantic_similarity.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;semantic similarity&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;evaluating-generation&#34;&gt;
  Evaluating Generation
  
&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-04-29-odsc-east-rag/uptrain-logo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Evaluating an LLM&amp;rsquo;s answers to a question is still a developing art, and several libraries can help with the task. One commonly used framework is &lt;a href=&#34;https://haystack.deepset.ai/integrations/uptrain?utm_campaign=odsc-east&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UpTrain&lt;/a&gt;, which implements an &amp;ldquo;LLM-as-a-judge&amp;rdquo; approach. This means that the answers given by an LLM are then evaluated by another LLM, normally a larger and more powerful model.&lt;/p&gt;
&lt;p&gt;This approach has the benefit that responses are not simply checked strictly for the presence or absence of keywords but can be evaluated according to much more sophisticated criteria like &lt;a href=&#34;https://docs.uptrain.ai/predefined-evaluations/response-quality/response-completeness&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;completeness&lt;/a&gt;, &lt;a href=&#34;https://docs.uptrain.ai/predefined-evaluations/response-quality/response-conciseness&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;conciseness&lt;/a&gt;, &lt;a href=&#34;https://docs.uptrain.ai/predefined-evaluations/response-quality/response-relevance&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;relevance&lt;/a&gt;, &lt;a href=&#34;https://docs.uptrain.ai/predefined-evaluations/context-awareness/factual-accuracy&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;factual accuracy&lt;/a&gt;, &lt;a href=&#34;https://docs.uptrain.ai/predefined-evaluations/conversation-evals/user-satisfaction&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;conversation quality&lt;/a&gt;, and more.&lt;/p&gt;
&lt;p&gt;This approach leads to a far more detailed view of what the LLM is good at and what aspects of the generation could or should be improved. The criteria to select depend strongly on the application: for example, in medical or legal apps, factual accuracy should be the primary metric to optimize for, while in customer support, user satisfaction and conversation quality are also essential. For personal assistants, it&amp;rsquo;s usually best to focus on conciseness, and so on.&lt;/p&gt;
&lt;div class=&#34;notice info&#34;&gt;
  &lt;div class=&#34;notice-content&#34;&gt;💡 &lt;em&gt;UpTrain can also be used to evaluate RAG applications end-to-end. Check &lt;a href=&#34;https://docs.uptrain.ai/getting-started/introduction&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;its documentation&lt;/a&gt; for details.&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;end-to-end-evaluation&#34;&gt;
  End-to-end evaluation
  
&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-04-29-odsc-east-rag/ragas-logo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The evaluation of RAG systems end-to-end is also quite complex and can be implemented in many ways, depending on the aspect you wish to monitor. One of the simplest approaches is to focus on semantic similarity between the question and the final answer.&lt;/p&gt;
&lt;p&gt;A popular framework that can be used for such high-level evaluation is &lt;a href=&#34;https://haystack.deepset.ai/integrations/ragas?utm_campaign=odsc-east&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RAGAS&lt;/a&gt;. In fact, RAGAS offers two interesting metrics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.ragas.io/en/stable/concepts/metrics/semantic_similarity.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Answer semantic similarity&lt;/strong&gt;&lt;/a&gt;. This is computed simply by taking the cosine similarity between the answer and the ground truth.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.ragas.io/en/stable/concepts/metrics/answer_correctness.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Answer correctness&lt;/strong&gt;&lt;/a&gt;. Answer correctness is defined as a weighted average of the semantic similarity and the F1 score between the generated answer and the ground truth. This metric is more oriented towards fact-based answers, where F1 can help ensure that relevant facts such as dates, names, and so on are explicitly stated.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On top of evaluation metrics, RAGAS also offers the capability to build &lt;a href=&#34;https://docs.ragas.io/en/stable/concepts/testset_generation.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;synthetic evaluation datasets&lt;/a&gt; to evaluate your app against. Such datasets spare you the work-intensive process of building a real-world evaluation dataset with human-generated questions and answers but also trade high quality for volume and speed. If your domain is very specific or you need extreme quality, synthetic datasets might not be an option, but for most real-world apps, such datasets can save tons of labeling time and resources.&lt;/p&gt;
&lt;div class=&#34;notice info&#34;&gt;
  &lt;div class=&#34;notice-content&#34;&gt;💡 &lt;em&gt;RAGAS can also be used to evaluate each step of a RAG application in isolation. Check &lt;a href=&#34;https://docs.ragas.io/en/stable/getstarted/index.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;its documentation&lt;/a&gt; for details.&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;notice info&#34;&gt;
  &lt;div class=&#34;notice-content&#34;&gt;&lt;p&gt;💡 &lt;em&gt;I recently discovered an even more comprehensive framework for end-to-end evaluation called &lt;a href=&#34;https://docs.relari.ai/v0.3&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;continuous-eval&lt;/strong&gt;&lt;/a&gt; from &lt;a href=&#34;https://relari.ai/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Relari.ai&lt;/a&gt;, which focuses on modular evaluation of RAG pipelines. Check it out if you&amp;rsquo;re interested in this topic and RAGAS doesn&amp;rsquo;t offer enough flexibility for your use case.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-04-29-odsc-east-rag/relari-logo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;putting-it-all-together&#34;&gt;
  Putting it all together
  
&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-04-29-odsc-east-rag/haystack-logo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once you know how you want to evaluate your app, it&amp;rsquo;s time to put it together. A convenient framework for this step is &lt;a href=&#34;https://haystack.deepset.ai/?utm_campaign=odsc-east&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haystack&lt;/a&gt;, a Python open-source LLM framework focused on building RAG applications. Haystack is an excellent choice because it can be used through all stages of the application lifecycle, from prototyping to production, including evaluation.&lt;/p&gt;
&lt;p&gt;Haystack supports several evaluation libraries including &lt;a href=&#34;https://haystack.deepset.ai/integrations/uptrain?utm_campaign=odsc-east&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UpTrain&lt;/a&gt;, &lt;a href=&#34;https://haystack.deepset.ai/integrations/ragas?utm_campaign=odsc-east&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RAGAS&lt;/a&gt; and &lt;a href=&#34;https://haystack.deepset.ai/integrations/deepeval?utm_campaign=odsc-east&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepEval&lt;/a&gt;. To understand more about how to implement and evaluate a RAG application with it, check out their tutorial about model evaluation &lt;a href=&#34;https://haystack.deepset.ai/tutorials/35_model_based_evaluation_of_rag_pipelines?utm_campaign=odsc-east&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;advanced-flavors-of-rag&#34;&gt;
  Advanced flavors of RAG
  
&lt;/h1&gt;
&lt;p&gt;Once our RAG app is ready and deployed in production, the natural next step is to look for ways to improve it even further. RAG is a very versatile technique, and many different flavors of &amp;ldquo;advanced RAG&amp;rdquo; have been experimented with, many more than I can list here. Depending on the situation, you may focus on different aspects, so let&amp;rsquo;s list some examples of tactics you can deploy to make your pipeline more powerful, context-aware, accurate, and so on.&lt;/p&gt;
&lt;h2 id=&#34;use-multiple-retrievers&#34;&gt;
  Use multiple retrievers
  
&lt;/h2&gt;
&lt;p&gt;Sometimes, a RAG app needs access to vastly different types of data simultaneously. For example, a personal assistant might need access to the Internet, your Slack, your emails, your personal notes, and maybe even your pictures. Designing a single retriever that can handle data of so many different kinds is possible. Still, it can be a real challenge and require, in many cases, an entire data ingestion pipeline.&lt;/p&gt;
&lt;p&gt;Instead of going that way, you can instead use multiple retrievers, each specialized to a specific subset of your data: for example, one retriever that browses the web, one that searches on Slack and in your emails, one that checks for relevant pictures.&lt;/p&gt;
&lt;p&gt;When using many retrievers, however, it&amp;rsquo;s often best to introduce another step called &lt;strong&gt;reranking&lt;/strong&gt;. A reranker double-checks that all the results returned by each retriever are actually relevant and sorts them again before the RAG prompt is built. Rerankers are usually much more precise than retrievers in assessing the relative importance of various snippets of context, so they can dramatically improve the quality of the pipeline. In exceptional cases, they can be helpful even in RAG apps with a single retriever.&lt;/p&gt;
&lt;p&gt;Here is an &lt;a href=&#34;https://haystack.deepset.ai/tutorials/33_hybrid_retrieval?utm_campaign=odsc-east&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;example&lt;/a&gt; of such a pipeline built with Haystack.&lt;/p&gt;
&lt;h2 id=&#34;self-correction&#34;&gt;
  Self-correction
  
&lt;/h2&gt;
&lt;p&gt;We mentioned that one of the most common evaluation strategies for RAG output is &amp;ldquo;LLM-as-a-judge&amp;rdquo;: the idea of using another LLM to evaluate the answer of the first. However, why use this technique only for evaluation?&lt;/p&gt;
&lt;p&gt;Self-correcting RAG apps add one extra step at the end of the pipeline: they take the answer, pass it to a second LLM, and ask it to assess whether the answer is likely to be correct. If the check fails, the second LLM will provide some feedback on why it believes the answer is wrong, and this feedback will be given back to the first LLM to try answering another time until an agreement is reached.&lt;/p&gt;
&lt;p&gt;Self-correcting LLMs can help improve the accuracy of the answers at the expense of more LLM calls per user question.&lt;/p&gt;
&lt;h2 id=&#34;agentic-rag&#34;&gt;
  Agentic RAG
  
&lt;/h2&gt;
&lt;p&gt;In the LLMs field, the term &amp;ldquo;agent&amp;rdquo; or &amp;ldquo;agentic&amp;rdquo; is often used to identify systems that use LLMs to make decisions. In the case of a RAG application, this term refers to a system that does not always perform retrieval but decides whether to perform it by reading the question first.&lt;/p&gt;
&lt;p&gt;For example, imagine we&amp;rsquo;re building a RAG app to help primary school children with their homework. When the question refers to topics like history or geography, RAG is very helpful to avoid hallucinations. However, if the question regards math, the retrieval step is entirely unnecessary, and it might even confuse the LLM by retrieving similar math problems with different answers.&lt;/p&gt;
&lt;p&gt;Making your RAG app agentic is as simple as giving the question to an LLM before retrieval in a prompt such as:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Reply YES if the answer to this question should include facts and 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;figures, NO otherwise.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Question: What&amp;#39;s the capital of France?
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then, retrieval is run or skipped depending on whether the answer is YES or NO.&lt;/p&gt;
&lt;p&gt;This is the most basic version of agentic RAG. Some advanced LLMs can do better: they support so-called &amp;ldquo;function calling,&amp;rdquo; which means that they can tell you exactly how to invoke the retriever and even provide specific parameters instead of simply answering YES or NO.&lt;/p&gt;
&lt;p&gt;For more information about function calling with LLMs, check out &lt;a href=&#34;https://platform.openai.com/docs/guides/function-calling&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI&amp;rsquo;s documentation&lt;/a&gt; on the topic or the equivalent documentation of your LLM provider.&lt;/p&gt;
&lt;h2 id=&#34;multihop-rag&#34;&gt;
  Multihop RAG
  
&lt;/h2&gt;
&lt;p&gt;Multihop RAG is an even more complex version of agentic RAG. Multihop pipelines often use &lt;strong&gt;chain-of-thought prompts&lt;/strong&gt;, a type of prompt that looks like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;You are a helpful and knowledgeable agent.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;To answer questions, you&amp;#39;ll need to go through multiple steps involving step-by-step 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;thinking and using a search engine to do web searches. The browser will respond with 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;snippets of text from web pages. When you are ready for a final answer, respond with 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;`Final Answer:`&lt;/span&gt;.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Use the following format:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;-&lt;/span&gt; Question: the question to be answered
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;-&lt;/span&gt; Thought: Reason if you have the final answer. If yes, answer the question. If not, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    find out the missing information needed to answer it.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;-&lt;/span&gt; Search Query: the query for the search engine
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;-&lt;/span&gt; Observation: the search engine will respond with the results
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;-&lt;/span&gt; Final Answer: the final answer to the question, make it short (1-5 words)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Thought, Search Query, and Observation steps can be repeated multiple times, but 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sometimes, we can find an answer in the first pass.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;-&lt;/span&gt; Question: &amp;#34;Was the capital of France founded earlier than the discovery of America?&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;-&lt;/span&gt; Thought: 
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This prompt is very complex, so let&amp;rsquo;s break it down:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The LLM reads the question and decides which information to retrieve.&lt;/li&gt;
&lt;li&gt;The LLM returns a query for the search engine (or a retriever of our choice).&lt;/li&gt;
&lt;li&gt;Retrieval is run with the query the LLM provided, and the resulting context is appended to the original prompt.&lt;/li&gt;
&lt;li&gt;The entire prompt is returned to the LLM, which reads it, follows all the reasoning it did in the previous steps, and decides whether to do another search or reply to the user.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Multihop RAG is used for autonomous exploration of a topic, but it can be very expensive because many LLM calls are performed, and the prompts tend to become really long really quickly. The process can also take quite some time, so it&amp;rsquo;s not suitable for low-latency applications. However, the idea is quite powerful, and it can be adapted into other forms.&lt;/p&gt;
&lt;h1 id=&#34;a-word-on-finetuning&#34;&gt;
  A word on finetuning
  
&lt;/h1&gt;
&lt;p&gt;It&amp;rsquo;s important to remember that finetuning is not an alternative to RAG. Finetuning can and should be used together with RAG on very complex domains, such as medical or legal.&lt;/p&gt;
&lt;p&gt;When people think about finetuning, they usually focus on finetuning the LLM. In RAG, though, it is not only the LLM that needs to understand the question: it&amp;rsquo;s crucial that the retriever understands it well, too! This means &lt;strong&gt;the embedding model needs finetuning as much as the LLM&lt;/strong&gt;. Finetuning your embedding models, and in some cases also your reranker, can improve the effectiveness of your RAG by orders of magnitude. Such a finetune often requires only a fraction of the training data, so it&amp;rsquo;s well worth the investment.&lt;/p&gt;
&lt;p&gt;Finetuning the LLM is also necessary if you need to alter its behavior in production, such as making it more colloquial, more concise, or stick to a specific voice. Prompt engineering can also achieve these effects, but it&amp;rsquo;s often more brittle and can be more easily worked around. Finetuning the LLM has a much more powerful and lasting effect.&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;
  Conclusion
  
&lt;/h1&gt;
&lt;p&gt;RAG is a vast topic that could fill books: this was only an overview of some of the most important concepts to remember when working on a RAG application. For more on this topic, check out my &lt;a href=&#34;https://www.zansara.dev/posts&#34; &gt;other blog posts&lt;/a&gt; and stay tuned for &lt;a href=&#34;https://www.zansara.dev/talks&#34; &gt;future talks&lt;/a&gt;!&lt;/p&gt;
&lt;p class=&#34;fleuron&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-05-06-teranoptia/&#34;&gt;[K&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ODSC East: RAG, the bad parts (and the good!)</title>
      <link>https://www.zansara.dev/talks/2024-04-25-odsc-east-rag/</link>
      <pubDate>Thu, 25 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/talks/2024-04-25-odsc-east-rag/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://odsc.com/speakers/rag-the-bad-parts-and-the-good-building-a-deeper-understanding-of-this-hot-llm-paradigms-weaknesses-strengths-and-limitations/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Announcement&lt;/a&gt;,
&lt;a href=&#34;https://drive.google.com/file/d/19EDFCqOiAo9Cvx5fxx6Wq1Z-EoMKwxbs/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;.
Did you miss the talk? Check out the &lt;a href=&#34;https://www.zansara.dev/posts/2024-04-29-odsc-east-rag&#34; &gt;write-up&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;At &lt;a href=&#34;https://odsc.com/boston/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ODSC East 2024&lt;/a&gt; I talked about RAG: how it works, how it fails, and how to evaluate its performance objectively. I gave an overview of some useful open-source tools for RAG evalution and how to use them with &lt;a href=&#34;https://haystack.deepset.ai/?utm_campaign=odsc-east&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haystack&lt;/a&gt;, and then offered some ideas on how to expand your RAG architecture further than a simple two-step process.&lt;/p&gt;
&lt;p&gt;Some resources mentioned in the talk:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Haystack: open-source LLM framework for RAG and beyond: &lt;a href=&#34;https://haystack.deepset.ai/?utm_campaign=odsc-east&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://haystack.deepset.ai/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Build and evaluate RAG with Haystack: &lt;a href=&#34;https://haystack.deepset.ai/tutorials/35_model_based_evaluation_of_rag_pipelines/?utm_campaign=odsc-east&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://haystack.deepset.ai/tutorials/35_model_based_evaluation_of_rag_pipelines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Evaluating LLMs with UpTrain: &lt;a href=&#34;https://docs.uptrain.ai/getting-started/introduction&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://docs.uptrain.ai/getting-started/introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Evaluating RAG end-to-end with RAGAS: &lt;a href=&#34;https://docs.ragas.io/en/latest/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://docs.ragas.io/en/latest/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Semantic Answer Similarity (SAS) metric: &lt;a href=&#34;https://docs.ragas.io/en/latest/concepts/metrics/semantic_similarity.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://docs.ragas.io/en/latest/concepts/metrics/semantic_similarity.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Answer Correctness metric: &lt;a href=&#34;https://docs.ragas.io/en/latest/concepts/metrics/answer_correctness.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://docs.ragas.io/en/latest/concepts/metrics/answer_correctness.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Perplexity.ai: &lt;a href=&#34;https://www.perplexity.ai/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.perplexity.ai/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Plus, shout-out to a very interesting LLM evaluation library I discovered at ODSC: &lt;a href=&#34;https://docs.relari.ai/v0.3&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;continuous-eval&lt;/a&gt;. Worth checking out especially if SAS or answer correctness are too vague and high level for your domain.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Explain me LLMs like I&#39;m five: build a story to help anyone get the idea</title>
      <link>https://www.zansara.dev/posts/2024-04-14-eli5-llms/</link>
      <pubDate>Sun, 14 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2024-04-14-eli5-llms/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;
  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-04-14-eli5-llms/Explain%20me%20LLMs%20like%20I%27m%20five:%20build%20a%20story%20to%20help%20anyone%20get%20the%20idea%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/22f971d2-4a48-4b15-8f74-8afd8e4a0f1b?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;
  &lt;audio controls style=&#34;width:100%;&#34;&gt;
    &lt;source src=&#34;https://www.zansara.dev/posts/2024-04-14-eli5-llms/Explain%20me%20LLMs%20like%20I%27m%20five:%20build%20a%20story%20to%20help%20anyone%20get%20the%20idea%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt; 
  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;
    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;
    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;These days everyone&amp;rsquo;s boss seems to want some form of GenAI in their products. That doesn&amp;rsquo;t always make sense: however, understanding when it does and when it doesn&amp;rsquo;t is not obvious even for us experts, and nearly impossible for everyone else.&lt;/p&gt;
&lt;p&gt;How can we help our colleagues understand the pros and cons of this tech, and figure out when and how it makes sense to use it?&lt;/p&gt;
&lt;p&gt;In this post I am going to outline a narrative that explains LLMs without tecnicalities and help you frame some high level technical decisions, such as RAG vs finetuning, or which specific model size to use, in a way that a non-technical audience can not only grasp but also reason about. We&amp;rsquo;ll start by &amp;ldquo;translating&amp;rdquo; a few terms into their &amp;ldquo;human equivalent&amp;rdquo; and then use this metaphor to reason about the differences between RAG and finetuning.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s dive in!&lt;/p&gt;
&lt;h1 id=&#34;llms-are-high-school-students&#34;&gt;
  LLMs are high-school students
  
&lt;/h1&gt;
&lt;p&gt;Large Language Models are often described as &amp;ldquo;super-intelligent&amp;rdquo; entities that know far more than any human could possibly know. This makes a lot of people think that they are also extremely intelligent and are able to reason about anything in a super-human way. The reality is very different: LLMs are able to memorize and repeat far more facts that humans do, but in their abilities to reason they are often inferior to the average person.&lt;/p&gt;
&lt;p&gt;Rather than describing LLMs as all-knowing geniuses, it&amp;rsquo;s much better to frame them as &lt;strong&gt;an average high-school student&lt;/strong&gt;. They&amp;rsquo;re not the smartest humans on the planet, but they can help a lot if you guide them through the process. And just as a normal person might, sometimes they forget things, and occasionally they remember them wrong.&lt;/p&gt;
&lt;h1 id=&#34;some-llms-are-smarter-than-others&#34;&gt;
  Some LLMs are smarter than others
  
&lt;/h1&gt;
&lt;p&gt;Language models are not all born equal. Some are inherently able to do more complex reasoning, to learn more facts and to talk more smoothly in more languages.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;&amp;ldquo;IQ&amp;rdquo;&lt;/strong&gt; of an LLM can be approximated, more or less, to its &lt;strong&gt;parameter count&lt;/strong&gt;. An LLM with 7 billion parameters is almost always less clever than a 40 billion parameter model, will have a harder time learning more facts, and will be harder to reason with.&lt;/p&gt;
&lt;p&gt;However, just like with real humans, there are exceptions. Recent &amp;ldquo;small&amp;rdquo; models can easily outperform older and larger models, due to improvements in the way they&amp;rsquo;re built. Also, some small models are very good at some very specialized job and can outperform a large, general purpose model on that task.&lt;/p&gt;
&lt;h1 id=&#34;llms-learn-by-studying&#34;&gt;
  LLMs learn by &amp;ldquo;studying&amp;rdquo;
  
&lt;/h1&gt;
&lt;p&gt;Another similarity to human students is that LLMs also learn all the fact they know by &lt;strong&gt;&amp;ldquo;going to school&amp;rdquo;&lt;/strong&gt; and studying a ton of general and unrelated facts. This is what &lt;strong&gt;training&lt;/strong&gt; an LLM means. This implies that, just like with students, an LLM needs a lot of varied material to study from. This material is what is usually called &amp;ldquo;training data&amp;rdquo; or &amp;ldquo;training dataset&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;They can also learn more than what they currently know and &lt;strong&gt;specialize&lt;/strong&gt; on a topic: all they need to do is to study further on it. This is what &lt;strong&gt;finetuning&lt;/strong&gt; represents, and as you would expect, it also needs some study material. This is normally called &amp;ldquo;finetuning data/datasets&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;The distinction between training and fine tuning is not much about how it&amp;rsquo;s done, but mostly about &lt;strong&gt;the size and contents of the dataset required&lt;/strong&gt;. The initial training usually takes a lot of time, computing power, and tons of very varied data, just like what&amp;rsquo;s needed to bring a baby to the level of a high-schooler. Fine tuning instead looks like preparing for a specific test or exam: the study material is a lot less and a lot more specific.&lt;/p&gt;
&lt;p&gt;Keep in mind that, just like for humans, studying more can make a student a bit smarter, but it won&amp;rsquo;t make it a genius. In many cases, no amount of training and/or finetuning can close the gap between the 7 billion parameter version of an LLM and the 40 billion one.&lt;/p&gt;
&lt;h1 id=&#34;every-chat-is-an-exam&#34;&gt;
  Every chat is an exam
  
&lt;/h1&gt;
&lt;p&gt;One of the most common usecases for LLMs is question answering, an NLP task where users ask questions to the model and expect a correct answer back. The fact that the answer must be correct means that this interaction is very similar to an &lt;strong&gt;exam&lt;/strong&gt;: the LLM is being tested by the user on its knowledge.&lt;/p&gt;
&lt;p&gt;This means that, just like a student, when the LLM is used directly it has to rely on its knowledge to answer the question. If it studied the topic well it will answer accurately most of the times. However if it didn&amp;rsquo;t study the subject, it will do what students always do: they will &lt;strong&gt;make up stuff that sounds legit&lt;/strong&gt;, hoping that the teacher will not notice how little they know. This is what we call &lt;strong&gt;hallucinations&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;When the answer is known to the user the answer of the LLM can be graded, just like in a real exam, to make the LLM improve. This process is called &lt;strong&gt;evaluation&lt;/strong&gt;. Just like with humans, there are many ways in which the answer can be graded: the LLM can be graded on the accuracy of the facts it recalled, or the fluency it delivered its answer with, or it can be scored on the correctness of a reasoning exercise, like a math problem. These ways of grading an LLM are called &lt;strong&gt;metrics&lt;/strong&gt;.&lt;/p&gt;
&lt;h1 id=&#34;making-the-exams-easier&#34;&gt;
  Making the exams easier
  
&lt;/h1&gt;
&lt;p&gt;Hallucinations are very dangerous if the user doesn&amp;rsquo;t know what the LLM was supposed to reply, so they need to be reduced, possibly eliminated entirely. It&amp;rsquo;s like we really need the students to pass the exam with flying colors, no matter how much they studied.&lt;/p&gt;
&lt;p&gt;Luckily there are many ways to help our student succeed. One way to improve the score is, naturally, to make them study more and better. Giving them more time to study (&lt;strong&gt;more finetuning&lt;/strong&gt;) and better material (&lt;strong&gt;better finetuning datasets&lt;/strong&gt;) is one good way to make LLMs reply correctly more often. The issue is that this method is &lt;strong&gt;expensive&lt;/strong&gt;, because it needs a lot of computing power and high quality data, and the student may still forget something during the exam.&lt;/p&gt;
&lt;p&gt;We can make the exams even easier by converting them into &lt;strong&gt;open-book exams&lt;/strong&gt;. Instead of asking the students to memorize all the facts and recall them during the exam, we can let them bring the book and lookup the information they need when the teacher asks the question. This method can be applied to LLMs too and is called &lt;strong&gt;RAG&lt;/strong&gt;, which stands for &amp;ldquo;retrieval augmented generation&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;RAG has a few interesting properties. First of all, it can make very easy even for &amp;ldquo;dumb&amp;rdquo;, small LLMs to recall nearly all the important facts correctly and consistently. By letting your students carry their history books to the exam, all of them will be able to tell you the date of any historical event by just looking it up, regardless of how smart they are or how much they studied.&lt;/p&gt;
&lt;p&gt;RAG doesn&amp;rsquo;t need a lot of data, but you need an &lt;strong&gt;efficient way to access it&lt;/strong&gt;. In our metaphor, you need a well structured book with a good index to help the student find the correct facts when asked, or they might fail to find the information they need when they&amp;rsquo;re quizzed.&lt;/p&gt;
&lt;p&gt;A trait that makes RAG unique is that is can be used to keep the LLM up-to-date with &lt;strong&gt;information that can&amp;rsquo;t be &amp;ldquo;studied&amp;rdquo;&lt;/strong&gt; because it changes too fast. Let&amp;rsquo;s imagine a teacher that wants to quiz the students about today&amp;rsquo;s stock prices. They can&amp;rsquo;t expect the pupils to know them if they don&amp;rsquo;t have access to the latest financial data. Even if they were to study the prices every hour the result would be quite pointless, because all the knowledge they acquire becomes immediately irrelevant and might even confuse them.&lt;/p&gt;
&lt;p&gt;Last but not least, RAG can be used &lt;em&gt;together&lt;/em&gt; with finetuning. Just as a teacher can make students study the topic and then also bring the book to the exam to make sure they will answer correctly, you can also use RAG and finetuning together.&lt;/p&gt;
&lt;p&gt;However, there are situations where RAG doesn&amp;rsquo;t help. For example, it&amp;rsquo;s pointless if the questions are asked in language that the LLM doesn&amp;rsquo;t know, or if the exam is made of tasks that require complex reasoning. This is true for human students too: books won&amp;rsquo;t help them much to understand a foreign language to the point that they can take an exam in it, and won&amp;rsquo;t be useful to crack a hard math problem. For these sort of exams the students just need to be smart and study more, which in LLM terms means that you should prefer a large model and you probably need to finetune it.&lt;/p&gt;
&lt;h1 id=&#34;telling-a-story&#34;&gt;
  Telling a story
  
&lt;/h1&gt;
&lt;p&gt;Let&amp;rsquo;s recap the terminology we used:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;LLM&lt;/strong&gt; is a &lt;strong&gt;student&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;LLM&amp;rsquo;s IQ&lt;/strong&gt; corresponds to its &lt;strong&gt;parameter count&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training&lt;/strong&gt; an LLM is the same as making a student &lt;strong&gt;go to school&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Finetuning&lt;/strong&gt; it means to make it &lt;strong&gt;specialize on a subject&lt;/strong&gt; by making it study only books and other material on the subject&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;training dataset&lt;/strong&gt; is the &lt;strong&gt;books and material&lt;/strong&gt; the student needs to study on&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;User interactions&lt;/strong&gt; are like &lt;strong&gt;university exams&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evaluating&lt;/strong&gt; an LLM means to &lt;strong&gt;score its answers&lt;/strong&gt; as if they were the responses to a test&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;metric&lt;/strong&gt; is a &lt;strong&gt;type of evaluation&lt;/strong&gt; that focuses on a specific trait of the answer&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;hallucination&lt;/strong&gt; is a &lt;strong&gt;wrong answer&lt;/strong&gt; that the LLM makes up just like a student would, to in order to try passing an exam when it doesn&amp;rsquo;t know the answer or can&amp;rsquo;t recall it in that moment&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RAG (retrieval augmented generation)&lt;/strong&gt; is like an &lt;strong&gt;open-book exam&lt;/strong&gt;: it gives the LLM access to some material on the question&amp;rsquo;s topic, so it won&amp;rsquo;t need to hallucinate an answer. It will help the LLM recall facts, but it won&amp;rsquo;t make it smarter.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By drawing a parallel with a human student it can be a lot easier to explain to non-technical audience why some decisions were taken.&lt;/p&gt;
&lt;p&gt;For example, it might not be obvious why RAG is cheaper than finetuning, because both need domain-specific data. By explaining that RAG is like an open-book exam versus a closed-book one, the difference is clearer: the students need less time and effort to prepare and they&amp;rsquo;re less likely to make trivial mistakes if they can bring the book with them at the exam.&lt;/p&gt;
&lt;p&gt;Another example is hallucinations. It&amp;rsquo;s difficult for many people to understand why LLMs don&amp;rsquo;t like to say &amp;ldquo;I don&amp;rsquo;t know&amp;rdquo;, until they realise that from the LLM&amp;rsquo;s perspective every question is like an exam: better make up something that admit they&amp;rsquo;re unprepared! And so on.&lt;/p&gt;
&lt;p&gt;Building a shared, simple intuition of how LLM works is a very powerful tool. Next time you&amp;rsquo;re asked to explain a technical decision related to LLMs, building a story around it may get the message across in a much more effective way and help everyone be on the same page. Give it a try!&lt;/p&gt;
&lt;p class=&#34;fleuron&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-05-06-teranoptia/&#34;&gt;WYZ&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ClozeGPT: Write Anki cloze cards with a custom GPT</title>
      <link>https://www.zansara.dev/posts/2024-02-28-create-anki-cloze-cards-with-custom-gpt/</link>
      <pubDate>Wed, 28 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2024-02-28-create-anki-cloze-cards-with-custom-gpt/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;
  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-02-28-create-anki-cloze-cards-with-custom-gpt/ClozeGPT:%20Write%20Anki%20cloze%20cards%20with%20a%20custom%20GPT%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/10ebaf1f-dc2f-41ce-a9d6-0ad29238d96e?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;
  &lt;audio controls style=&#34;width:100%;&#34;&gt;
    &lt;source src=&#34;https://www.zansara.dev/posts/2024-02-28-create-anki-cloze-cards-with-custom-gpt/ClozeGPT:%20Write%20Anki%20cloze%20cards%20with%20a%20custom%20GPT%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt; 
  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;
    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;
    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;As everyone who has been serious about studying with &lt;a href=&#34;https://apps.ankiweb.net/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anki&lt;/a&gt; knows, the first step of the journey is writing your own flashcards. Writing the cards yourself is often cited as the most straigthforward way to make the review process more effective. However, this can become a big chore, and not having enough cards to study is a sure way to not learn anything.&lt;/p&gt;
&lt;p&gt;What can we do to make this process less tedious?&lt;/p&gt;
&lt;h1 id=&#34;write-simple-cards&#34;&gt;
  Write simple cards
  
&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.reddit.com/r/Anki/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A lot&lt;/a&gt; has been written about the best way to create Anki cards. However, as a &lt;a href=&#34;https://news.ycombinator.com/item?id=39002138&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HackerNews commenter&lt;/a&gt; once said:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;One massively overlooked way to improve spaced repetition is to make easier cards.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Cards can hardly be &lt;a href=&#34;https://www.supermemo.com/en/blog/twenty-rules-of-formulating-knowledge&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;too simple to be effective&lt;/a&gt;. You don&amp;rsquo;t need to write complicated tricky questions to make sure you are making the most of your reviews. On the contrary, even a long sentence where the word you need to study is highlighted is often enough to make the review worthwhile.&lt;/p&gt;
&lt;p&gt;In the case of language learning, if you&amp;rsquo;re an advanced learner one of the easiest way to create such cards is to &lt;a href=&#34;https://www.supermemo.com/en/blog/learn-whole-phrases-supertip-4&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;copy-paste a sentence&lt;/a&gt; with your target word into a card and write the translation of that word (or sentence) on the back. But if you&amp;rsquo;re a beginner, even these cards can be complicated both to write and to review. What if the sentence where you found the new word is too complex? You&amp;rsquo;ll need to write a brand new sentence. But what if you write an incorrect sentence? And so on.&lt;/p&gt;
&lt;h1 id=&#34;automating-the-process&#34;&gt;
  Automating the process
  
&lt;/h1&gt;
&lt;p&gt;Automated card generation has been often compared to the usage of &lt;a href=&#34;https://www.reddit.com/r/languagelearning/comments/6ysx7g/is_there_value_in_making_your_own_anki_deck_or/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pre-made decks&lt;/a&gt;, because the students don&amp;rsquo;t see the content of the cards they&amp;rsquo;re adding to their decks before doing so. However, this depends a lot on how much the automation is hiding from the user.&lt;/p&gt;
&lt;p&gt;In my family we&amp;rsquo;re currently learning Portuguese, so we end up creating a lot of cards with Portuguese vocabulary. Given that many useful words are hard to make sense of without context, having cards with sample sentences helps me massively to remember them. But our sample sentences often sound unnatural in Portuguese, even when they&amp;rsquo;re correct. It would be great if we could have a &amp;ldquo;sample sentence generator&amp;rdquo; that creates such sample sentences for me in more colloquial Portuguese!&lt;/p&gt;
&lt;p&gt;This is when we&amp;rsquo;ve got the idea of using an LLM to help with the task. GPT models are great sentence generators: can we get them to make some good sample sentence cards?&lt;/p&gt;
&lt;p&gt;A &lt;a href=&#34;https://chat.openai.com/share/89c821b8-6048-45f3-9fc1-c3875fdbe1c5&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;quick experiment&lt;/a&gt; proves that there is potential to this concept.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-02-28-create-anki-cloze-cards-with-custom-gpt/chatgpt-anki-card-creation.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;custom-gpts&#34;&gt;
  Custom GPTs
  
&lt;/h1&gt;
&lt;p&gt;The natural next step is to store that set of instructions into a custom prompt, or as they&amp;rsquo;re called now, a &lt;a href=&#34;https://help.openai.com/en/articles/8554407-gpts-faq#h_40756527ce&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;custom GPT&lt;/a&gt;. Making these small wrapper is &lt;a href=&#34;https://help.openai.com/en/articles/8554397-creating-a-gpt&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;really easy&lt;/a&gt;: it requires no coding, only a well crafted prompt and a catchy name. So we called our new GPT &amp;ldquo;ClozeGPT&amp;rdquo; and started off with a prompt like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Your job is to create Portuguese Anki cloze cards. 
I might give you a single word or a pair (word + translation). 

Front cards:
- Use Anki&#39;s `{{c1::...}}` feature to template in cards. 
- You can create cards with multiple clozes.
- Keep the verb focused, and don&#39;t rely too much on auxiliary verbs like 
  &amp;quot;precisar&amp;quot;, &amp;quot;gostar&amp;quot;, etc...
- Use the English translation as a cloze hint.

Back cards:
- The back card should contain the Portuguese word.
- If the word could be mistaken (e.g. &amp;quot;levantar&amp;quot; vs. &amp;quot;acordar&amp;quot;), 
  write a hint that can help me remember the difference. 
- The hint should be used sparingly.

Examples:

---------

Input: cozinhar

# FRONT
```
Eu {{c1::cozinho::cook}} todos os dias para minha família.
```

# BACK
```
cozinhar - to cook
```
---------

Input: levantar

# FRONT
```
Eu preciso {{c1::levantar::get up}} cedo amanhã para ir ao trabalho.
```

# BACK
```
levantar - to get up, to raise (don&#39;t mistake this with &amp;quot;acordar&amp;quot;, which is to wake up from sleep)
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This simple prompt already gives very nice results!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-02-28-create-anki-cloze-cards-with-custom-gpt/beber-flashcard.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;bells-and-whistles&#34;&gt;
  Bells and whistles
  
&lt;/h1&gt;
&lt;p&gt;Naturally, once a tool works well it&amp;rsquo;s hard to resist the urge to add some new features to it. So for our ClozeGPT we added a few more abilities:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Commands

## `+&amp;lt;&amp;lt;word&amp;gt;&amp;gt;`
Expands the back card with an extra word present in the sentence.
Include all the previous words, plus the one given.
In this case, only the back card needs to be printed; don&#39;t show the front card again.

## `R[: &amp;lt;&amp;lt;optional hint&amp;gt;&amp;gt;]`
Regenerates the response based on the hint given.
If the hint is absent, regenerate the sentence with a different context.
Do not change the target words, the hint most often a different context I would like to have a sentence for.

## `Q: &amp;lt;&amp;lt;question&amp;gt;&amp;gt;`
This is an escape to a normal chat about a related question.
Answer the question as usual, you don&#39;t need to generate anything.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;+&lt;/code&gt; command is useful when the generated sentence contains some other interesting word you can take the occasion to learn as well:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-02-28-create-anki-cloze-cards-with-custom-gpt/maca-flashcard.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;R&lt;/code&gt; command can be used to direct the card generation a bit better than with a simple press on the &amp;ldquo;Regenerate&amp;rdquo; icon:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-02-28-create-anki-cloze-cards-with-custom-gpt/morango-flashcard.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;And finally &lt;code&gt;Q&lt;/code&gt; is a convenient escape hatch to make this GPT revert back to its usual helpful self, where it can engage in conversation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-02-28-create-anki-cloze-cards-with-custom-gpt/esquecer-flashcard.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;have-fun&#34;&gt;
  Have fun
  
&lt;/h1&gt;
&lt;p&gt;Our small &lt;a href=&#34;https://chat.openai.com/g/g-wmHCaGcCZ-clozegpt&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ClozeGPT&lt;/a&gt; works only for Portuguese now, but feel free to play with it if you find it useful. And, of course, always keep in mind that LLMs are only &lt;a href=&#34;https://chat.openai.com/share/07295647-9f43-4346-97a5-b35f62251d55&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pretending to be humans&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-02-28-create-anki-cloze-cards-with-custom-gpt/laranja-flashcard.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Front: I like orange juice in my morning coffee.&lt;/em&gt;&lt;/p&gt;
&lt;p class=&#34;fleuron&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-05-06-teranoptia/&#34;&gt;SDE&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Is RAG all you need? A look at the limits of retrieval augmentation</title>
      <link>https://www.zansara.dev/posts/2024-02-20-is-rag-all-you-need-odsc-east-2024-teaser/</link>
      <pubDate>Wed, 21 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2024-02-20-is-rag-all-you-need-odsc-east-2024-teaser/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;
  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-02-20-is-rag-all-you-need-odsc-east-2024-teaser/Is%20RAG%20all%20you%20need%20A%20look%20at%20the%20limits%20of%20retrieval%20augmentation%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/ca710e7b-5af7-4733-b3d8-c012758bd171?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;
  &lt;audio controls style=&#34;width:100%;&#34;&gt;
    &lt;source src=&#34;https://www.zansara.dev/posts/2024-02-20-is-rag-all-you-need-odsc-east-2024-teaser/Is%20RAG%20all%20you%20need%20A%20look%20at%20the%20limits%20of%20retrieval%20augmentation%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt; 
  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;
    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;
    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Retrieval Augmented Generation (RAG) is by far one of the most popular and effective techniques to bring LLMs to production. Introduced by a Meta &lt;a href=&#34;https://arxiv.org/abs/2005.11401&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; in 2021, it since took off and evolved to become a field in itself, fueled by the immediate benefits that it provides: lowered risk of hallucinations, access to updated information, and so on. On top of this, RAG is relatively cheap to implement for the benefit it provides, especially when compared to costly techniques like LLM finetuning. This makes it a no-brainer for a lot of usecases, to the point that nowadays every production system that uses LLMs in production seems to be implemented as some form of RAG.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-02-20-is-rag-all-you-need-odsc-east-2024-teaser/rag_paper.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A diagram of a RAG system from the &lt;a href=&#34;https://arxiv.org/abs/2005.11401&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;original paper&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;However, retrieval augmentation is not a silver bullet that many claim it is. Among all these obvious benefits, RAG brings its own set of weaknesses and limitations, which it’s good to be aware of when scale and accuracy need to be improved further.&lt;/p&gt;
&lt;h1 id=&#34;how-does-a-rag-application-fail&#34;&gt;
  How does a RAG application fail?
  
&lt;/h1&gt;
&lt;p&gt;At a high level, RAG introduces a retrieval step right before the LLM generation. This means that we can classify the failure modes of a RAG system into two main categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Retrieval failures: when the retriever returns only documents which are irrelevant to the query or misleading, which in turn gives the LLM wrong information to build the final answer from.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generation failures: when the LLM generates a reply that is unrelated or directly contradicts the documents that were retrieved. This is a classic LLM hallucination.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When developing a simple system or a PoC, these sorts of errors tends to have a limited impact on the results as long as you are using the best available tools. Powerful LLMs such as GPT 4 and Mixtral are not at all prone to hallucination when the provided documents are correct and relevant, and specialized systems such as vector databases, combined with specialized embedding models, that can easily achieve high retrieval accuracy, precision and recall on most queries.&lt;/p&gt;
&lt;p&gt;However, as the system scales to larger corpora, lower quality documents, or niche and specialized fields, these errors end up amplifying each other and may degrade the overall system performance noticeably. Having a good grasp of the underlying causes of these issues, and an idea of how to minimize them, can make a huge difference.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-02-20-is-rag-all-you-need-odsc-east-2024-teaser/rag_failures.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The difference between retrieval and generation failures. Identifying where your RAG system is more likely to fail is key to improve the quality of its answers.&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;a-case-study-customer-support-chatbots&#34;&gt;
  A case study: customer support chatbots
  
&lt;/h1&gt;
&lt;p&gt;This is one of the most common applications of LLMs is a chatbot that helps users by answering their questions about a product or a service. Apps like this can be used in situations that are more or less sensitive for the user and difficult for the LLM: from simple developer documentation search, customer support for airlines or banks, up to bots that provide legal or medical advice.&lt;/p&gt;
&lt;p&gt;These three systems are very similar from a high level perspective: the LLM needs to use snippets retrieved from a a corpus of documents to build a coherent answer for the user. In fact, RAG is a fitting architecture for all of them, so let’s assume that all the three systems are build more or less equally, with a retrieval step followed by a generation one.
Let’s see what are the challenges involved in each of them.&lt;/p&gt;
&lt;h2 id=&#34;enhanced-search-for-developer-documentation&#34;&gt;
  Enhanced search for developer documentation
  
&lt;/h2&gt;
&lt;p&gt;For this usecase, RAG is usually sufficient to achieve good results. A simple proof of concept may even overshoots expectations.&lt;/p&gt;
&lt;p&gt;When present and done well, developer documentation is structured and easy for a chatbot to understand. Retrieval is usually easy and effective, and the LLM can reinterpret the retrieved snippets effectively. On top of that, hallucinations are easy to spot by the user or even by an automated system like a REPL, so they have a limited impact on the perceived quality of the results.&lt;/p&gt;
&lt;p&gt;As a bonus, the queries are very likely to always be in English, which happens to be the case for the documentation too and to be the language which LLMs are the strongest at.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-02-20-is-rag-all-you-need-odsc-east-2024-teaser/mongodb.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The MongoDB documentation provides a chatbot interface which is quite useful.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;customer-support-bots-for-airlines-and-banks&#34;&gt;
  Customer support bots for airlines and banks
  
&lt;/h2&gt;
&lt;p&gt;In this case, the small annoyances that are already present above have a &lt;a href=&#34;https://www.theguardian.com/world/2024/feb/16/air-canada-chatbot-lawsuit&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;much stronger impact&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Even if your airline or bank’s customer support pages are top notch, hallucinations are not as easy to spot, because to make sure that the answers are accurate the user needs to check the sources that the LLM is quoting… which voids the whole point of the generation step. And what if the user cannot read such pages at all? Maybe they speak a minority language, so they can’t read them. Also, LLMs tend to perform worse on languages other than English and hallucinate more often, exacerbating the problem where it’s already more acute.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-02-20-is-rag-all-you-need-odsc-east-2024-teaser/air_canada.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;You are going to need a very good RAG system and a huge disclaimer to avoid &lt;a href=&#34;https://www.theguardian.com/world/2024/feb/16/air-canada-chatbot-lawsuit&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this scenario&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;bots-that-provide-legal-or-medical-advice&#34;&gt;
  Bots that provide legal or medical advice
  
&lt;/h2&gt;
&lt;p&gt;The third case brings the exact same issues to a whole new level. In these scenarios, vanilla RAG is normally not enough.&lt;/p&gt;
&lt;p&gt;Laws and scientific articles are hard to read for the average person, require specialized knowledge to understand, and they need to be read in context: asking the user to check the sources that the LLM is quoting is just not possible. And while retrieval on this type of documents is feasible, its accuracy is not as high as on simple, straightforward text.&lt;/p&gt;
&lt;p&gt;Even worse, LLMs often have no reliable background knowledge on these topics, so their reply need to be strongly grounded by relevant documents for the answers to be correct and dependable. While a simple RAG implementation is still better than a vanilla reply from GPT-4, the results can be problematic in entirely different ways.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2024-02-20-is-rag-all-you-need-odsc-east-2024-teaser/medical_questions.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2949761223000366&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Research is being done&lt;/a&gt;, but the results are not promising yet.&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;ok-but-what-can-we-do&#34;&gt;
  Ok, but what can we do?
  
&lt;/h1&gt;
&lt;p&gt;Moving your simple PoC to real world use cases without reducing the quality of the response requires a deeper understanding of how the retrieval and the generation work together. You need to be able to measure your system’s performance, to analyze the causes of the failures, and to plan experiments to improve such metrics. Often you will need to complement it with other techniques that can improve its retrieval and generation abilities to reach the quality thresholds that makes such a system useful at all.&lt;/p&gt;
&lt;p&gt;In my upcoming talk at ODSC East “RAG, the bad parts (and the good!): building a deeper understanding of this hot LLM paradigm’s weaknesses, strengths, and limitations” we are going to cover all these topics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;how to &lt;strong&gt;measure the performance&lt;/strong&gt; of your RAG applications, from simple metrics like F1 to more sophisticated approaches like Semantic Answer Similarity.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;how to &lt;strong&gt;identify if you’re dealing with a retrieval or a generation failure&lt;/strong&gt; and where to look for a solution: is the problem in your documents content, in their size, in the way you chunk them or embed them? Or is the LLM that is causing the most trouble, maybe due to the way you are prompting it?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;what &lt;strong&gt;techniques can help you raise the quality of the answers&lt;/strong&gt;, from simple prompt engineering tricks like few-shot prompting, all the way up to finetuning, self-correction loops and entailment checks.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Make sure to attend to the &lt;a href=&#34;https://odsc.com/speakers/rag-the-bad-parts-and-the-good-building-a-deeper-understanding-of-this-hot-llm-paradigms-weaknesses-strengths-and-limitations/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;talk&lt;/a&gt; to learn more about all these techniques and how to apply them in your projects.&lt;/p&gt;
&lt;p class=&#34;fleuron&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-05-06-teranoptia/&#34;&gt;ažo&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Headless WiFi setup on Raspberry Pi OS &#34;Bookworm&#34; without the Raspberry Pi Imager</title>
      <link>https://www.zansara.dev/posts/2024-01-06-raspberrypi-headless-bookworm-wifi-config/</link>
      <pubDate>Sat, 06 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2024-01-06-raspberrypi-headless-bookworm-wifi-config/</guid>
      <description>&lt;div style=&#34;padding: 5px 15px; font-family: sans; font-style: italic;&#34;&gt;
  &lt;p style=&#34;margin:5px 0 10px 0;&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-01-06-raspberrypi-headless-bookworm-wifi-config/Headless%20WiFi%20setup%20on%20Raspberry%20Pi%20OS%20Bookworm%20without%20the%20Raspberry%20Pi%20Imager%20-%20Sara%20Zan.mp3&#34;&gt;Listen to this article&lt;/a&gt; or &lt;a href=&#34;https://app.speechify.com/share/6a234a55-265b-4a94-82e7-57bbb05b738e?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34;&gt;read it in the app&lt;/a&gt;:&lt;/p&gt;
  &lt;audio controls style=&#34;width:100%;&#34;&gt;
    &lt;source src=&#34;https://www.zansara.dev/posts/2024-01-06-raspberrypi-headless-bookworm-wifi-config/Headless%20WiFi%20setup%20on%20Raspberry%20Pi%20OS%20Bookworm%20without%20the%20Raspberry%20Pi%20Imager%20-%20Sara%20Zan.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
  &lt;/audio&gt; 
  &lt;div style=&#34;text-align:right;vertical-align:middle;&#34;&gt;
    &lt;span style=&#34;font-style: italic;&#34;&gt;Powered by&lt;/span&gt;
    &lt;a href=&#34;https://speechify.com/text-to-speech-online/?utm_campaign=partners&amp;utm_content=rewardful&amp;via=zansaradev-blog&#34; target=&#34;_blank&#34; style=&#34;display:inline;margin-left:5px;&#34;&gt;&lt;img src=&#34;https://www.zansara.dev/global/speechify_logo.svg&#34; alt=&#34;Speechify&#34; style=&#34;width:150px;background-color:white;margin:0;vertical-align:middle;&#34;/&gt;&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Setting up a Raspberry Pi headless without the Raspberry Pi Imager used to be a fairly simple process for the average Linux user, to the point where a how-to and a few searches on the Raspberry Pi forums would sort the process out. After flashing the image with &lt;code&gt;dd&lt;/code&gt;, creating &lt;code&gt;ssh&lt;/code&gt; in the boot partition and populating &lt;code&gt;wpa_supplicant.conf&lt;/code&gt; was normally enough to get started.&lt;/p&gt;
&lt;p&gt;However with the &lt;a href=&#34;https://www.raspberrypi.com/news/bookworm-the-new-version-of-raspberry-pi-os/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recently released Raspberry Pi OS 12 &amp;ldquo;Bookworm&amp;rdquo;&lt;/a&gt; this second step &lt;a href=&#34;https://www.raspberrypi.com/documentation/computers/configuration.html#connect-to-a-wireless-network&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doesn&amp;rsquo;t work anymore&lt;/a&gt; and the only recommendation that users receive is to &amp;ldquo;just use the Raspberry Pi Imager&amp;rdquo; (like &lt;a href=&#34;https://github.com/raspberrypi/bookworm-feedback/issues/72&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;But what does the Imager really do to configure the OS? Is it really that complex that it requires downloading a dedicated installer?&lt;/p&gt;
&lt;p&gt;In this post I&amp;rsquo;m going to find out first how to get the OS connect to the WiFi without Imager, and then I&amp;rsquo;m going to dig a bit deeper to find out why such advice is given and how the Imager performs this configuration step.&lt;/p&gt;
&lt;h1 id=&#34;network-manager&#34;&gt;
  Network Manager
  
&lt;/h1&gt;
&lt;p&gt;In the &lt;a href=&#34;https://www.raspberrypi.com/news/bookworm-the-new-version-of-raspberry-pi-os/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;announcement&lt;/a&gt; of the new OS release, one of the highlights is the move to &lt;a href=&#34;https://networkmanager.dev/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NetworkManager&lt;/a&gt; as the default mechanism to deal with networking. While this move undoubtely brings many advantages, it is the reason why the classic technique of dropping a &lt;code&gt;wpa_supplicant.conf&lt;/code&gt; file under &lt;code&gt;/etc/wpa_supplicant/&lt;/code&gt; no longer works.&lt;/p&gt;
&lt;p&gt;The good news is that also NetworkManager can be manually configured with a text file. The file needs to be called &lt;code&gt;SSID.nmconnection&lt;/code&gt; (replace &lt;code&gt;SSID&lt;/code&gt; with your network&amp;rsquo;s SSID) and placed under &lt;code&gt;/etc/NetworkManager/system-connections/&lt;/code&gt; in the Pi&amp;rsquo;s &lt;code&gt;rootfs&lt;/code&gt; partition.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-toml&#34; data-lang=&#34;toml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[connection]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;id=SSID
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;uuid= &lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# random UUID in the format 11111111-1111-1111-1111-111111111111&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;type=wifi
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;autoconnect=&lt;span style=&#34;color:#79c0ff&#34;&gt;true&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[wifi]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mode=infrastructure
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ssid=SSID
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[wifi-security]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;auth-alg=open
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;key-mgmt=wpa-psk
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;psk=PASSWORD
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[ipv4]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;method=auto
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[ipv6]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;method=auto
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;(replace &lt;code&gt;SSID&lt;/code&gt; and &lt;code&gt;PASSWORD&lt;/code&gt; with your wifi network&amp;rsquo;s SSID and password). &lt;a href=&#34;https://developer-old.gnome.org/NetworkManager/stable/nm-settings-keyfile.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Here&lt;/a&gt; you can find the full syntax for this file.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll need also to configure its access rights as:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo chmod -R &lt;span style=&#34;color:#a5d6ff&#34;&gt;600&lt;/span&gt; &amp;lt;path-to-rootfs&amp;gt;/etc/NetworkManager/system-connections/SSID.nmconnection
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo chown -R root:root &amp;lt;path-to-rootfs&amp;gt;/etc/NetworkManager/system-connections/SSID.nmconnection
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once this is done, let&amp;rsquo;s not forget to create an empty &lt;code&gt;ssh&lt;/code&gt; file in the &lt;code&gt;bootfs&lt;/code&gt; partition to enable the SSH server:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;touch &amp;lt;path-to-bootfs&amp;gt;/ssh
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;and, as it was already the case in Bullseye to &lt;a href=&#34;https://www.raspberrypi.com/news/raspberry-pi-bullseye-update-april-2022/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;configure the default user&lt;/a&gt; with &lt;code&gt;userconfig.txt&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;echo &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#39;mypassword&amp;#39;&lt;/span&gt; | openssl passwd -6 -stdin | awk &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#39;{print &amp;#34;myuser:&amp;#34;$1}&amp;#39;&lt;/span&gt; &amp;gt; &amp;lt;path-to-bootfs&amp;gt;/userconfig.txt
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So far it doesn&amp;rsquo;t seem too complicated. However, interestingly, this is &lt;strong&gt;not&lt;/strong&gt; what the Raspberry Pi Imager does, because if you use it to flash the image and check the result, these files are nowhere to be found. Is there a better way to go about this?&lt;/p&gt;
&lt;h1 id=&#34;raspberry-pi-imager&#34;&gt;
  Raspberry Pi Imager
  
&lt;/h1&gt;
&lt;p&gt;To find out what the Imager does, my first idea was to have a peek at its &lt;a href=&#34;https://github.com/raspberrypi/rpi-imager&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source code&lt;/a&gt;. Being a Qt application the source might be quite intimidating, but with a some searching it&amp;rsquo;s possible to locate this interesting &lt;a href=&#34;https://github.com/raspberrypi/rpi-imager/blob/6f6a90adbb88c135534d5f20cc2a10f167ea43a3/src/imagewriter.cpp#L1214&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;snippet&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;void&lt;/span&gt; ImageWriter&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;::&lt;/span&gt;setImageCustomization(&lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; QByteArray &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;amp;&lt;/span&gt;config, &lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; QByteArray &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;amp;&lt;/span&gt;cmdline, &lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; QByteArray &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;amp;&lt;/span&gt;firstrun, &lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; QByteArray &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;amp;&lt;/span&gt;cloudinit, &lt;span style=&#34;color:#ff7b72&#34;&gt;const&lt;/span&gt; QByteArray &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;amp;&lt;/span&gt;cloudinitNetwork)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    _config &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; config;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    _cmdline &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; cmdline;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    _firstrun &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; firstrun;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    _cloudinit &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; cloudinit;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    _cloudinitNetwork &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; cloudinitNetwork;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    qDebug() &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Custom config.txt entries:&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; config;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    qDebug() &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Custom cmdline.txt entries:&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; cmdline;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    qDebug() &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Custom firstuse.sh:&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; firstrun;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    qDebug() &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Cloudinit:&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; cloudinit;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I&amp;rsquo;m no C++ expert, but this function tells me a few things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The Imager writes the configuration in these files: &lt;code&gt;config.txt&lt;/code&gt;, &lt;code&gt;cmdline.txt&lt;/code&gt;, &lt;code&gt;firstuse.sh&lt;/code&gt; (we&amp;rsquo;ll soon figure out this is a typo: the file is actually called &lt;code&gt;firstrun.sh&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;It also prepares a &amp;ldquo;Cloudinit&amp;rdquo; configuration file, but it&amp;rsquo;s unclear if it writes it and where&lt;/li&gt;
&lt;li&gt;The content of these files is printed to the console as debug output.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So let&amp;rsquo;s enable the debug logs and see what they produce:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;rpi-imager --debug
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The console stays quiet until I configure the user, password, WiFi and so on in the Imager, at which point it starts printing all the expected configuration files to the console.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;&lt;i&gt;Click here to see the full output&lt;/i&gt;&lt;/summary&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Custom config.txt entries: &amp;#34;&amp;#34;
Custom cmdline.txt entries: &amp;#34; cfg80211.ieee80211_regdom=PT&amp;#34;
Custom firstuse.sh: &amp;#34;#!/bin/bash

set +e

CURRENT_HOSTNAME=`cat /etc/hostname | tr -d &amp;#34; \    \
\\r&amp;#34;`
if [ -f /usr/lib/raspberrypi-sys-mods/imager_custom ]; then
   /usr/lib/raspberrypi-sys-mods/imager_custom set_hostname raspberrypi
else
   echo raspberrypi &amp;gt;/etc/hostname
   sed -i &amp;#34;s/127.0.1.1.*$CURRENT_HOSTNAME/127.0.1.1\    raspberrypi/g&amp;#34; /etc/hosts
fi
FIRSTUSER=`getent passwd 1000 | cut -d: -f1`
FIRSTUSERHOME=`getent passwd 1000 | cut -d: -f6`
if [ -f /usr/lib/raspberrypi-sys-mods/imager_custom ]; then
   /usr/lib/raspberrypi-sys-mods/imager_custom enable_ssh
else
   systemctl enable ssh
fi
if [ -f /usr/lib/userconf-pi/userconf ]; then
   /usr/lib/userconf-pi/userconf &amp;#39;myuser&amp;#39; &amp;#39;&amp;lt;hash-of-the-user-password&amp;gt;&amp;#39;
else
   echo &amp;#34;$FIRSTUSER:&amp;#34;&amp;#39;&amp;lt;hash-of-the-user-password&amp;gt;&amp;#39; | chpasswd -e
   if [ &amp;#34;$FIRSTUSER&amp;#34; != &amp;#34;myuser&amp;#34; ]; then
      usermod -l &amp;#34;myuser&amp;#34; &amp;#34;$FIRSTUSER&amp;#34;
      usermod -m -d &amp;#34;/home/myuser&amp;#34; &amp;#34;myuser&amp;#34;
      groupmod -n &amp;#34;myuser&amp;#34; &amp;#34;$FIRSTUSER&amp;#34;
      if grep -q &amp;#34;^autologin-user=&amp;#34; /etc/lightdm/lightdm.conf ; then
         sed /etc/lightdm/lightdm.conf -i -e &amp;#34;s/^autologin-user=.*/autologin-user=myuser/&amp;#34;
      fi
      if [ -f /etc/systemd/system/getty@tty1.service.d/autologin.conf ]; then
         sed /etc/systemd/system/getty@tty1.service.d/autologin.conf -i -e &amp;#34;s/$FIRSTUSER/myuser/&amp;#34;
      fi
      if [ -f /etc/sudoers.d/010_pi-nopasswd ]; then
         sed -i &amp;#34;s/^$FIRSTUSER /myuser /&amp;#34; /etc/sudoers.d/010_pi-nopasswd
      fi
   fi
fi
if [ -f /usr/lib/raspberrypi-sys-mods/imager_custom ]; then
   /usr/lib/raspberrypi-sys-mods/imager_custom set_wlan &amp;#39;MY-SSID&amp;#39; &amp;#39;MY-PASSWORD&amp;#39; &amp;#39;PT&amp;#39;
else
cat &amp;gt;/etc/wpa_supplicant/wpa_supplicant.conf &amp;lt;&amp;lt;&amp;#39;WPAEOF&amp;#39;
country=PT
ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev
ap_scan=1

update_config=1
network={
    ssid=&amp;#34;MY-SSID&amp;#34;
    psk=MY-PASSWORD
}

WPAEOF
   chmod 600 /etc/wpa_supplicant/wpa_supplicant.conf
   rfkill unblock wifi
   for filename in /var/lib/systemd/rfkill/*:wlan ; do
       echo 0 &amp;gt; $filename
   done
fi
if [ -f /usr/lib/raspberrypi-sys-mods/imager_custom ]; then
   /usr/lib/raspberrypi-sys-mods/imager_custom set_keymap &amp;#39;us&amp;#39;
   /usr/lib/raspberrypi-sys-mods/imager_custom set_timezone &amp;#39;Europe/Lisbon&amp;#39;
else
   rm -f /etc/localtime
   echo &amp;#34;Europe/Lisbon&amp;#34; &amp;gt;/etc/timezone
   dpkg-reconfigure -f noninteractive tzdata
cat &amp;gt;/etc/default/keyboard &amp;lt;&amp;lt;&amp;#39;KBEOF&amp;#39;
XKBMODEL=&amp;#34;pc105&amp;#34;
XKBLAYOUT=&amp;#34;us&amp;#34;
XKBVARIANT=&amp;#34;&amp;#34;
XKBOPTIONS=&amp;#34;&amp;#34;

KBEOF
   dpkg-reconfigure -f noninteractive keyboard-configuration
fi
rm -f /boot/firstrun.sh
sed -i &amp;#39;s| systemd.run.*||g&amp;#39; /boot/cmdline.txt
exit 0
&amp;#34;

Cloudinit: &amp;#34;hostname: raspberrypi
manage_etc_hosts: true
packages:
- avahi-daemon
apt:
  conf: |
    Acquire {
      Check-Date &amp;#34;false&amp;#34;;
    };

users:
- name: myuser
  groups: users,adm,dialout,audio,netdev,video,plugdev,cdrom,games,input,gpio,spi,i2c,render,sudo
  shell: /bin/bash
  lock_passwd: false
  passwd: &amp;lt;hash-of-the-user-password&amp;gt;

ssh_pwauth: true

timezone: Europe/Lisbon
runcmd:
- localectl set-x11-keymap &amp;#34;us&amp;#34; pc105
- setupcon -k --force || true


&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;/details&gt;
&lt;p&gt;Among these the most interesting file is &lt;code&gt;firstrun.sh&lt;/code&gt;, which we can quickly locate in the &lt;code&gt;bootfs&lt;/code&gt; partition. Here is its content:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-weight:bold;font-style:italic&#34;&gt;#!/bin/bash
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-weight:bold;font-style:italic&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;set +e
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#79c0ff&#34;&gt;CURRENT_HOSTNAME&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;`&lt;/span&gt;cat /etc/hostname | tr -d &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34; \    \
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;\\r&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;`&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;[&lt;/span&gt; -f /usr/lib/raspberrypi-sys-mods/imager_custom &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;]&lt;/span&gt;; &lt;span style=&#34;color:#ff7b72&#34;&gt;then&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   /usr/lib/raspberrypi-sys-mods/imager_custom set_hostname raspberrypi
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;else&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   echo raspberrypi &amp;gt;/etc/hostname
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   sed -i &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;s/127.0.1.1.*&lt;/span&gt;&lt;span style=&#34;color:#79c0ff&#34;&gt;$CURRENT_HOSTNAME&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;/127.0.1.1\    raspberrypi/g&amp;#34;&lt;/span&gt; /etc/hosts
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;fi&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#79c0ff&#34;&gt;FIRSTUSER&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;`&lt;/span&gt;getent passwd &lt;span style=&#34;color:#a5d6ff&#34;&gt;1000&lt;/span&gt; | cut -d: -f1&lt;span style=&#34;color:#a5d6ff&#34;&gt;`&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#79c0ff&#34;&gt;FIRSTUSERHOME&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;`&lt;/span&gt;getent passwd &lt;span style=&#34;color:#a5d6ff&#34;&gt;1000&lt;/span&gt; | cut -d: -f6&lt;span style=&#34;color:#a5d6ff&#34;&gt;`&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;[&lt;/span&gt; -f /usr/lib/raspberrypi-sys-mods/imager_custom &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;]&lt;/span&gt;; &lt;span style=&#34;color:#ff7b72&#34;&gt;then&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   /usr/lib/raspberrypi-sys-mods/imager_custom enable_ssh
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;else&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   systemctl enable ssh
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;fi&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;[&lt;/span&gt; -f /usr/lib/userconf-pi/userconf &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;]&lt;/span&gt;; &lt;span style=&#34;color:#ff7b72&#34;&gt;then&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   /usr/lib/userconf-pi/userconf &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#39;myuser&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#39;&amp;lt;hash-of-the-user-password&amp;gt;&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;else&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   echo &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#79c0ff&#34;&gt;$FIRSTUSER&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;:&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#39;&amp;lt;hash-of-the-user-password&amp;gt;&amp;#39;&lt;/span&gt; | chpasswd -e
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   &lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;[&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#79c0ff&#34;&gt;$FIRSTUSER&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&lt;/span&gt; !&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;myuser&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;]&lt;/span&gt;; &lt;span style=&#34;color:#ff7b72&#34;&gt;then&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      usermod -l &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;myuser&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#79c0ff&#34;&gt;$FIRSTUSER&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      usermod -m -d &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;/home/myuser&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;myuser&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      groupmod -n &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;myuser&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#79c0ff&#34;&gt;$FIRSTUSER&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; grep -q &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;^autologin-user=&amp;#34;&lt;/span&gt; /etc/lightdm/lightdm.conf ; &lt;span style=&#34;color:#ff7b72&#34;&gt;then&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;         sed /etc/lightdm/lightdm.conf -i -e &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;s/^autologin-user=.*/autologin-user=myuser/&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#ff7b72&#34;&gt;fi&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;[&lt;/span&gt; -f /etc/systemd/system/getty@tty1.service.d/autologin.conf &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;]&lt;/span&gt;; &lt;span style=&#34;color:#ff7b72&#34;&gt;then&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;         sed /etc/systemd/system/getty@tty1.service.d/autologin.conf -i -e &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;s/&lt;/span&gt;&lt;span style=&#34;color:#79c0ff&#34;&gt;$FIRSTUSER&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;/myuser/&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#ff7b72&#34;&gt;fi&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;[&lt;/span&gt; -f /etc/sudoers.d/010_pi-nopasswd &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;]&lt;/span&gt;; &lt;span style=&#34;color:#ff7b72&#34;&gt;then&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;         sed -i &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;s/^&lt;/span&gt;&lt;span style=&#34;color:#79c0ff&#34;&gt;$FIRSTUSER&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt; /myuser /&amp;#34;&lt;/span&gt; /etc/sudoers.d/010_pi-nopasswd
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#ff7b72&#34;&gt;fi&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   &lt;span style=&#34;color:#ff7b72&#34;&gt;fi&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;fi&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;[&lt;/span&gt; -f /usr/lib/raspberrypi-sys-mods/imager_custom &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;]&lt;/span&gt;; &lt;span style=&#34;color:#ff7b72&#34;&gt;then&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   /usr/lib/raspberrypi-sys-mods/imager_custom set_wlan &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#39;MY-SSID&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#39;MY-PASSWORD&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#39;PT&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;else&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cat &amp;gt;/etc/wpa_supplicant/wpa_supplicant.conf &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;lt;&amp;lt;&amp;#39;WPAEOF&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;country=PT
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;ap_scan=1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;update_config=1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;network={
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;    ssid=&amp;#34;MY-SSID&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;    psk=MY-PASSWORD
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;WPAEOF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   chmod &lt;span style=&#34;color:#a5d6ff&#34;&gt;600&lt;/span&gt; /etc/wpa_supplicant/wpa_supplicant.conf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   rfkill unblock wifi
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   &lt;span style=&#34;color:#ff7b72&#34;&gt;for&lt;/span&gt; filename in /var/lib/systemd/rfkill/*:wlan ; &lt;span style=&#34;color:#ff7b72&#34;&gt;do&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;       echo &lt;span style=&#34;color:#a5d6ff&#34;&gt;0&lt;/span&gt; &amp;gt; &lt;span style=&#34;color:#79c0ff&#34;&gt;$filename&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   &lt;span style=&#34;color:#ff7b72&#34;&gt;done&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;fi&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;[&lt;/span&gt; -f /usr/lib/raspberrypi-sys-mods/imager_custom &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;]&lt;/span&gt;; &lt;span style=&#34;color:#ff7b72&#34;&gt;then&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   /usr/lib/raspberrypi-sys-mods/imager_custom set_keymap &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#39;us&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   /usr/lib/raspberrypi-sys-mods/imager_custom set_timezone &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#39;Europe/Lisbon&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;else&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   rm -f /etc/localtime
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   echo &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Europe/Lisbon&amp;#34;&lt;/span&gt; &amp;gt;/etc/timezone
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   dpkg-reconfigure -f noninteractive tzdata
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cat &amp;gt;/etc/default/keyboard &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;lt;&amp;lt;&amp;#39;KBEOF&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;XKBMODEL=&amp;#34;pc105&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;XKBLAYOUT=&amp;#34;us&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;XKBVARIANT=&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;XKBOPTIONS=&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;KBEOF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   dpkg-reconfigure -f noninteractive keyboard-configuration
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;fi&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;rm -f /boot/firstrun.sh
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sed -i &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#39;s| systemd.run.*||g&amp;#39;&lt;/span&gt; /boot/cmdline.txt
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;exit &lt;span style=&#34;color:#a5d6ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;details&gt;
&lt;summary&gt; &lt;i&gt;Side note: how does the OS know that it should run this file on  its first boot? &lt;/i&gt;&lt;/summary&gt;
&lt;p&gt;Imager also writes a file called &lt;code&gt;cmdline.txt&lt;/code&gt; in the boot partition, which contains the following:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;console=serial0,115200 console=tty1 root=PARTUUID=57c84f67-02 rootfstype=ext4 fsck.repair=yes rootwait quiet init=/usr/lib/raspberrypi-sys-mods/firstboot cfg80211.ieee80211_regdom=PT systemd.run=/boot/firstrun.sh systemd.run_success_action=reboot systemd.unit=kernel-command-line.target
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Note the reference to &lt;code&gt;/boot/firstrun.sh&lt;/code&gt;. If you plan to implement your own &lt;code&gt;firstrun.sh&lt;/code&gt; file and want to change its name, don&amp;rsquo;t forget to modify this line as well.&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;That&amp;rsquo;s a lot of Bash in one go, but upon inspection one can spot a recurring pattern. For example, when setting the hostname, it does this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;[&lt;/span&gt; -f /usr/lib/raspberrypi-sys-mods/imager_custom &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;]&lt;/span&gt;; &lt;span style=&#34;color:#ff7b72&#34;&gt;then&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   /usr/lib/raspberrypi-sys-mods/imager_custom set_hostname raspberrypi
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;else&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   echo raspberrypi &amp;gt;/etc/hostname
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   sed -i &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;s/127.0.1.1.*&lt;/span&gt;&lt;span style=&#34;color:#79c0ff&#34;&gt;$CURRENT_HOSTNAME&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;/127.0.1.1\    raspberrypi/g&amp;#34;&lt;/span&gt; /etc/hosts
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;fi&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The script clearly messages that there is a &amp;ldquo;preferred&amp;rdquo; way to set the hostname: to use &lt;code&gt;/usr/lib/raspberrypi-sys-mods/imager_custom set_hostname [NAME]&lt;/code&gt;. Only if this executable is not available, then it falls back to the &amp;ldquo;traditional&amp;rdquo; way of setting the hostname by editing &lt;code&gt;/etc/hosts&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The same patterns repeat a few times to perform the following operations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;set the hostname (&lt;code&gt;/usr/lib/raspberrypi-sys-mods/imager_custom set_hostname [NAME]&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;enable ssh (&lt;code&gt;/usr/lib/raspberrypi-sys-mods/imager_custom enable_ssh&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;configure the user (&lt;code&gt;/usr/lib/userconf-pi/userconf [USERNAME] [HASHED-PASSWORD]&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;configure the WiFi (&lt;code&gt;/usr/lib/raspberrypi-sys-mods/imager_custom set_wlan [MY-SSID [MY-PASSWORD] [2-LETTER-COUNTRY-CODE]&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;set the keyboard layout (&lt;code&gt;/usr/lib/raspberrypi-sys-mods/imager_custom set_keymap [CODE]&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;set the timezone (&lt;code&gt;/usr/lib/raspberrypi-sys-mods/imager_custom set_timezone [TIMEZONE-NAME]&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It seems like using &lt;code&gt;raspberrypi-sys-mods&lt;/code&gt; to configure the OS at the first boot is the way to go in this RPi OS version, and it might be true in future versions as well. There are &lt;a href=&#34;https://github.com/RPi-Distro/raspberrypi-sys-mods/issues/82#issuecomment-1779109991&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;hints&lt;/a&gt; that the Raspberry PI OS team is going to move to &lt;a href=&#34;https://cloudinit.readthedocs.io/en/latest/index.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;cloud-init&lt;/code&gt;&lt;/a&gt; in the near future, but for now this seems to be the way that the initial setup is done.&lt;/p&gt;
&lt;h1 id=&#34;raspberrypi-sys-mods&#34;&gt;
  raspberrypi-sys-mods
  
&lt;/h1&gt;
&lt;p&gt;So let&amp;rsquo;s check out what &lt;code&gt;raspberrypi-sys-mods&lt;/code&gt; do! The source code can be found here: &lt;a href=&#34;https://github.com/RPi-Distro/raspberrypi-sys-mods&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raspberrypi-sys-mods&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Given that we&amp;rsquo;re interested in the WiFi configuration, let&amp;rsquo;s head straight to the &lt;code&gt;imager_custom&lt;/code&gt; script (&lt;a href=&#34;https://github.com/RPi-Distro/raspberrypi-sys-mods/blob/2e256445b65995f62db80e6a267313275cad51e4/usr/lib/raspberrypi-sys-mods/imager_custom#L97&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;), where we discover that it&amp;rsquo;s a Bash script which does this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#79c0ff&#34;&gt;CONNFILE&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;/etc/NetworkManager/system-connections/preconfigured.nmconnection
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#79c0ff&#34;&gt;UUID&lt;/span&gt;&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;$(&lt;/span&gt;uuid -v4&lt;span style=&#34;color:#ff7b72&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  cat &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;lt;&amp;lt;- EOF &amp;gt;${CONNFILE}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;	[connection]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;	id=preconfigured
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;	uuid=${UUID}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;	type=wifi
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;	[wifi]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;	mode=infrastructure
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;	ssid=${SSID}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;	hidden=${HIDDEN}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;	[ipv4]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;	method=auto
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;	[ipv6]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;	addr-gen-mode=default
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;	method=auto
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;	[proxy]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;	EOF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;[&lt;/span&gt; ! -z &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#79c0ff&#34;&gt;PASS&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;]&lt;/span&gt;; &lt;span style=&#34;color:#ff7b72&#34;&gt;then&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    cat &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;lt;&amp;lt;- EOF &amp;gt;&amp;gt;${CONNFILE}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;	[wifi-security]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;	key-mgmt=wpa-psk
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;	psk=${PASS}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;	EOF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#ff7b72&#34;&gt;fi&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# NetworkManager will ignore nmconnection files with incorrect permissions,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# to prevent Wi-Fi credentials accidentally being world-readable.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  chmod &lt;span style=&#34;color:#a5d6ff&#34;&gt;600&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#79c0ff&#34;&gt;CONNFILE&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So after all this searching, we&amp;rsquo;re back to square one. This utility is doing exactly what we&amp;rsquo;ve done at the start: it writes a NetworkManager configuration file called &lt;code&gt;preconfigured.nmconnection&lt;/code&gt; and it fills it in with the information that we&amp;rsquo;ve provided to the Imager, then changes the permissions to make sure NetworkManager can use it.&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;
  Conclusion
  
&lt;/h1&gt;
&lt;p&gt;It would be great if the Raspberry Pi OS team would expand their documentation to include this information, so that users aren&amp;rsquo;t left wondering what makes the RPi Imager so special and whether their manual setup is the right way to go or rather a hack that is likely to break. For now it seems like there is one solid good approach to this problem, and we are going to see what is going to change in the next version of the Raspberry Pi OS.&lt;/p&gt;
&lt;p&gt;On this note you should remember that doing a manual configuration of NetworkManager, using the Imager, or using &lt;code&gt;raspberrypi-sys-mods&lt;/code&gt; may be nearly identical right now, but when choosing which approach to use for your project you should also keep in mind the maintenance burden that this decision brings.&lt;/p&gt;
&lt;p&gt;Doing a manual configuration is easier on many levels, but only if you don&amp;rsquo;t intend to support other versions of RPi OS. If you do, or if you expect to migrate when a new version comes out, you should consider doing something similar to what the Imager does: use a &lt;code&gt;firstrun.sh&lt;/code&gt; file that tries to use &lt;code&gt;raspberrypi-sys-mods&lt;/code&gt; and falls back to a manual configuration only if that executable is missing. That is likely to make migrations easier if the Raspberry Pi OS team should choose once again to modify the way that headless setups work.&lt;/p&gt;
&lt;p class=&#34;fleuron&#34;&gt;&lt;a href=&#34;https://www.zansara.dev/posts/2024-05-06-teranoptia/&#34;&gt;FŻ&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Haystack</title>
      <link>https://www.zansara.dev/projects/haystack/</link>
      <pubDate>Sun, 31 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/projects/haystack/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://haystack.deepset.ai/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haystack&lt;/a&gt; is an &lt;a href=&#34;https://github.com/deepset-ai/haystack&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;open-source&lt;/a&gt; end-to-end framework that allows you to build applications powered by LLMs, Transformer models, vector search and more. Whether you want to perform retrieval-augmented generation (RAG), document search, question answering, or build agent-based workflows, Haystack can orchestrate state-of-the-art embedding models and LLMs into pipelines to build end-to-end NLP applications and solve your use case. Haystack has also an active &lt;a href=&#34;https://discord.com/invite/xYvH6drSmA&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Discord&lt;/a&gt; community you can join to learn more about the framework and get help if you face problems.&lt;/p&gt;
&lt;p&gt;In early 2024 Haystack published a &lt;a href=&#34;https://haystack.deepset.ai/release-notes/2.0.0&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;major release (2.0)&lt;/a&gt;, which was a full redesign of the framework. During my time at &lt;a href=&#34;https://www.deepset.ai/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;deepset&lt;/a&gt; leading this redesign has been one of my major accomplishments.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Demos and notebooks</title>
      <link>https://www.zansara.dev/projects/demos/</link>
      <pubDate>Sat, 30 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/projects/demos/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DataHour: Optimizing LLMs with Retrieval Augmented Generation and Haystack 2.0</title>
      <link>https://www.zansara.dev/talks/2023-12-15-datahour-rag/</link>
      <pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/talks/2023-12-15-datahour-rag/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1OkFr4u9ZOraJRF406IQgQh4YC8GLHbzA/view?usp=drive_link&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recording&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/1n1tbiUW2wZPGC49WK9pYEIZlZuCER-hu/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/17FXuS7X70UF02IYmOr-yEDQYg_gp9cFv/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colab&lt;/a&gt;, &lt;a href=&#34;https://gist.github.com/ZanSara/6075d418c1494e780f7098db32bc6cf6&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gist&lt;/a&gt;. All the material can also be found on &lt;a href=&#34;https://community.analyticsvidhya.com/c/datahour/optimizing-llms-with-retrieval-augmented-generation-and-haystack-2-0&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Analytics Vidhya&amp;rsquo;s community&lt;/a&gt; and on &lt;a href=&#34;https://drive.google.com/drive/folders/1KwCEDTCsm9hrRaFUPHpzdTpVsOJSnvGk?usp=drive_link&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my backup&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;div class=&#39;iframe-wrapper&#39;&gt;
  &lt;iframe src=&#34;https://drive.google.com/file/d/1OkFr4u9ZOraJRF406IQgQh4YC8GLHbzA/preview&#34; width=100% height=100% allow=&#34;autoplay&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;In this hour-long workshop organized by &lt;a href=&#34;https://www.analyticsvidhya.com/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Analytics Vidhya&lt;/a&gt; I give an overview of what RAG is, what problems it solves, and how it works.&lt;/p&gt;
&lt;p&gt;After a brief introduction to Haystack, I show in practice how to use Haystack 2.0 to assemble a Pipeline that performs RAG on a local database and then on the Web with a simple change.&lt;/p&gt;
&lt;p&gt;I also mention how to use and implement custom Haystack components, and share a lot of resources on the topic of RAG and Haystack 2.0.&lt;/p&gt;
&lt;p&gt;This was my most popular talk to date, with over a hundred attendees watching live and several questions.&lt;/p&gt;
&lt;p&gt;Other resources mentioned in the talk are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://haystack.deepset.ai/blog/customizing-rag-to-summarize-hacker-news-posts-with-haystack2?utm_campaign=developer-relations&amp;amp;utm_source=data-hour-event&amp;amp;utm_medium=webinar&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog post about custom components&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://haystack.deepset.ai/tutorials/28_structured_output_with_loop?utm_campaign=developer-relations&amp;amp;utm_source=data-hour-event&amp;amp;utm_medium=webinar&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LLM structured output example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://haystack.deepset.ai/advent-of-haystack?utm_campaign=developer-relations&amp;amp;utm_source=data-hour-event&amp;amp;utm_medium=webinar&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Advent of Haystack&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Optimizing LLMs with Retrieval Augmented Generation and Haystack 2.0</title>
      <link>https://www.zansara.dev/demos/2023-12-15-optimizing-llms-with-rag-and-haystack/</link>
      <pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/demos/2023-12-15-optimizing-llms-with-rag-and-haystack/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Pointer[183]: Haystack, creare LLM Applications in modo facile</title>
      <link>https://www.zansara.dev/talks/2023-12-15-pointerpodcast-haystack/</link>
      <pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/talks/2023-12-15-pointerpodcast-haystack/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://pointerpodcast.it/p/pointer183-haystack-creare-llm-applications-in-modo-facile-con-stefano-fiorucci-e-sara-zanzottera&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Episode link&lt;/a&gt;. Backup recording &lt;a href=&#34;https://drive.google.com/file/d/1BOoAhfvWou_J4J7RstgKAHPs3Pre2YAw/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;The podcast was recorded in Italian for &lt;a href=&#34;https://pointerpodcast.it&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PointerPodcast&lt;/a&gt; with &lt;a href=&#34;https://www.linkedin.com/in/luca-corbucci-b6156a123/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Luca Corbucci&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/eugenio-paluello-851b3280/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Eugenio Paluello&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/stefano-fiorucci/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stefano Fiorucci&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;


&lt;div&gt;
&lt;audio controls src=&#34;https://hosting.pointerpodcast.it/records/pointer183.mp3&#34; style=&#34;width: 100%&#34;&gt;&lt;/audio&gt;
&lt;/div&gt;


&lt;p&gt;Per concludere in grande stile il 2023, in questa puntata ci occupiamo delle LLM che sono state un argomento centrale della scena tech dell&amp;rsquo;anno che sta per terminare. Abbiamo invitato due esperti del settore, Sara Zanzottera e Stefano Fiorucci.&lt;/p&gt;
&lt;p&gt;Entrambi i nostri ospiti lavorano per deepset come NLP Engineer. Deepset è l&amp;rsquo;azienda produttrice di Haystack uno dei framework opensource per LLM più noti, che ha da poco raggiunto la versione 2.0 beta. Proprio Haystack è stato uno degli argomenti di cui ci siamo occupati con i nostri ospiti, cercando di capirne le potenzialità.&lt;/p&gt;
&lt;p&gt;Ma è possibile riuscire a lavorare ad un framework di questo tipo rimanendo anche aggiornati sulle novità di un mondo in costante evoluzione? Questa è una delle tante domande a cui Sara e Stefano hanno risposto. Vi interessa il mondo delle LLM? Non perdetevi questa puntata!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Brekeke</title>
      <link>https://www.zansara.dev/projects/brekeke/</link>
      <pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/projects/brekeke/</guid>
      <description>&lt;p&gt;With the rise of more and more powerful LLMs, I am experimenting with different ways to interact with them in ways that don&amp;rsquo;t necessarily involve a laptop, a keyboard or a screen.&lt;/p&gt;
&lt;p&gt;I codenamed all of these experiments &amp;ldquo;Brekeke&amp;rdquo;, the sound frogs make in Hungarian (don&amp;rsquo;t ask why). The focus of these experiments is mostly small home automation tasks and run on a swarm of Raspberry Pis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Haystack 2.0: The World of Web RAG</title>
      <link>https://www.zansara.dev/demos/2023-11-09-the-world-of-web-rag/</link>
      <pubDate>Thu, 09 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/demos/2023-11-09-the-world-of-web-rag/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The World of Web RAG</title>
      <link>https://www.zansara.dev/posts/2023-11-09-haystack-series-simple-web-rag/</link>
      <pubDate>Thu, 09 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2023-11-09-haystack-series-simple-web-rag/</guid>
      <description>&lt;p&gt;&lt;em&gt;Last updated: 18/01/2024&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;In an earlier post of the Haystack 2.0 series, we&amp;rsquo;ve seen how to build RAG and indexing pipelines. An application that uses these two pipelines is practical if you have an extensive, private collection of documents and need to perform RAG on such data only. However, in many cases, you may want to get data from the Internet: from news outlets, documentation pages, and so on.&lt;/p&gt;
&lt;p&gt;In this post, we will see how to build a Web RAG application: a RAG pipeline that can search the Web for the information needed to answer your questions.&lt;/p&gt;
&lt;div class=&#34;notice info&#34;&gt;
  &lt;div class=&#34;notice-content&#34;&gt;💡 &lt;em&gt;Do you want to see the code in action? Check out the &lt;a href=&#34;https://colab.research.google.com/drive/1dGMPxReo730j7_zQDZOu-0SGf-pk4XDL?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colab notebook&lt;/a&gt; or the &lt;a href=&#34;https://gist.github.com/ZanSara/0907a8f3ae19f62998cc061ed6e8ce53&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gist&lt;/a&gt;.&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;notice warning&#34;&gt;
  &lt;div class=&#34;notice-content&#34;&gt;&lt;i&gt;⚠️ &lt;strong&gt;Warning:&lt;/strong&gt;&lt;/i&gt; &lt;em&gt;This code was tested on &lt;code&gt;haystack-ai==2.0.0b5&lt;/code&gt;. Haystack 2.0 is still unstable, so later versions might introduce breaking changes without notice until Haystack 2.0 is officially released. The concepts and components, however, stay the same.&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;h1 id=&#34;searching-the-web&#34;&gt;
  Searching the Web
  
&lt;/h1&gt;
&lt;p&gt;As we&amp;rsquo;ve seen &lt;a href=&#34;https://www.zansara.dev/posts/2023-10-27-haystack-series-rag&#34; &gt;earlier&lt;/a&gt;, a Haystack RAG Pipeline is made of three components: a Retriever, a PromptBuilder, and a Generator, and looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-11-09-haystack-series-simple-web-rag/bm25-rag-pipeline.png&#34; alt=&#34;BM25 RAG Pipeline&#34;&gt;&lt;/p&gt;
&lt;p&gt;To make this pipeline use the Web as its data source, we need to change the retriever with a component that does not look into a local document store for information but can search the web.&lt;/p&gt;
&lt;p&gt;Haystack 2.0 already provides a search engine component called &lt;code&gt;SerperDevWebSearch&lt;/code&gt;. It uses &lt;a href=&#34;https://serper.dev/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SerperDev&amp;rsquo;s API&lt;/a&gt; to query popular search engines and return two types of data: a list of text snippets coming from the search engine&amp;rsquo;s preview boxes and a list of links, which point to the top search results.&lt;/p&gt;
&lt;p&gt;To begin, let&amp;rsquo;s see how to use this component in isolation.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.components.websearch&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; SerperDevWebSearch
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;question &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;What&amp;#39;s the official language of the Republic of Rose Island?&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;search &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; SerperDevWebSearch(api_key&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;serperdev_api_key)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;results &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; search&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run(query&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;question)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns {&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     &amp;#34;documents&amp;#34;: [&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         Document(content=&amp;#39;Esperanto&amp;#39;, meta={&amp;#39;title&amp;#39;: &amp;#39;Republic of Rose Island - Wikipedia&amp;#39;, &amp;#39;link&amp;#39;: &amp;#39;https://en.wikipedia.org/wiki/Republic_of_Rose_Island&amp;#39;}),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         Document(content=&amp;#34;The Republic of Rose Island was a short-lived micronation on a man-made platform in the Adriatic Sea. It&amp;#39;s a story that few people knew of until recently, ...&amp;#34;, meta={&amp;#39;title&amp;#39;: &amp;#39;Rose Island - The story of a micronation&amp;#39;, &amp;#39;link&amp;#39;: &amp;#39;https://www.rose-island.co/&amp;#39;, &amp;#39;imageUrl&amp;#39;: &amp;#39;https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQiRCfTO6OwFS32SX37S-7OadDZCNK6Fy_NZVGsci2gcIS-zcinhOcGhgU&amp;amp;s&amp;#39;, &amp;#39;position&amp;#39;: 1},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     ], &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     &amp;#34;links&amp;#34;: [&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         &amp;#39;https://www.rose-island.co/&amp;#39;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         &amp;#39;https://www.defactoborders.org/places/rose-island&amp;#39;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         ...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     ]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# }&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;SerperDevWebSearch&lt;/code&gt; is a component with a simple interface. Starting from its output, we can see that it returns not one but two different values in the returned dictionary: &lt;code&gt;documents&lt;/code&gt; and &lt;code&gt;links&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;links&lt;/code&gt; is the most straightforward and represents the top results that Google found relevant for the input query. It&amp;rsquo;s a list of strings, each containing a URL. You can configure the number of links to return with the &lt;code&gt;top_k&lt;/code&gt; init parameter.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;documents&lt;/code&gt; instead is a list of already fully formed Document objects. The content of these objects corresponds to the &amp;ldquo;answer boxes&amp;rdquo; that Google often returns together with its search results. Given that these code snippets are usually clean and short pieces of text, they&amp;rsquo;re perfect to be fed directly to an LLM without further processing.&lt;/p&gt;
&lt;p&gt;Other than expecting an API key as an init parameter and &lt;code&gt;top_k&lt;/code&gt; to control the number of results, &lt;code&gt;SerperDevWebSearch&lt;/code&gt; also accepts an &lt;code&gt;allowed_domains&lt;/code&gt; parameter, which lets you configure the domains Google is allowed to look into during search, and &lt;code&gt;search_params&lt;/code&gt;, a more generic dictionary input that lets you pass any additional search parameter SerperDev&amp;rsquo;s API understand.&lt;/p&gt;
&lt;h1 id=&#34;a-minimal-web-rag-pipeline&#34;&gt;
  A Minimal Web RAG Pipeline
  
&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;SerperDevWebSearch&lt;/code&gt; is actually the bare minimum we need to be able to build our very first Web RAG Pipeline. All we need to do is replace our original example&amp;rsquo;s Retriever with our search component.&lt;/p&gt;
&lt;p&gt;This is the result:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; Pipeline
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.components.builders&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; PromptBuilder
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.components.generators&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; OpenAIGenerator
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;template &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Question: {{ question }}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Google Search Answer Boxes:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;% f&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;or document in documents %}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;    {{ document.content }}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;% e&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;ndfor %}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Please reformulate the information above to 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;answer the user&amp;#39;s question.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;search&amp;#34;&lt;/span&gt;, SerperDevWebSearch(api_key&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;serperdev_api_key))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;, PromptBuilder(template&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;template))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;, OpenAIGenerator(api_key&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;api_key))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;search.documents&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder.documents&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;question &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;What&amp;#39;s the official language of the Republic of Rose Island?&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run({
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;search&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;query&amp;#34;&lt;/span&gt;: question},
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;question&amp;#34;&lt;/span&gt;: question}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;})
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns {&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     &amp;#39;llm&amp;#39;: {&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         &amp;#39;replies&amp;#39;: [&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#             &amp;#34;The official language of the Republic of Rose Island is Esperanto. This artificial language was chosen by the residents of Rose Island as their national language when they declared independence in 1968. However, it&amp;#39;s important to note that despite having their own language, government, currency, and postal service, Rose Island was never officially recognized as an independent nation by any country.&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         ],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         &amp;#39;metadata&amp;#39;: [...]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     }&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# }&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-11-09-haystack-series-simple-web-rag/minimal-web-rag-pipeline.png&#34; alt=&#34;Minimal Web RAG Pipeline&#34;&gt;&lt;/p&gt;
&lt;p&gt;This solution is already quite effective for simple questions because Google does most of the heavy lifting of reading the content of the top results, extracting the relevant snippets, and packaging them up in a way that is really easy to access and understand by the model.&lt;/p&gt;
&lt;p&gt;However, there are situations in which this approach is not sufficient. For example, for highly technical or nuanced questions, the answer box does not provide enough context for the LLM to elaborate and grasp the entire scope of the discussion. In these situations, we may need to turn to the second output of &lt;code&gt;SerperDevWebSearch&lt;/code&gt;: the links.&lt;/p&gt;
&lt;h1 id=&#34;fetching-urls&#34;&gt;
  Fetching URLs
  
&lt;/h1&gt;
&lt;p&gt;Haystack offers components to read the content of a URL: it&amp;rsquo;s &lt;code&gt;LinkContentFetcher&lt;/code&gt;. Let&amp;rsquo;s see this component in action.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.components.fetchers&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; LinkContentFetcher
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;fetcher &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; LinkContentFetcher()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;fetcher&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run(urls&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;https://en.wikipedia.org/wiki/Republic_of_Rose_Island&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns {&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     &amp;#34;streams&amp;#34;: [&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         ByteStream(data=b&amp;#34;&amp;lt;DOCTYPE html&amp;gt;\n&amp;lt;...&amp;#34;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     ]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# }&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;First, let&amp;rsquo;s notice that &lt;code&gt;LinkContentFetcher&lt;/code&gt; outputs a list of &lt;code&gt;ByteStream&lt;/code&gt; objects. &lt;code&gt;ByteStream&lt;/code&gt; is a Haystack abstraction that makes handling binary streams and files equally easy. When a component produces &lt;code&gt;ByteStream&lt;/code&gt; as output, you can directly pass these objects to a Converter component that can extract its textual content without saving such binary content to a file.&lt;/p&gt;
&lt;p&gt;These features come in handy to connect &lt;code&gt;LinkContentFetcher&lt;/code&gt; to a component we&amp;rsquo;ve already met before: &lt;code&gt;HTMLToDocument&lt;/code&gt;.&lt;/p&gt;
&lt;h1 id=&#34;processing-the-page&#34;&gt;
  Processing the page
  
&lt;/h1&gt;
&lt;p&gt;In a &lt;a href=&#34;https://www.zansara.dev/posts/2023-11-05-haystack-series-minimal-indexing&#34; &gt;previous post&lt;/a&gt;, we&amp;rsquo;ve seen how Haystack can convert web pages into clean Documents ready to be stored in a Document Store. We will reuse many of the components we have discussed there, so if you missed it, make sure to check it out.&lt;/p&gt;
&lt;p&gt;From the pipeline in question, we&amp;rsquo;re interested in three of its components: &lt;code&gt;HTMLToDocument&lt;/code&gt;, &lt;code&gt;DocumentCleaner&lt;/code&gt;, and &lt;code&gt;DocumentSplitter&lt;/code&gt;. Once the search component returns the links and &lt;code&gt;LinkContentFetcher&lt;/code&gt; downloaded their content, we can connect it to &lt;code&gt;HTMLToDocument&lt;/code&gt; to extract the text and &lt;code&gt;DocumentCleaner&lt;/code&gt; and &lt;code&gt;DocumentSplitter&lt;/code&gt; to clean and chunk the content, respectively. These documents then can go to the &lt;code&gt;PromptBuilder&lt;/code&gt;, resulting in a pipeline such as this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;template &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Question: {{ question }}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Context:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;% f&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;or document in documents %}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;    {{ document.content }}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;% e&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;ndfor %}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Please reformulate the information above to answer the user&amp;#39;s question.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;search&amp;#34;&lt;/span&gt;, SerperDevWebSearch(api_key&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;serperdev_api_key))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;fetcher&amp;#34;&lt;/span&gt;, LinkContentFetcher())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;converter&amp;#34;&lt;/span&gt;, HTMLToDocument())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;cleaner&amp;#34;&lt;/span&gt;, DocumentCleaner())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;splitter&amp;#34;&lt;/span&gt;, DocumentSplitter(split_by&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;sentence&amp;#34;&lt;/span&gt;, split_length&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;3&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;, PromptBuilder(template&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;template))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;, OpenAIGenerator(api_key&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;api_key))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;search.links&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;fetcher&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;fetcher&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;converter&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;converter&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;cleaner&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;cleaner&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;splitter&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;splitter&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder.documents&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;question &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;What&amp;#39;s the official language of the Republic of Rose Island?&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run({
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;search&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;query&amp;#34;&lt;/span&gt;: question},
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;question&amp;#34;&lt;/span&gt;: question}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;})
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-11-09-haystack-series-simple-web-rag/incorrect-web-rag-pipeline.png&#34; alt=&#34;Incorrect Web RAG Pipeline&#34;&gt;&lt;/p&gt;
&lt;p&gt;However, running this pipeline results in a crash.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;PipelineRuntimeError: llm raised &amp;#39;InvalidRequestError: This model&amp;#39;s maximum context 
length is 4097 tokens. However, your messages resulted in 4911 tokens. Please reduce 
the length of the messages.&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Reading the error message reveals the issue right away: the LLM received too much text. And that&amp;rsquo;s to be expected because we just passed the entire content of several web pages to it.&lt;/p&gt;
&lt;p&gt;We need to find a way to filter only the most relevant documents from the long list that is generated by &lt;code&gt;DocumentSplitter&lt;/code&gt;.&lt;/p&gt;
&lt;h1 id=&#34;ranking-documents-on-the-fly&#34;&gt;
  Ranking Documents on the fly
  
&lt;/h1&gt;
&lt;p&gt;Retrievers are optimized to use the efficient retrieval engines of document stores to sift quickly through vast collections of Documents. However, Haystack also provides smaller, standalone components that work very well on shorter lists and don&amp;rsquo;t require a full-blown vector database engine to function.&lt;/p&gt;
&lt;p&gt;These components are called rankers. One example of such a component is &lt;code&gt;TransformersSimilarityRanker&lt;/code&gt;: a ranker that uses a model from the &lt;code&gt;transformers&lt;/code&gt; library to rank Documents by their similarity to a given query.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see how it works:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.components.rankers&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; TransformersSimilarityRanker
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ranker &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; TransformersSimilarityRanker()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ranker&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;warm_up()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ranker&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    query&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;What&amp;#39;s the official language of the Republic of Rose Island?&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    documents&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;documents,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    top_k&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns {&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     &amp;#39;documents&amp;#39;: [&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         Document(content=&amp;#34;Island under construction\nRepublic of Rose Island\nThe Republic of Rose Island ( Esperanto : Respubliko de la Insulo de la Rozoj; Italian : Repubblica dell&amp;#39;Isola delle Rose) was a short-lived micronation on a man-made platform in the Adriatic Sea , 11 kilometres (6.8\xa0mi) off the coast of the province of Rimini , Italy, built by Italian engineer Giorgio Rosa, who made himself its president and declared it an independent state on 1 May 1968. [1] [2] Rose Island had its own government, currency, post office, and commercial establishments, and the official language was Esperanto .&amp;#34;, meta={&amp;#39;source_id&amp;#39;: &amp;#39;03bfe5f7b7a7ec623e854d2bc5eb36ba3cdf06e1e2771b3a529eeb7e669431b6&amp;#39;}, score=7.594357490539551)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     ]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# }&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This component has a feature we haven&amp;rsquo;t encountered before: the &lt;code&gt;warm_up()&lt;/code&gt; method.&lt;/p&gt;
&lt;p&gt;Components that need to initialize heavy resources, such as a language model, always perform this operation after initializing them in the &lt;code&gt;warm_up()&lt;/code&gt; method. When they are used in a Pipeline, &lt;code&gt;Pipeline.run()&lt;/code&gt; takes care of calling &lt;code&gt;warm_up()&lt;/code&gt; on all components before running; when used standalone, users need to call &lt;code&gt;warm_up()&lt;/code&gt; explicitly to prepare the object to run.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;TransformersSimilarityRanker&lt;/code&gt; accepts a few parameters. When initialized, it accepts a &lt;code&gt;model_name_or_path&lt;/code&gt; with the HuggingFace ID of the model to use for ranking: this value defaults to &lt;code&gt;cross-encoder/ms-marco-MiniLM-L-6-v2&lt;/code&gt;. It also takes &lt;code&gt;token&lt;/code&gt;, to allow users to download private models from the Models Hub, &lt;code&gt;device&lt;/code&gt;, to let them leverage PyTorch&amp;rsquo;s ability to select the hardware to run on, and &lt;code&gt;top_k&lt;/code&gt;, the maximum number of documents to return. &lt;code&gt;top_k&lt;/code&gt;, as we see above, can also be passed to &lt;code&gt;run()&lt;/code&gt;, and the latter overcomes the former if both are set. This value defaults to 10.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s also put this component in the pipeline: its place is between the splitter and the prompt builder.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;template &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Question: {{ question }}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Context:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;% f&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;or document in documents %}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;    {{ document.content }}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;% e&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;ndfor %}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Please reformulate the information above to answer the user&amp;#39;s question.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;search&amp;#34;&lt;/span&gt;, SerperDevWebSearch(api_key&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;serperdev_api_key))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;fetcher&amp;#34;&lt;/span&gt;, LinkContentFetcher())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;converter&amp;#34;&lt;/span&gt;, HTMLToDocument())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;cleaner&amp;#34;&lt;/span&gt;, DocumentCleaner())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;splitter&amp;#34;&lt;/span&gt;, DocumentSplitter(split_by&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;sentence&amp;#34;&lt;/span&gt;, split_length&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;3&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ranker&amp;#34;&lt;/span&gt;, TransformersSimilarityRanker())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;, PromptBuilder(template&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;template))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;, OpenAIGenerator(api_key&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;api_key))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;search.links&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;fetcher&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;fetcher&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;converter&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;converter&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;cleaner&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;cleaner&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;splitter&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;splitter&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ranker&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ranker&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder.documents&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;question &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;What&amp;#39;s the official language of the Republic of Rose Island?&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run({
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;search&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;query&amp;#34;&lt;/span&gt;: question},
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ranker&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;query&amp;#34;&lt;/span&gt;: question},
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;question&amp;#34;&lt;/span&gt;: question}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;})
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns {&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     &amp;#39;llm&amp;#39;: {&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         &amp;#39;replies&amp;#39;: [&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#             &amp;#39;The official language of the Republic of Rose Island was Esperanto.&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         ],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         &amp;#39;metadata&amp;#39;: [...]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     }&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# }&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-11-09-haystack-series-simple-web-rag/unfiltered-web-rag-pipeline.png&#34; alt=&#34;Unfiltered Web RAG Pipeline&#34;&gt;&lt;/p&gt;
&lt;p&gt;Note how the ranker needs to know the question to compare the documents, just like the search and prompt builder components do. So, we need to pass the value to the pipeline&amp;rsquo;s &lt;code&gt;run()&lt;/code&gt; call.&lt;/p&gt;
&lt;h1 id=&#34;filtering-file-types&#34;&gt;
  Filtering file types
  
&lt;/h1&gt;
&lt;p&gt;The pipeline we just built works great in most cases. However, it may occasionally fail if the search component happens to return some URL that does not point to a web page but, for example, directly to a video, a PDF, or a PPTX.&lt;/p&gt;
&lt;p&gt;Haystack does offer some facilities to deal with these file types, but we will see these converters in another post. For now, let&amp;rsquo;s only filter those links out to prevent &lt;code&gt;HTMLToDocument&lt;/code&gt; from crashing.&lt;/p&gt;
&lt;p&gt;This task could be approached with Haystack in several ways, but the simplest in this scenario is to use a component that would typically be used for a slightly different purpose. This component is called &lt;code&gt;FileTypeRouter&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;FileTypeRouter&lt;/code&gt; is designed to route different files to their appropriate converters by checking their mime type. It does so by inspecting the content or the extension of the files it receives in input and producing an output dictionary with a separate list for each identified type.&lt;/p&gt;
&lt;p&gt;However, we can also conveniently use this component as a filter. Let&amp;rsquo;s see how!&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.components.routers&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; FileTypeRouter
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;router &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; FileTypeRouter(mime_types&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;text/html&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;router&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run(sources&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Republic_of_Rose_Island.txt&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Republic_of_Rose_Island.html&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns defaultdict(list,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         {&amp;#39;unclassified&amp;#39;: [PosixPath(&amp;#39;Republic_of_Rose_Island.txt&amp;#39;)],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#          &amp;#39;text/html&amp;#39;: [PosixPath(&amp;#39;Republic_of_Rose_Island.html&amp;#39;)]})&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;FileTypeRouter&lt;/code&gt; must always be initialized with the list of mime types it is supposed to handle. Not only that, but this component can also deal with files that do not match any of the expected mime types by putting them all under the &lt;code&gt;unclassified&lt;/code&gt; category.&lt;/p&gt;
&lt;p&gt;By putting this component between &lt;code&gt;LinkContentFetcher&lt;/code&gt; and &lt;code&gt;HTMLToDocument&lt;/code&gt;, we can make it forward along the pipeline only the files that match the &lt;code&gt;text/html&lt;/code&gt; mime type and silently discard all others.&lt;/p&gt;
&lt;p&gt;Notice how, in the pipeline below, I explicitly connect the &lt;code&gt;text/html&lt;/code&gt; output only:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;template &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Question: {{ question }}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Google Search Answer Boxes:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;% f&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;or document in documents %}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;    {{ document.content }}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;% e&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;ndfor %}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Please reformulate the information above to answer the user&amp;#39;s question.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;search&amp;#34;&lt;/span&gt;, SerperDevWebSearch(api_key&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;serperdev_api_key))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;fetcher&amp;#34;&lt;/span&gt;, LinkContentFetcher())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;filter&amp;#34;&lt;/span&gt;, FileTypeRouter(mime_types&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;text/html&amp;#34;&lt;/span&gt;]))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;converter&amp;#34;&lt;/span&gt;, HTMLToDocument())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;cleaner&amp;#34;&lt;/span&gt;, DocumentCleaner())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;splitter&amp;#34;&lt;/span&gt;, DocumentSplitter(split_by&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;sentence&amp;#34;&lt;/span&gt;, split_length&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;3&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ranker&amp;#34;&lt;/span&gt;, TransformersSimilarityRanker())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;, PromptBuilder(template&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;template))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;, OpenAIGenerator(api_key&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;api_key))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;search.links&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;fetcher&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;fetcher&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;filter&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;filter.text/html&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;converter&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;converter&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;cleaner&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;cleaner&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;splitter&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;splitter&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ranker&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ranker&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder.documents&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;question &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;What&amp;#39;s the official language of the Republic of Rose Island?&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run({
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;search&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;query&amp;#34;&lt;/span&gt;: question},
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ranker&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;query&amp;#34;&lt;/span&gt;: question},
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;question&amp;#34;&lt;/span&gt;: question}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;})
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns {&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     &amp;#39;llm&amp;#39;: {&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         &amp;#39;replies&amp;#39;: [&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#             &amp;#39;The official language of the Republic of Rose Island was Esperanto.&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         ],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         &amp;#39;metadata&amp;#39;: [...]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     }&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# }&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-11-09-haystack-series-simple-web-rag/html-web-rag-pipeline.png&#34; alt=&#34;HTML-only Web RAG Pipeline&#34;&gt;&lt;/p&gt;
&lt;p&gt;With this last addition, we added quite a bit of robustness to our pipeline, making it less likely to fail.&lt;/p&gt;
&lt;h1 id=&#34;wrapping-up&#34;&gt;
  Wrapping up
  
&lt;/h1&gt;
&lt;p&gt;Web RAG is a use case that can be expanded to cover many use cases, resulting in very complex pipelines. Haystack helps make sense of their complexity by pipeline graphs and detailed error messages in case of mismatch connections. However, pipelines this large can become overwhelming, especially when more branches are added.&lt;/p&gt;
&lt;p&gt;In one of our next posts, we will see how to cover such use cases while keeping the resulting complexity as low as possible.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Previous: &lt;a href=&#34;https://www.zansara.dev/posts/2023-11-05-haystack-series-minimal-indexing&#34; &gt;Indexing data for RAG applications&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;See the entire series here: &lt;a href=&#34;https://www.zansara.dev/series/haystack-2.0-series/&#34; &gt;Haystack 2.0 series&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;em&gt;Cover image from &lt;a href=&#34;https://commons.wikimedia.org/wiki/File:Isola_delle_Rose_1968.jpg&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia&lt;/a&gt;&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Haystack 2.0: Indexing data for RAG applications</title>
      <link>https://www.zansara.dev/demos/2023-11-05-indexing-data-for-rag-applications/</link>
      <pubDate>Sun, 05 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/demos/2023-11-05-indexing-data-for-rag-applications/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Indexing data for RAG applications</title>
      <link>https://www.zansara.dev/posts/2023-11-05-haystack-series-minimal-indexing/</link>
      <pubDate>Sun, 05 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2023-11-05-haystack-series-minimal-indexing/</guid>
      <description>&lt;p&gt;&lt;em&gt;Last updated: 18/01/2024&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;In the &lt;a href=&#34;https://www.zansara.dev/posts/2023-10-27-haystack-series-rag&#34; &gt;previous post&lt;/a&gt; of the Haystack 2.0 series, we saw how to build RAG pipelines using a generator, a prompt builder, and a retriever with its document store. However, the content of our document store wasn&amp;rsquo;t extensive, and populating one with clean, properly formatted data is not an easy task. How can we approach this problem?&lt;/p&gt;
&lt;p&gt;In this post, I will show you how to use Haystack 2.0 to create large amounts of documents from a few web pages and write them a document store that you can then use for retrieval.&lt;/p&gt;
&lt;div class=&#34;notice info&#34;&gt;
  &lt;div class=&#34;notice-content&#34;&gt;💡 &lt;em&gt;Do you want to see the code in action? Check out the &lt;a href=&#34;https://colab.research.google.com/drive/155CtcumiK5w3wX6FWyM1dG3OqnhwnCqy?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colab notebook&lt;/a&gt; or the &lt;a href=&#34;https://gist.github.com/ZanSara/ba7efd241c61ccfd12ed48195e23bb34&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gist&lt;/a&gt;.&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;notice warning&#34;&gt;
  &lt;div class=&#34;notice-content&#34;&gt;&lt;i&gt;⚠️ &lt;strong&gt;Warning:&lt;/strong&gt;&lt;/i&gt; &lt;em&gt;This code was tested on &lt;code&gt;haystack-ai==2.0.0b5&lt;/code&gt;. Haystack 2.0 is still unstable, so later versions might introduce breaking changes without notice until Haystack 2.0 is officially released. The concepts and components, however, stay the same.&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;h1 id=&#34;the-task&#34;&gt;
  The task
  
&lt;/h1&gt;
&lt;p&gt;In Haystack&amp;rsquo;s terminology, the process of extracting information from a group of files and storing the data in a document store is called &amp;ldquo;indexing&amp;rdquo;. The process includes, at the very minimum, reading the content of a file, generating a Document object containing all its text, and then storing it in a document store.&lt;/p&gt;
&lt;p&gt;However, indexing pipelines often do more than this. They can process more than one file type, like .txt, .pdf, .docx, .html, audio, video, and images. Having many file types to convert, they route each file to the proper converter based on its type. Files tend to contain way more text than a normal LLM can chew, so they need to split those huge Documents into smaller chunks. Also, the converters are not perfect at reading text from the files, so they need to clean the data from artifacts such as page numbers, headers, footers, and so on. On top of all of this, if you plan to use a retriever that is based on embedding similarity, your indexing pipeline will also need to embed all documents before writing them into the store.&lt;/p&gt;
&lt;p&gt;Sounds like a lot of work!&lt;/p&gt;
&lt;p&gt;In this post, we will focus on the preprocessing part of the pipeline: cleaning, splitting, and writing documents. I will talk about the other functionalities of indexing pipelines, such as document embedding and multiple file types routing, in later posts.&lt;/p&gt;
&lt;h1 id=&#34;converting-files&#34;&gt;
  Converting files
  
&lt;/h1&gt;
&lt;p&gt;As we&amp;rsquo;ve just seen, the most important task of this pipeline is to convert files into Documents. Haystack provides several converters for this task: at the time of writing, it supports:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Raw text files (&lt;code&gt;TextFileToDocument&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;HTML files, so web pages in general (&lt;code&gt;HTMLToDocument&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;PDF files, by extracting text natively (&lt;code&gt;PyPDFToDocument&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Image files, PDFs with images, and Office files with images, by OCR (&lt;code&gt;AzureOCRDocumentConverter&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Audio files, doing transcription with Whisper either locally (&lt;code&gt;LocalWhisperTranscriber&lt;/code&gt;) or remotely using OpenAI&amp;rsquo;s hosted models (&lt;code&gt;RemoteWhisperTranscriber&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;A ton of &lt;a href=&#34;https://tika.apache.org/2.9.1/formats.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;other formats&lt;/a&gt;, such as Microsoft&amp;rsquo;s Office formats, thanks to &lt;a href=&#34;https://tika.apache.org/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Apache Tika&lt;/a&gt; (&lt;code&gt;TikaDocumentConverter&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For this example, let&amp;rsquo;s assume we have a collection of web pages downloaded from the Internet. These pages are our only source of information and contain all we want our RAG application to know about.&lt;/p&gt;
&lt;p&gt;In this case, our converter of choice is &lt;code&gt;HTMLToDocument&lt;/code&gt;. &lt;code&gt;HTMLToDocument&lt;/code&gt; is a Haystack component that understands HTML and can filter all the markup away, leaving only meaningful text. Remember that this is a file converter, not a URL fetcher: it can only process local files, such as a website crawl. Haystack provides some components to fetch web pages, but we will see them later.&lt;/p&gt;
&lt;p&gt;Here is how you can use this converter:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.components.converters&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; HTMLToDocument
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;path &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Republic_of_Rose_Island.html&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;converter &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; HTMLToDocument()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;converter&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run(sources&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[path])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns {&amp;#34;documents&amp;#34;: [Document(content=&amp;#34;The Republic of Rose Isla...&amp;#34;)]}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;HTMLToDocument&lt;/code&gt; is a straightforward component that offers close to no parameters to customize its behavior. Of its API, one notable feature is its input type: this converter can take paths to local files in the form of strings or &lt;code&gt;Path&lt;/code&gt; objects, but it also accepts &lt;code&gt;ByteStream&lt;/code&gt; objects.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ByteStream&lt;/code&gt; is a handy Haystack abstraction that makes handling binary streams easier. If a component accepts &lt;code&gt;ByteStream&lt;/code&gt; as input, you don&amp;rsquo;t necessarily have to save your web pages to file before passing them to this converter. This allows components that retrieve large files from the Internet to pipe their output directly into this component without saving the data to disk first, which can save a lot of time.&lt;/p&gt;
&lt;h1 id=&#34;cleaning-the-text&#34;&gt;
  Cleaning the text
  
&lt;/h1&gt;
&lt;p&gt;With &lt;code&gt;HTMLToDocument&lt;/code&gt;, we can convert whole web pages into large Document objects. The converter typically does a decent job of filtering out the markup. Still, it&amp;rsquo;s not always perfect. To compensate for these occasional issues, Haystack offers a component called &lt;code&gt;DocumentCleaner&lt;/code&gt; that can remove noise from the text of the documents.&lt;/p&gt;
&lt;p&gt;Just like any other component, &lt;code&gt;DocumentCleaner&lt;/code&gt; is straightforward to use:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.components.preprocessors&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; DocumentCleaner
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cleaner &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; DocumentCleaner()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cleaner&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run(documents&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;documents)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns {&amp;#34;documents&amp;#34;: [Document(content=...), Document(content=...), ...]}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The effectiveness of &lt;code&gt;DocumentCleaner&lt;/code&gt; depends a lot on the type of converter you use. Some flags, such as &lt;code&gt;remove_empty_lines&lt;/code&gt; and &lt;code&gt;remove_extra_whitespace&lt;/code&gt;, are minor fixes that can come in handy but usually have little impact on the quality of the results when used in a RAG pipeline. They can, however, make a vast difference for Extractive QA pipelines.&lt;/p&gt;
&lt;p&gt;Other parameters, like &lt;code&gt;remove_substrings&lt;/code&gt; or &lt;code&gt;remove_regex&lt;/code&gt;, work very well but need manual inspection and iteration from a human to get right. For example, for Wikipedia pages, we could use these parameters to remove all instances of the word &lt;code&gt;&amp;quot;Wikipedia&amp;quot;&lt;/code&gt;, which are undoubtedly many and irrelevant.&lt;/p&gt;
&lt;p&gt;Finally, &lt;code&gt;remove_repeated_substrings&lt;/code&gt; is a convenient method that removes headers and footers from long text, for example, books and articles. However, it works only for PDFs and, to a limited degree, for text files because it relies on the presence of form feed characters (&lt;code&gt;\f&lt;/code&gt;), which are rarely present in web pages.&lt;/p&gt;
&lt;h1 id=&#34;splitting-the-text&#34;&gt;
  Splitting the text
  
&lt;/h1&gt;
&lt;p&gt;Now that the text is cleaned up, we can move onto a more exciting step: text splitting.&lt;/p&gt;
&lt;p&gt;So far, each Document stored the content of an entire file. If a file was a whole book with hundreds of pages, a single Document would contain hundreds of thousands of words, which is clearly too much for an LLM to make sense of. Such a large Document is also challenging for Retrievers to understand because it contains so much text that it looks relevant to every possible question. To populate our document store with data that can be used effectively by a RAG pipeline, we need to chunk this data into much smaller Documents.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s where &lt;code&gt;DocumentSplitter&lt;/code&gt; comes into play.&lt;/p&gt;
&lt;div class=&#34;notice info&#34;&gt;
  &lt;div class=&#34;notice-content&#34;&gt;&lt;p&gt;💡 &lt;em&gt;With LLMs in a race to offer the &lt;a href=&#34;https://magic.dev/blog/ltm-1&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;largest context window&lt;/a&gt; and research showing that such a chase is &lt;a href=&#34;https://arxiv.org/abs/2307.03172&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;counterproductive&lt;/a&gt;, there is no general consensus about how splitting Documents for RAG impacts the LLM&amp;rsquo;s performance.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;What you need to keep in mind is that splitting implies a tradeoff. Huge documents will always be slightly relevant for every question, but they will bring a lot of context, which may or may not confuse the model. On the other hand, tiny Documents are much more likely to be retrieved only for questions they&amp;rsquo;re highly relevant for, but they might provide too little context for the LLM to really understand their meaning.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Tweaking the size of your Documents for the specific LLM you&amp;rsquo;re using and the topic of your documents is one way to optimize your RAG pipeline, so be ready to experiment with different Document sizes before committing to one.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;How is it used?&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.components.preprocessors.text_document_splitter&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; DocumentSplitter
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;text_splitter &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; DocumentSplitter(split_by&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;sentence&amp;#34;&lt;/span&gt;, split_length&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;5&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;text_splitter&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run(documents&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;documents)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns {&amp;#34;documents&amp;#34;: [Document(content=...), Document(content=...), ...]}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;DocumentSplitter&lt;/code&gt; lets you configure the approximate size of the chunks you want to generate with three parameters: &lt;code&gt;split_by&lt;/code&gt;, &lt;code&gt;split_length&lt;/code&gt;, and &lt;code&gt;split_overlap&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;split_by&lt;/code&gt; defines the unit to use when splitting some text. For now, the options are &lt;code&gt;word&lt;/code&gt;, &lt;code&gt;sentence&lt;/code&gt;, and &lt;code&gt;passage&lt;/code&gt; (paragraph), but we will soon add other options.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;split_length&lt;/code&gt; is the number of the units defined above each document should include. For example, if the unit is &lt;code&gt;sentence&lt;/code&gt;, &lt;code&gt;split_length=10&lt;/code&gt; means that all your Documents will contain 10 sentences worth of text (except usually for the last document, which may have less). If the unit was &lt;code&gt;word&lt;/code&gt;, it would instead contain 10 words.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;split_overlap&lt;/code&gt; is the amount of units that should be included from the previous Document. For example, if the unit is &lt;code&gt;sentence&lt;/code&gt; and the length is &lt;code&gt;10&lt;/code&gt;, setting &lt;code&gt;split_overlap=2&lt;/code&gt; means that the last two sentences of the first document will also be present at the start of the second, which will include only 8 new sentences for a total of 10. Such repetition carries over to the end of the text to split.&lt;/p&gt;
&lt;h1 id=&#34;writing-to-the-store&#34;&gt;
  Writing to the store
  
&lt;/h1&gt;
&lt;p&gt;Once all of this is done, we can finally move on to the last step of our journey: writing the Documents into our document store. We first create the document store:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.document_stores.in_memory&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; InMemoryDocumentStore
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;document_store &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; InMemoryDocumentStore()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;and then use &lt;code&gt;DocumentWriter&lt;/code&gt; to actually write the documents in:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.components.writers&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; DocumentWriter
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;writer &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; DocumentWriter(document_store&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;document_store)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;writer&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run(documents&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;documents_with_embeddings)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns {&amp;#34;documents_written&amp;#34;: 120}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you&amp;rsquo;ve read my &lt;a href=&#34;https://www.zansara.dev/posts/2023-10-27-haystack-series-rag&#34; &gt;previous post&lt;/a&gt; about RAG pipelines, you may wonder: why use &lt;code&gt;DocumentWriter&lt;/code&gt; when we could call the &lt;code&gt;.write_documents()&lt;/code&gt; method of our document store?&lt;/p&gt;
&lt;p&gt;In fact, the two methods are fully equivalent: &lt;code&gt;DocumentWriter&lt;/code&gt; does nothing more than calling the &lt;code&gt;.write_documents()&lt;/code&gt; method of the document store. The difference is that &lt;code&gt;DocumentWriter&lt;/code&gt; is the way to go if you are using a Pipeline, which is what we&amp;rsquo;re going to do next.&lt;/p&gt;
&lt;h1 id=&#34;putting-it-all-together&#34;&gt;
  Putting it all together
  
&lt;/h1&gt;
&lt;p&gt;We finally have all the components we need to go from a list of web pages to a document store populated with clean and short Document objects. Let&amp;rsquo;s build a Pipeline to sum up this process:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; Pipeline
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;document_store &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; InMemoryDocumentStore()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;converter&amp;#34;&lt;/span&gt;, HTMLToDocument())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;cleaner&amp;#34;&lt;/span&gt;, DocumentCleaner())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;splitter&amp;#34;&lt;/span&gt;, DocumentSplitter(split_by&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;sentence&amp;#34;&lt;/span&gt;, split_length&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;5&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;writer&amp;#34;&lt;/span&gt;, DocumentWriter(document_store&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;document_store))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;converter&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;cleaner&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;cleaner&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;splitter&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;splitter&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;writer&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;draw(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;simple-indexing-pipeline.png&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run({&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;converter&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;sources&amp;#34;&lt;/span&gt;: file_names}})
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-11-05-haystack-series-minimal-indexing/simple-indexing-pipeline.png&#34; alt=&#34;Indexing Pipeline&#34;&gt;&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s it! We now have a fully functional indexing pipeline that can take a list of web pages and convert them into Documents that our RAG pipeline can use. As long as the RAG pipeline reads from the same store we are writing the Documents to, we can add as many Documents as we need to keep the chatbot&amp;rsquo;s answers up to date without having to touch the RAG pipeline.&lt;/p&gt;
&lt;p&gt;To try it out, we only need to take the RAG pipeline we built in &lt;a href=&#34;https://www.zansara.dev/posts/2023-10-27-haystack-series-rag&#34; &gt;my previous post&lt;/a&gt; and connect it to the same document store we just populated:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.components.generators&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; OpenAIGenerator
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.components.builders.prompt_builder&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; PromptBuilder
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.components.retrievers.in_memory&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; InMemoryBM25Retriever
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;template &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Given the following information, answer the question: {{ question }}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;% f&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;or document in documents %}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;    {{ document.content }}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;% e&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;ndfor %}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;retriever&amp;#34;&lt;/span&gt;, InMemoryBM25Retriever(document_store&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;document_store))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;, PromptBuilder(template&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;template))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;, OpenAIGenerator(api_key&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;api_key))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;retriever&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder.documents&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;question &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Is there any documentary about the story of Rose Island? Can you tell me something about that?&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run({
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;retriever&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;query&amp;#34;&lt;/span&gt;: question},
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;question&amp;#34;&lt;/span&gt;: question}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;})
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns {&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     &amp;#39;llm&amp;#39;: {&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         &amp;#39;replies&amp;#39;: [&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#             &amp;#39;Yes, there is a documentary about the story of Rose Island. It is &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#             called &amp;#34;Rose Island&amp;#34; and was released on Netflix on 8 December 2020. &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#             The documentary follows the true story of Giorgio Rosa, an Italian &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#             engineer who built his own island in the Adriatic sea in the late &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#             1960s. The island housed a restaurant, bar, souvenir shop, and even &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#             a post office. Rosa\&amp;#39;s goal was to have his self-made structure &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#             recognized as an independent state, leading to a battle with the &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#             Italian authorities. The film depicts the construction of the island &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#             and Rosa\&amp;#39;s refusal to dismantle it despite government demands. The &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#             story of Rose Island was relatively unknown until the release of the &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#             documentary. The film showcases the technology Rosa invented to build &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#             the island and explores themes of freedom and resilience.&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         ],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         &amp;#39;metadata&amp;#39;: [...]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     }&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# }&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And suddenly, our chatbot knows everything about Rose Island without us having to feed the data to the document store by hand.&lt;/p&gt;
&lt;h1 id=&#34;wrapping-up&#34;&gt;
  Wrapping up
  
&lt;/h1&gt;
&lt;p&gt;Indexing pipelines can be powerful tools, even in their simplest form, like the one we just built. However, it doesn&amp;rsquo;t end here: Haystack offers many more facilities to extend what&amp;rsquo;s possible with indexing pipelines, like doing web searches, downloading files from the web, processing many other file types, and so on.&lt;/p&gt;
&lt;p&gt;We will see how soon, so stay tuned!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Next: &lt;a href=&#34;https://www.zansara.dev/posts/2023-11-09-haystack-series-simple-web-rag&#34; &gt;The World of Web RAG&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Previous: &lt;a href=&#34;https://www.zansara.dev/posts/2023-10-27-haystack-series-rag&#34; &gt;RAG Pipelines from scratch&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;See the entire series here: &lt;a href=&#34;https://www.zansara.dev/series/haystack-2.0-series/&#34; &gt;Haystack 2.0 series&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;em&gt;Cover image from &lt;a href=&#34;https://bertolamifineart.bidinside.com/en/lot/126352/1968-insula-de-la-rozoj-o-isola-delle-/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this website.&lt;/a&gt;&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Haystack 2.0: RAG Pipelines from scratch</title>
      <link>https://www.zansara.dev/demos/2023-10-27-rag-pipelines-from-scratch/</link>
      <pubDate>Fri, 27 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/demos/2023-10-27-rag-pipelines-from-scratch/</guid>
      <description></description>
    </item>
    
    <item>
      <title>RAG Pipelines from scratch</title>
      <link>https://www.zansara.dev/posts/2023-10-27-haystack-series-rag/</link>
      <pubDate>Fri, 27 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2023-10-27-haystack-series-rag/</guid>
      <description>&lt;p&gt;&lt;em&gt;Last updated: 18/01/2024 - Read it on the &lt;a href=&#34;https://haystack.deepset.ai/blog/rag-pipelines-from-scratch&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haystack Blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Retrieval Augmented Generation (RAG) is quickly becoming an essential technique to make LLMs more reliable and effective at answering any question, regardless of how specific. To stay relevant in today&amp;rsquo;s NLP landscape, Haystack must enable it.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see how to build such applications with Haystack 2.0, from a direct call to an LLM to a fully-fledged, production-ready RAG pipeline that scales. At the end of this post, we will have an application that can answer questions about world countries based on data stored in a private database. At that point, the knowledge of the LLM will be only limited by the content of our data store, and all of this can be accomplished without fine-tuning language models.&lt;/p&gt;
&lt;div class=&#34;notice info&#34;&gt;
  &lt;div class=&#34;notice-content&#34;&gt;💡 &lt;em&gt;I recently gave a talk about RAG applications in Haystack 2.0, so if you prefer videos to blog posts, you can find the recording &lt;a href=&#34;https://zansara.dev/talks/2023-10-12-office-hours-rag-pipelines/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. Keep in mind that the code shown might be outdated.&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;what-is-rag&#34;&gt;
  What is RAG?
  
&lt;/h2&gt;
&lt;p&gt;The idea of Retrieval Augmented Generation was first defined in a &lt;a href=&#34;https://arxiv.org/abs/2005.11401&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; by Meta in 2020. It was designed to solve a few of the inherent limitations of seq2seq models (language models that, given a sentence, can finish writing it for you), such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Their internal knowledge, as vast as it may be, will always be limited and at least slightly out of date.&lt;/li&gt;
&lt;li&gt;They work best on generic topics rather than niche and specific areas unless they&amp;rsquo;re fine-tuned on purpose, which is a costly and slow process.&lt;/li&gt;
&lt;li&gt;All models, even those with subject-matter expertise, tend to &amp;ldquo;hallucinate&amp;rdquo;: they confidently produce false statements backed by apparently solid reasoning.&lt;/li&gt;
&lt;li&gt;They cannot reliably cite their sources or tell where their knowledge comes from, which makes fact-checking their replies nontrivial.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;RAG solves these issues of &amp;ldquo;grounding&amp;rdquo; the LLM to reality by providing some relevant, up-to-date, and trusted information to the model together with the question. In this way, the LLM doesn&amp;rsquo;t need to draw information from its internal knowledge, but it can base its replies on the snippets provided by the user.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-27-haystack-series-rag/rag-paper-image.png&#34; alt=&#34;RAG Paper diagram&#34; title=&#34;A visual representation of RAG from the original paper&#34;&gt;&lt;/p&gt;
&lt;p&gt;As you can see in the image above (taken directly from the original paper), a system such as RAG is made of two parts: one that finds text snippets that are relevant to the question asked by the user and a generative model, usually an LLM, that rephrases the snippets into a coherent answer for the question.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s build one of these with Haystack 2.0!&lt;/p&gt;
&lt;div class=&#34;notice info&#34;&gt;
  &lt;div class=&#34;notice-content&#34;&gt;💡 &lt;em&gt;Do you want to see this code in action? Check out the Colab notebook &lt;a href=&#34;https://colab.research.google.com/drive/1FkDNS3hTO4oPXHFbXQcldls0kf-KTq-r?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; or the gist &lt;a href=&#34;https://gist.github.com/ZanSara/0af1c2ac6c71d0a723c179cc6ec1ac41&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/em&gt;.&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&#34;notice warning&#34;&gt;
  &lt;div class=&#34;notice-content&#34;&gt;⚠️ &lt;strong&gt;Warning:&lt;/strong&gt; &lt;em&gt;This code was tested on &lt;code&gt;haystack-ai==2.0.0b5&lt;/code&gt;. Haystack 2.0 is still unstable, so later versions might introduce breaking changes without notice until Haystack 2.0 is officially released. The concepts and components however stay the same.&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;generators-haystacks-llm-components&#34;&gt;
  Generators: Haystack&amp;rsquo;s LLM components
  
&lt;/h2&gt;
&lt;p&gt;As every NLP framework that deserves its name, Haystack supports LLMs in different ways. The easiest way to query an LLM in Haystack 2.0 is through a Generator component: depending on which LLM and how you intend to query it (chat, text completion, etc&amp;hellip;), you should pick the appropriate class.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re going to use &lt;code&gt;gpt-3.5-turbo&lt;/code&gt; (the model behind ChatGPT) for these examples, so the component we need is &lt;a href=&#34;https://docs.haystack.deepset.ai/v2.0/docs/openaigenerator&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;OpenAIGenerator&lt;/code&gt;&lt;/a&gt;. Here is all the code required to use it to query OpenAI&amp;rsquo;s &lt;code&gt;gpt-3.5-turbo&lt;/code&gt; :&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.components.generators&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; OpenAIGenerator
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;generator &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; OpenAIGenerator(api_key&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;api_key)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;generator&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run(prompt&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;What&amp;#39;s the official language of France?&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns {&amp;#34;replies&amp;#34;: [&amp;#39;The official language of France is French.&amp;#39;]}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can select your favorite OpenAI model by specifying a &lt;code&gt;model_name&lt;/code&gt; at initialization, for example, &lt;code&gt;gpt-4&lt;/code&gt;. It also supports setting an &lt;code&gt;api_base_url&lt;/code&gt; for private deployments, a &lt;code&gt;streaming_callback&lt;/code&gt; if you want to see the output generated live in the terminal, and optional &lt;code&gt;kwargs&lt;/code&gt; to let you pass whatever other parameter the model understands, such as the number of answers (&lt;code&gt;n&lt;/code&gt;), the temperature (&lt;code&gt;temperature&lt;/code&gt;), etc.&lt;/p&gt;
&lt;p&gt;Note that in this case, we&amp;rsquo;re passing the API key to the component&amp;rsquo;s constructor. This is unnecessary: &lt;code&gt;OpenAIGenerator&lt;/code&gt; can read the value from the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable and also from the &lt;code&gt;api_key&lt;/code&gt; module variable of &lt;a href=&#34;https://github.com/openai/openai-python#usage&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;openai&lt;/code&gt;&amp;rsquo;s SDK&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Right now, Haystack supports HuggingFace models through the &lt;a href=&#34;https://docs.haystack.deepset.ai/v2.0/docs/huggingfacelocalgenerator&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;HuggingFaceLocalGenerator&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://docs.haystack.deepset.ai/v2.0/docs/huggingfacetgigenerator&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;HuggingFaceTGIGenerator&lt;/code&gt;&lt;/a&gt; components, and many more LLMs are coming soon.&lt;/p&gt;
&lt;h2 id=&#34;promptbuilder-structured-prompts-from-templates&#34;&gt;
  PromptBuilder: structured prompts from templates
  
&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s imagine that our LLM-powered application also comes with some pre-defined questions that the user can select instead of typing in full. For example, instead of asking them to type &lt;code&gt;What&#39;s the official language of France?&lt;/code&gt;, we let them select &lt;code&gt;Tell me the official languages&lt;/code&gt; from a list, and they simply need to type &amp;ldquo;France&amp;rdquo; (or &amp;ldquo;Wakanda&amp;rdquo; for a change - our chatbot needs some challenges too).&lt;/p&gt;
&lt;p&gt;In this scenario, we have two pieces of the prompt: a variable (the country name, like &amp;ldquo;France&amp;rdquo;) and a prompt template, which in this case is &lt;code&gt;&amp;quot;What&#39;s the official language of {{ country }}?&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Haystack offers a component that can render variables into prompt templates: it&amp;rsquo;s called &lt;a href=&#34;https://docs.haystack.deepset.ai/v2.0/docs/promptbuilder&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;PromptBuilder&lt;/code&gt;&lt;/a&gt;. As the generators we&amp;rsquo;ve seen before, also &lt;code&gt;PromptBuilder&lt;/code&gt; is nearly trivial to initialize and use.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.components.builders&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; PromptBuilder
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;prompt_builder &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; PromptBuilder(template&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;What&amp;#39;s the official language of {{ country }}?&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;prompt_builder&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run(country&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;France&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns {&amp;#39;prompt&amp;#39;: &amp;#34;What&amp;#39;s the official language of France?&amp;#34;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note how we defined a variable, &lt;code&gt;country&lt;/code&gt;, by wrapping its name in double curly brackets. PromptBuilder lets you define any input variable that way: if the prompt template was &lt;code&gt;&amp;quot;What&#39;s the official language of {{ nation }}?&amp;quot;&lt;/code&gt;, the &lt;code&gt;run()&lt;/code&gt; method of &lt;code&gt;PromptBuilder&lt;/code&gt; would have expected a &lt;code&gt;nation&lt;/code&gt; input.&lt;/p&gt;
&lt;p&gt;This syntax comes from &lt;a href=&#34;https://jinja.palletsprojects.com/en/3.0.x/intro/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jinja2&lt;/a&gt;, a popular templating library for Python. If you have ever used Flask, Django, or Ansible, you will feel at home with &lt;code&gt;PromptBuilder&lt;/code&gt;. Instead, if you never heard of any of these libraries, you can check out the &lt;a href=&#34;https://jinja.palletsprojects.com/en/3.0.x/templates/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;syntax&lt;/a&gt; on Jinja&amp;rsquo;s documentation. Jinja has a powerful templating language and offers way more features than you&amp;rsquo;ll ever need in prompt templates, ranging from simple if statements and for loops to object access through dot notation, nesting of templates, variables manipulation, macros, full-fledged import and encapsulation of templates, and more.&lt;/p&gt;
&lt;h2 id=&#34;a-simple-generative-pipeline&#34;&gt;
  A Simple Generative Pipeline
  
&lt;/h2&gt;
&lt;p&gt;With these two components, we can assemble a minimal pipeline to see how they work together. Connecting them is trivial: &lt;code&gt;PromptBuilder&lt;/code&gt; generates a &lt;code&gt;prompt&lt;/code&gt; output, and &lt;code&gt;OpenAIGenerator&lt;/code&gt; expects an input with the same name and type.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; Pipeline
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.components.generators&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; OpenAIGenerator
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.components.builders&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; PromptBuilder
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;, PromptBuilder(template&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;What&amp;#39;s the official language of {{ country }}?&amp;#34;&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;, OpenAIGenerator(api_key&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;api_key))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run({&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;country&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;France&amp;#34;&lt;/span&gt;}})
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns {&amp;#34;llm&amp;#34;: {&amp;#34;replies&amp;#34;: [&amp;#39;The official language of France is French.&amp;#39;] }}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here is the pipeline graph:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-27-haystack-series-rag/simple-llm-pipeline.png&#34; alt=&#34;Simple LLM pipeline&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;make-the-llm-cheat&#34;&gt;
  Make the LLM cheat
  
&lt;/h2&gt;
&lt;p&gt;Building the Generative part of a RAG application was very simple! So far, we only provided the question to the LLM, but no information to base its answers on. Nowadays, LLMs possess a lot of general knowledge, so questions about famous countries such as France or Germany are easy for them to reply to correctly. However, when using an app about world countries, some users may be interested in knowing more about obscure or defunct microstates that don&amp;rsquo;t exist anymore. In this case, ChatGPT is unlikely to provide the correct answer without any help.&lt;/p&gt;
&lt;p&gt;For example, let&amp;rsquo;s ask our pipeline something &lt;em&gt;really&lt;/em&gt; obscure.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run({&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;country&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;the Republic of Rose Island&amp;#34;&lt;/span&gt;}})
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns {&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     &amp;#34;llm&amp;#34;: {&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         &amp;#34;replies&amp;#34;: [&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#             &amp;#39;The official language of the Republic of Rose Island was Italian.&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         ]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     }&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# }&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The answer is an educated guess but is not accurate: although it was located just outside of Italy&amp;rsquo;s territorial waters, according to &lt;a href=&#34;https://en.wikipedia.org/wiki/Republic_of_Rose_Island&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia&lt;/a&gt; the official language of this short-lived micronation was Esperanto.&lt;/p&gt;
&lt;p&gt;How can we get ChatGPT to reply to such a question correctly? One way is to make it &amp;ldquo;cheat&amp;rdquo; by providing the answer as part of the question. In fact, &lt;code&gt;PromptBuilder&lt;/code&gt; is designed to serve precisely this use case.&lt;/p&gt;
&lt;p&gt;Here is our new, more advanced prompt:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Given the following information, answer the question.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Context: {{ context }}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Question: {{ question }}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&amp;rsquo;s build a new pipeline using this prompt!&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;context_template &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Given the following information, answer the question.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Context: {{ context }}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Question: {{ question }}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;language_template &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;What&amp;#39;s the official language of {{ country }}?&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;context_prompt&amp;#34;&lt;/span&gt;, PromptBuilder(template&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;context_template))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;language_prompt&amp;#34;&lt;/span&gt;, PromptBuilder(template&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;language_template))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;, OpenAIGenerator(api_key&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;api_key))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;language_prompt&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;context_prompt.question&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;context_prompt&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run({
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;context_prompt&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;context&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Rose Island had its own government, currency, post office, and commercial establishments, and the official language was Esperanto.&amp;#34;&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;language_prompt&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;country&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;the Republic of Rose Island&amp;#34;&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;})
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns {&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     &amp;#34;llm&amp;#34;: {&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         &amp;#34;replies&amp;#34;: [&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#             &amp;#39;The official language of the Republic of Rose Island is Esperanto.&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         ]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     }&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# }&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&amp;rsquo;s look at the graph of our Pipeline:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-27-haystack-series-rag/double-promptbuilder-pipeline.png&#34; alt=&#34;Double PromptBuilder pipeline&#34;&gt;&lt;/p&gt;
&lt;p&gt;The beauty of &lt;code&gt;PromptBuilder&lt;/code&gt; lies in its flexibility. It allows users to chain instances together to assemble complex prompts from simpler schemas: for example, we used the output of the first &lt;code&gt;PromptBuilder&lt;/code&gt; as the value of &lt;code&gt;question&lt;/code&gt; in the second prompt.&lt;/p&gt;
&lt;p&gt;However, in this specific scenario, we can build a simpler system by merging the two prompts into one.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Given the following information, answer the question.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Context: {{ context }}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Question: What&amp;#39;s the official language of {{ country }}?
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Using this new prompt, the resulting pipeline becomes again very similar to our first.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;template &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Given the following information, answer the question.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Context: {{ context }}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Question: What&amp;#39;s the official language of {{ country }}?
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;, PromptBuilder(template&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;template))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;, OpenAIGenerator(api_key&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;api_key))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run({
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;: {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;context&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Rose Island had its own government, currency, post office, and commercial establishments, and the official language was Esperanto.&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;country&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;the Republic of Rose Island&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;})
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns {&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     &amp;#34;llm&amp;#34;: {&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         &amp;#34;replies&amp;#34;: [&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#             &amp;#39;The official language of the Republic of Rose Island is Esperanto.&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         ]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     }&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# }&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-27-haystack-series-rag/double-variable-promptbuilder-pipeline.png&#34; alt=&#34;PromptBuilder with two inputs pipeline&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;retrieving-the-context&#34;&gt;
  Retrieving the context
  
&lt;/h2&gt;
&lt;p&gt;For now, we&amp;rsquo;ve been playing with prompts, but the fundamental question remains unanswered: where do we get the correct text snippet for the question the user is asking? We can&amp;rsquo;t expect such information as part of the input: we need our system to be able to fetch this information independently, based uniquely on the query.&lt;/p&gt;
&lt;p&gt;Thankfully, retrieving relevant information from large &lt;a href=&#34;https://en.wikipedia.org/wiki/Text_corpus&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;corpora&lt;/a&gt; (a technical term for extensive collections of data, usually text) is a task that Haystack excels at since its inception: the components that perform this task are called &lt;a href=&#34;https://docs.haystack.deepset.ai/v2.0/docs/retrievers&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Retrievers&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Retrieval can be performed on different data sources: to begin, let&amp;rsquo;s assume we&amp;rsquo;re searching for data in a local database, which is the use case that most Retrievers are geared towards.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s create a small local database to store information about some European countries. Haystack offers a neat object for these small-scale demos: &lt;code&gt;InMemoryDocumentStore&lt;/code&gt;. This document store is little more than a Python dictionary under the hood but provides the same exact API as much more powerful data stores and vector stores, such as &lt;a href=&#34;https://github.com/deepset-ai/haystack-core-integrations/tree/main/document_stores/elasticsearch&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Elasticsearch&lt;/a&gt; or &lt;a href=&#34;https://haystack.deepset.ai/integrations/chroma-documentstore&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ChromaDB&lt;/a&gt;. Keep in mind that the object is called &amp;ldquo;Document Store&amp;rdquo; and not simply &amp;ldquo;datastore&amp;rdquo; because what it stores is Haystack&amp;rsquo;s Document objects: a small dataclass that helps other components make sense of the data that they receive.&lt;/p&gt;
&lt;p&gt;So, let&amp;rsquo;s initialize an &lt;code&gt;InMemoryDocumentStore&lt;/code&gt; and write some &lt;code&gt;Documents&lt;/code&gt; into it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.dataclasses&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; Document
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.document_stores.in_memory&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; InMemoryDocumentStore
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;documents &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; [
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Document(content&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;German is the the official language of Germany.&amp;#34;&lt;/span&gt;), 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Document(content&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;The capital of France is Paris, and its official language is French.&amp;#34;&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Document(content&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Italy recognizes a few official languages, but the most widespread one is Italian.&amp;#34;&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Document(content&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Esperanto has been adopted as official language for some microstates as well, such as the Republic of Rose Island, a short-lived microstate built on a sea platform in the Adriatic Sea.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docstore &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; InMemoryDocumentStore()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docstore&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;write_documents(documents&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;documents)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docstore&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;filter_documents()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns [&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     Document(content=&amp;#34;German is the the official language of Germany.&amp;#34;), &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     Document(content=&amp;#34;The capital of France is Paris, and its official language is French.&amp;#34;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     Document(content=&amp;#34;Esperanto has been adopted as official language for some microstates as well, such as the Republic of Rose Island, a short-lived microstate built on a sea platform in the Adriatic Sea.&amp;#34;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     Document(content=&amp;#34;Italy recognizes a few official languages, but the most widespread one is Italian.&amp;#34;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# ]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once the document store is set up, we can initialize a retriever. In Haystack 2.0, each document store comes with its own set of highly optimized retrievers: &lt;code&gt;InMemoryDocumentStore&lt;/code&gt; offers two, one based on BM25 ranking and one based on embedding similarity.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start with the BM25-based retriever, which is slightly easier to set up. Let&amp;rsquo;s first use it in isolation to see how it behaves.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.components.retrievers.in_memory&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; InMemoryBM25Retriever
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;retriever &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; InMemoryBM25Retriever(document_store&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;docstore)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;retriever&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run(query&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Rose Island&amp;#34;&lt;/span&gt;, top_k&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns [&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     Document(content=&amp;#34;Esperanto has been adopted as official language for some microstates as well, such as the Republic of Rose Island, a short-lived microstate built on a sea platform in the Adriatic Sea.&amp;#34;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# ]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;retriever&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run(query&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Rose Island&amp;#34;&lt;/span&gt;, top_k&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;3&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns [&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     Document(content=&amp;#34;Esperanto has been adopted as official language for some microstates as well, such as the Republic of Rose Island, a short-lived microstate built on a sea platform in the Adriatic Sea.&amp;#34;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     Document(content=&amp;#34;Italy recognizes a few official languages, but the most widespread one is Italian.&amp;#34;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     Document(content=&amp;#34;The capital of France is Paris, and its official language is French.&amp;#34;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# ]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We see that &lt;a href=&#34;https://docs.haystack.deepset.ai/v2.0/reference/retriever-api#inmemorybm25retriever&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;InMemoryBM25Retriever&lt;/code&gt;&lt;/a&gt; accepts a few parameters. &lt;code&gt;query&lt;/code&gt; is the question we want to find relevant documents for. In the case of BM25, the algorithm only searches for exact word matches. The resulting retriever is very fast, but it doesn&amp;rsquo;t fail gracefully: it can&amp;rsquo;t handle spelling mistakes, synonyms, or descriptions of an entity. For example, documents containing the word &amp;ldquo;cat&amp;rdquo; would be considered irrelevant against a query such as &amp;ldquo;felines&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;top_k&lt;/code&gt; controls the number of documents returned. We can see that in the first example, only one document is returned, the correct one. In the second, where &lt;code&gt;top_k = 3&lt;/code&gt;, the retriever is forced to return three documents even if just one is relevant, so it picks the other two randomly. Although the behavior is not optimal, BM25 guarantees that if there is a document that is relevant to the query, it will be in the first position, so for now, we can use it with &lt;code&gt;top_k=1&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Retrievers also accepts a &lt;code&gt;filters&lt;/code&gt; parameter, which lets you pre-filter the documents before retrieval. This is a powerful technique that comes useful in complex applications, but for now we have no use for it. I will talk more in detail about this topic, called metadata filtering, in a later post.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now make use of this new component in our Pipeline.&lt;/p&gt;
&lt;h2 id=&#34;our-first-rag-pipeline&#34;&gt;
  Our first RAG Pipeline
  
&lt;/h2&gt;
&lt;p&gt;The retriever does not return a single string but a list of Documents. How do we put the content of these objects into our prompt template?&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s time to use Jinja&amp;rsquo;s powerful syntax to do some unpacking on our behalf.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Given the following information, answer the question.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Context: 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{% for document in documents %}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    {{ document.content }}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{% endfor %}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Question: What&amp;#39;s the official language of {{ country }}?
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Notice how, despite the slightly alien syntax for a Python programmer, what the template does is reasonably evident: it iterates over the documents and, for each of them, renders their &lt;code&gt;content&lt;/code&gt; field.&lt;/p&gt;
&lt;p&gt;With all these pieces set up, we can finally put them all together.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;template &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Given the following information, answer the question.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Context: 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;% f&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;or document in documents %}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;    {{ document.content }}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;% e&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;ndfor %}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Question: What&amp;#39;s the official language of {{ country }}?
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;retriever&amp;#34;&lt;/span&gt;, InMemoryBM25Retriever(document_store&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;docstore))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;, PromptBuilder(template&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;template))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;, OpenAIGenerator(api_key&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;api_key))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;retriever&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder.documents&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run({
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;retriever&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;query&amp;#34;&lt;/span&gt;: country},
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;: {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;country&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;the Republic of Rose Island&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;})
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns {&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     &amp;#34;llm&amp;#34;: {&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         &amp;#34;replies&amp;#34;: [&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#             &amp;#39;The official language of the Republic of Rose Island is Esperanto.&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         ]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     }&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# }&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-27-haystack-series-rag/bm25-rag-pipeline.png&#34; alt=&#34;BM25 RAG Pipeline&#34;&gt;&lt;/p&gt;
&lt;p&gt;Congratulations! We&amp;rsquo;ve just built our first, true-to-its-name RAG Pipeline.&lt;/p&gt;
&lt;h2 id=&#34;scaling-up-elasticsearch&#34;&gt;
  Scaling up: Elasticsearch
  
&lt;/h2&gt;
&lt;p&gt;So, we now have our running prototype. What does it take to scale this system up for production workloads?&lt;/p&gt;
&lt;p&gt;Of course, scaling up a system to production readiness is no simple task that can be addressed in a paragraph. Still, we can start this journey with one component that can readily be improved: the document store.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;InMemoryDocumentStore&lt;/code&gt; is clearly a toy implementation: Haystack supports much more performant document stores that make more sense to use in a production scenario. Since we have built our app with a BM25 retriever, let&amp;rsquo;s select &lt;a href=&#34;https://haystack.deepset.ai/integrations/elasticsearch-document-store&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Elasticsearch&lt;/a&gt; as our production-ready document store of choice.&lt;/p&gt;
&lt;div class=&#34;notice warning&#34;&gt;
  &lt;div class=&#34;notice-content&#34;&gt;⚠️ &lt;strong&gt;Warning:&lt;/strong&gt; &lt;em&gt;While ES is a valid document store to use in this scenario, nowadays if often makes more sense to choose a more specialized document store such as &lt;a href=&#34;https://github.com/deepset-ai/haystack-core-integrations/tree/main/integrations/weaviate&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Weaviate&lt;/a&gt;, &lt;a href=&#34;https://github.com/deepset-ai/haystack-core-integrations/tree/main/integrations/qdrant&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Qdrant&lt;/a&gt;, and so on. Check &lt;a href=&#34;https://github.com/deepset-ai/haystack-core-integrations/tree/main&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this page&lt;/a&gt; to see which document stores are currently supported for Haystack 2.0.&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;How do we use Elasticsearch on our pipeline? All it takes is to swap out &lt;code&gt;InMemoryDocumentStore&lt;/code&gt; and &lt;code&gt;InMemoryBM25Retriever&lt;/code&gt; with their Elasticsearch counterparts, which offer nearly identical APIs.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s create the document store: we will need a slightly more complex setup to connect to the Elasticearch backend. In this example, we use Elasticsearch version 8.8.0, but every Elasticsearch 8 version should work.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;elasticsearch_haystack.document_store&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; ElasticsearchDocumentStore
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;host &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; os&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;environ&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;get(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ELASTICSEARCH_HOST&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;https://localhost:9200&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;user &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;elastic&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pwd &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; os&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;environ[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ELASTICSEARCH_PASSWORD&amp;#34;&lt;/span&gt;]  &lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# You need to provide this value&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docstore &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; ElasticsearchDocumentStore(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    hosts&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[host], 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    basic_auth&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;(user, pwd), 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ca_certs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;/content/elasticsearch-8.8.0/config/certs/http_ca.crt&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now, let&amp;rsquo;s write again our four documents into the store. In this case, we specify the duplicate policy, so if the documents were already present, they would be overwritten. All Haystack document stores offer three policies to handle duplicates: &lt;code&gt;FAIL&lt;/code&gt; (the default), &lt;code&gt;SKIP&lt;/code&gt;, and &lt;code&gt;OVERWRITE&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;haystack.document_stores&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; DuplicatePolicy
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;documents &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; [
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Document(content&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;German is the the official language of Germany.&amp;#34;&lt;/span&gt;), 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Document(content&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;The capital of France is Paris, and its official language is French.&amp;#34;&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Document(content&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Italy recognizes a few official languages, but the most widespread one is Italian.&amp;#34;&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Document(content&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Esperanto has been adopted as official language for some microstates as well, such as the Republic of Rose Island, a short-lived microstate built on a sea platform in the Adriatic Sea.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docstore&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;write_documents(documents&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;documents, policy&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;DuplicatePolicy&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;OVERWRITE)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once this is done, we are ready to build the same pipeline as before, but using &lt;code&gt;ElasticsearchBM25Retriever&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;elasticsearch_haystack.bm25_retriever&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; ElasticsearchBM25Retriever
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;template &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Given the following information, answer the question.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Context: 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;% f&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;or document in documents %}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;    {{ document.content }}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;% e&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;ndfor %}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;Question: What&amp;#39;s the official language of {{ country }}?
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;retriever&amp;#34;&lt;/span&gt;, ElasticsearchBM25Retriever(document_store&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;docstore))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;, PromptBuilder(template&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;template))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;, OpenAIGenerator(api_key&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;api_key))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;retriever&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder.documents&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;draw(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;elasticsearch-rag-pipeline.png&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;country &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;the Republic of Rose Island&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipe&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run({
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;retriever&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;query&amp;#34;&lt;/span&gt;: country},
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;prompt_builder&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;country&amp;#34;&lt;/span&gt;: country}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;})
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns {&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     &amp;#34;llm&amp;#34;: {&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         &amp;#34;replies&amp;#34;: [&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#             &amp;#39;The official language of the Republic of Rose Island is Esperanto.&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#         ]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;#     }&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# }&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-27-haystack-series-rag/elasticsearch-rag-pipeline.png&#34; alt=&#34;Elasticsearch RAG Pipeline&#34;&gt;&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s it! We&amp;rsquo;re now running the same pipeline over a production-ready Elasticsearch instance.&lt;/p&gt;
&lt;h2 id=&#34;wrapping-up&#34;&gt;
  Wrapping up
  
&lt;/h2&gt;
&lt;p&gt;In this post, we&amp;rsquo;ve detailed some fundamental components that make RAG applications possible with Haystack: Generators, the PromptBuilder, and Retrievers. We&amp;rsquo;ve seen how they can all be used in isolation and how you can make Pipelines out of them to achieve the same goal. Last, we&amp;rsquo;ve experimented with some of the (very early!) features that make Haystack 2.0 production-ready and easy to scale up from a simple demo with minimal changes.&lt;/p&gt;
&lt;p&gt;However, this is just the start of our journey into RAG. Stay tuned!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Next: &lt;a href=&#34;https://www.zansara.dev/posts/2023-11-05-haystack-series-minimal-indexing&#34; &gt;Indexing data for RAG applications&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Previous: &lt;a href=&#34;https://www.zansara.dev/posts/2023-10-26-haystack-series-canals&#34; &gt;Canals: a new concept of Pipeline&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;See the entire series here: &lt;a href=&#34;https://www.zansara.dev/series/haystack-2.0-series/&#34; &gt;Haystack 2.0 series&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;em&gt;Cover image from &lt;a href=&#34;https://it.wikipedia.org/wiki/File:Isoladellerose.jpg&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia&lt;/a&gt;&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A New Approach to Haystack Pipelines</title>
      <link>https://www.zansara.dev/posts/2023-10-26-haystack-series-canals/</link>
      <pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2023-10-26-haystack-series-canals/</guid>
      <description>&lt;p&gt;&lt;em&gt;Updated on 21/12/2023&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;As we have seen in &lt;a href=&#34;https://www.zansara.dev/posts/2023-10-15-haystack-series-pipeline/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the previous episode of this series&lt;/a&gt;, Haystack&amp;rsquo;s Pipeline is a powerful concept that comes with its set of benefits and shortcomings. In Haystack 2.0, the pipeline was one of the first items that we focused our attention on, and it was the starting point of the entire rewrite.&lt;/p&gt;
&lt;p&gt;What does this mean in practice? Let&amp;rsquo;s look at what Haystack Pipelines in 2.0 will be like, how they differ from their 1.x counterparts, and the pros and cons of this new paradigm.&lt;/p&gt;
&lt;h2 id=&#34;new-use-cases&#34;&gt;
  New Use Cases
  
&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;ve already written &lt;a href=&#34;https://www.zansara.dev/posts/2023-10-15-haystack-series-pipeline/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;at length&lt;/a&gt; about what made the original Pipeline concept so powerful and its weaknesses. Pipelines were overly effective for the use cases we could conceive while developing them, but they didn&amp;rsquo;t generalize well on unforeseen situations.&lt;/p&gt;
&lt;p&gt;For a long time, Haystack was able to afford not focusing on use cases that didn&amp;rsquo;t fit its architecture, as I have mentioned in my &lt;a href=&#34;https://www.zansara.dev/posts/2023-10-11-haystack-series-why/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt; about the reasons for the rewrite. The pipeline was then more than sufficient for its purposes.&lt;/p&gt;
&lt;p&gt;However, the situation flipped as LLMs and Generative AI &amp;ldquo;entered&amp;rdquo; the scene abruptly at the end of 2022 (although it&amp;rsquo;s certainly been around for longer). Our &lt;code&gt;Pipeline&lt;/code&gt; although useable and still quite powerful in many LLM use-cases, seemingly overfit the original use-cases it was designed for.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take one of these use cases and see where it leads us.&lt;/p&gt;
&lt;h2 id=&#34;rag-pipelines&#34;&gt;
  RAG Pipelines
  
&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s take one typical example: &lt;a href=&#34;https://www.deepset.ai/blog/llms-retrieval-augmentation&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;retrieval augmented generation&lt;/a&gt;, or RAG for short. This technique has been used since the very early days of the Generative AI boom as an easy way to strongly &lt;a href=&#34;https://haystack.deepset.ai/blog/generative-vs-extractive-models&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reduce hallucinations&lt;/a&gt; and improve the alignment of LLMs. The basic idea is: instead of asking directly a question, such as &lt;code&gt;&amp;quot;What&#39;s the capital of France?&amp;quot;&lt;/code&gt;, we send to the model a more complex prompt, that includes both the question and the answer. Such a prompt might be:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Given the following paragraph, answer the question.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Paragraph: France is a unitary semi-presidential republic with its capital in Paris, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;the country&amp;#39;s largest city and main cultural and commercial centre; other major urban 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;areas include Marseille, Lyon, Toulouse, Lille, Bordeaux, Strasbourg and Nice.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Question: What&amp;#39;s the capital of France?
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Answer:
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In this situation, the task of the LLM becomes far easier: instead of drawing facts from its internal knowledge, which might be lacking, inaccurate, or out-of-date, the model can use the paragraph&amp;rsquo;s content to answer the question, improving the model&amp;rsquo;s performance significantly.&lt;/p&gt;
&lt;p&gt;We now have a new problem, though. How can we provide the correct snippets of text to the LLM? This is where the &amp;ldquo;retrieval&amp;rdquo; keyword comes up.&lt;/p&gt;
&lt;p&gt;One of Haystack&amp;rsquo;s primary use cases had been &lt;a href=&#34;https://huggingface.co/tasks/question-answering&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Extractive Question Answering&lt;/a&gt;: a system where a Retriever component searches a Document Store (such as a vector or SQL database) for snippets of text that are the most relevant to a given question. It then sends such snippets to a Reader (an extractive model), which highlights the keywords that answer the original question.&lt;/p&gt;
&lt;p&gt;By replacing a Reader model with an LLM, we get a Retrieval Augmented Generation Pipeline. Easy!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-26-haystack-series-canals/gen-vs-ext-qa-pipeline.png&#34; alt=&#34;Generative vs Extractive QA Pipeline Graph&#34;&gt;&lt;/p&gt;
&lt;p&gt;So far, everything checks out. Supporting RAG with Haystack feels not only possible but natural. Let&amp;rsquo;s take this simple example one step forward: what if, instead of getting the data from a document store, I want to retrieve data from the Internet?&lt;/p&gt;
&lt;h2 id=&#34;web-rag&#34;&gt;
  Web RAG
  
&lt;/h2&gt;
&lt;p&gt;At first glance, the task may not seem daunting. We surely need a special Retriever that, instead of searching through a DB, searches through the Internet using a search engine. But the core concepts stay the same, and so, we assume, should the pipeline&amp;rsquo;s graph. The end result should be something like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-26-haystack-series-canals/initial-web-rag-pipeline.png&#34; alt=&#34;Initial Web RAG Pipeline Graph&#34;&gt;&lt;/p&gt;
&lt;p&gt;However, the problem doesn&amp;rsquo;t end there. Search engines return links, which need to be accessed, and the content of the webpage downloaded. Such pages may be extensive and contain artifacts, so the resulting text needs to be cleaned, reduced into paragraphs, potentially embedded by a retrieval model, ranked against the original query, and only the top few resulting pieces of text need to be passed over to the LLM. Just by including these minimal requirements, our pipeline already looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-26-haystack-series-canals/linear-web-rag-pipeline.png&#34; alt=&#34;Linear Web RAG Pipeline Graph&#34;&gt;&lt;/p&gt;
&lt;p&gt;And we still need to consider that URLs may reference not HTML pages but PDFs, videos, zip files, and so on. We need file converters, zip extractors, audio transcribers, and so on.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-26-haystack-series-canals/multifile-web-rag-pipeline.png&#34; alt=&#34;Multiple File Type Web RAG Pipeline Graph&#34;&gt;&lt;/p&gt;
&lt;p&gt;You may notice how this use case moved quickly from looking like a simple query pipeline into a strange overlap of a query and an indexing pipeline. As we&amp;rsquo;ve learned in the previous post, indexing pipelines have their own set of quirks, one of which is that they can&amp;rsquo;t simultaneously process files of different types. But we can only expect the Search Engine to retrieve HTML files or PDFs if we filter them out on purpose, which makes the pipeline less effective. In fact, a pipeline that can read content from different file types, such as the one above, can&amp;rsquo;t really be made to work.&lt;/p&gt;
&lt;p&gt;And what if, on top of this, we need to cache the resulting documents to reduce latency? What if I wanted to get the results from Google&amp;rsquo;s page 2, but only if the content of page 1 did not answer our question? At this point, the pipeline is hard to imagine, let alone draw.&lt;/p&gt;
&lt;p&gt;Although Web RAG is somewhat possible in Haystack, it stretches far beyond what the pipeline was designed to handle. Can we do better?&lt;/p&gt;
&lt;h2 id=&#34;pinpointing-the-issue&#34;&gt;
  Pinpointing the issue
  
&lt;/h2&gt;
&lt;p&gt;When we went back to the drawing board to address these concerns, the first step was pinpointing the issue.&lt;/p&gt;
&lt;p&gt;The root problem, as we realized, is that Haystack Pipelines treats each component as a locomotive treats its wagons. They all look the same from the pipeline&amp;rsquo;s perspective, they can all be connected in any order, and they all go from A to B rolling over the same pair of rails, passing all through the same stations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-26-haystack-series-canals/train.png&#34; alt=&#34;Cargo Train&#34;&gt;&lt;/p&gt;
&lt;p&gt;In Haystack 1, components are designed to serve the pipeline&amp;rsquo;s needs first. A good component is identical to all the others, provides the exact interface the pipeline requires, and can be connected to any other in any order. The components are awkward to use outside of a pipeline due to the same &lt;code&gt;run()&lt;/code&gt; method that makes the pipeline so ergonomic. Why does the Ranker, which needs only a query and a list of Documents to operate, also accept &lt;code&gt;file_paths&lt;/code&gt; and &lt;code&gt;meta&lt;/code&gt; in its &lt;code&gt;run()&lt;/code&gt; method? It does so uniquely to satisfy the pipeline&amp;rsquo;s requirements, which in turn only exist to make all components forcefully compatible with each other.&lt;/p&gt;
&lt;p&gt;Just like a locomotive, the pipeline pushes the components over the input data one by one. When seen in this light, it&amp;rsquo;s painfully obvious why the indexing pipeline we&amp;rsquo;ve seen earlier can&amp;rsquo;t work: the &amp;ldquo;pipeline train&amp;rdquo; can only go on one branch at a time. Component trains can&amp;rsquo;t split mid-execution. They are designed to all see the same data all the time. Even when branching happens, all branches always see the same data. Sending different wagons onto different rails is not possible by design.&lt;/p&gt;
&lt;h2 id=&#34;breaking-it-down&#34;&gt;
  Breaking it down
  
&lt;/h2&gt;
&lt;p&gt;The issue&amp;rsquo;s core is more evident when seen in this light. The pipeline is the only object that drives the execution, while components tend to be as passive and uniform as possible. This approach doesn&amp;rsquo;t scale: components are fundamentally different, and asking them to all appear equal forces them to hide their differences, making bugs and odd behavior more likely. As the number of components to handle grows, their variety will increase regardless, so the pipeline must always be aware of all the possibilities to manage them and progressively add edge cases that rapidly increase its complexity.&lt;/p&gt;
&lt;p&gt;Therefore, the pipeline rewrite for Haystack 2.0 focused on one core principle: the components will define and drive the execution process. There is no locomotive anymore: every component needs to find its way, such as grabbing the data they need from the producers and sending their results to whoever needs them by declaring the proper connections. In the railway metaphor, it&amp;rsquo;s like adding a steering wheel to each container: the result is a truck, and the resulting system looks now like a highway.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-26-haystack-series-canals/highway.png&#34; alt=&#34;Highway&#34;&gt;&lt;/p&gt;
&lt;p&gt;Just as railways are excellent at going from A to B when you only need to take a few well-known routes and never another, highways are unbeatable at reaching every possible destination with the same effort, even though they need a driver for each wagon. A &amp;ldquo;highway&amp;rdquo; Pipeline requires more work from the Components&amp;rsquo; side, but it frees them to go wherever they need to with a precision that a &amp;ldquo;railway&amp;rdquo; pipeline cannot accomplish.&lt;/p&gt;
&lt;h2 id=&#34;the-structure-of-haystack-20&#34;&gt;
  The Structure of Haystack 2.0
  
&lt;/h2&gt;
&lt;p&gt;By design, pipelines in Haystack 2.0 is not geared toward specific NLP use cases, but it&amp;rsquo;s a minimal, generic &lt;a href=&#34;https://en.wikipedia.org/wiki/Extract,_transform,_load&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ETL&lt;/a&gt;-like class.&lt;/p&gt;
&lt;p&gt;At its core, Haystack 2.0 builds upon these two fundamental concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;Component&lt;/code&gt; protocol, a well-defined API that Python classes need to respect to be understood by the pipeline.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;Pipeline&lt;/code&gt; object, the graph resolver and execution engine that also performs validation and provides a few utilities on top.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s explore these two concepts one by one.&lt;/p&gt;
&lt;h3 id=&#34;the-pipeline-api&#34;&gt;
  The Pipeline API
  
&lt;/h3&gt;
&lt;p&gt;The new &lt;code&gt;Pipeline&lt;/code&gt; object may remind vaguely of Haystack&amp;rsquo;s original pipeline, and using one should feel very familiar. For example, this is how you assemble a simple Pipeline that performs two additions in Haystack 2.0.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;canals&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; Pipeline
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;sample_components&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; AddFixedValue
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# Create the Pipeline object&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# Add the components - note the missing`inputs` parameter&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;add_one&amp;#34;&lt;/span&gt;, AddFixedValue(add&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_component(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;add_two&amp;#34;&lt;/span&gt;, AddFixedValue(add&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;2&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# Connect them together&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;connect(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;add_one.result&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;add_two.value&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# Draw the pipeline&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;draw(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;two_additions_pipeline.png&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# Run the pipeline&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;results &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run({&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;add_one&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;value&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;}})
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(results)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# prints &amp;#39;{&amp;#34;add_two&amp;#34;: {&amp;#34;result&amp;#34;: 4}}&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Creating the pipeline requires no special attention: however, you can now pass a &lt;code&gt;max_loops_allowed&lt;/code&gt; parameter, to limit looping when it&amp;rsquo;s a risk. On the contrary, old Haystack 1.x Pipelines did not support loops at all.&lt;/p&gt;
&lt;p&gt;Next, components are added by calling the &lt;code&gt;Pipeline.add_component(name, component)&lt;/code&gt; method. This is also subject to very similar requirements to the previous &lt;code&gt;pipeline.add_node&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Every component needs a unique name.&lt;/li&gt;
&lt;li&gt;Some are reserved (for now, only &lt;code&gt;_debug&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Instances are not reusable.&lt;/li&gt;
&lt;li&gt;The object needs to be a component.
However, we no longer connect the components to each other using this function because, although it is possible to implement in principle, it feels more awkward to use in the case of loops.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Consequently, we introduced a new method, &lt;code&gt;Pipeline.connect()&lt;/code&gt;. This method follows the syntax &lt;code&gt;(&amp;quot;producer_component.output_name_&amp;quot;, &amp;quot;consumer_component.input_name&amp;quot;)&lt;/code&gt;: so we don&amp;rsquo;t simply line up two components one after the other, but we connect one of their outputs to one of their inputs in an explicit manner.&lt;/p&gt;
&lt;p&gt;This change allows pipelines to perform a much more careful validation of such connections. As we will discover soon, pipeline components in Haystack 2.0 must declare the type of their inputs and outputs. In this way, pipelines not only can make sure that the inputs and outputs exist for the given component, but they can also check whether their types match and can explain connection failures in great detail. For example, if there were a type mismatch, &lt;code&gt;Pipeline.connect()&lt;/code&gt; will return an error such as:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Cannot connect &amp;#39;greeter.greeting&amp;#39; with &amp;#39;add_two.value&amp;#39;: their declared input and output 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;types do not match.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;greeter:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;-&lt;/span&gt; greeting: str
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;add_two:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;-&lt;/span&gt; value: int (available)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;-&lt;/span&gt; add: Optional[int] (available)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once the components are connected together, the resulting pipeline can be drawn. Pipeline drawings in Haystack 2.0 show far more details than their predecessors because the components are forced to share much more information about what they need to run, the types of these variables, and so on. The pipeline above draws the following image:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-26-haystack-series-canals/two_additions_pipeline.png&#34; alt=&#34;A Pipeline making two additions&#34;&gt;&lt;/p&gt;
&lt;p&gt;You can see how the components classes, their inputs and outputs, and all the connections are named and typed.&lt;/p&gt;
&lt;p&gt;So, how do you run such a pipeline? By just providing a dictionary of input values. Each starting component should have a small dictionary with all the necessary inputs. In the example above, we pass &lt;code&gt;1&lt;/code&gt; to the &lt;code&gt;value&lt;/code&gt; input of &lt;code&gt;add_one&lt;/code&gt;. The results mirror the input&amp;rsquo;s structure: &lt;code&gt;add_two&lt;/code&gt; is at the end of the pipeline, so the pipeline will return a dictionary where under the &lt;code&gt;add_two&lt;/code&gt; key there is a dictionary: &lt;code&gt;{&amp;quot;result&amp;quot;: 4}&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;By looking at the diagram, you may have noticed that these two components have optional inputs. They&amp;rsquo;re not necessary for the pipeline to run, but they can be used to dynamically control the behavior of these components. In this case, &lt;code&gt;add&lt;/code&gt; controls the &amp;ldquo;fixed value&amp;rdquo; this component adds to its primary input. For example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run({&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;add_one&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;value&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;add&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;2&lt;/span&gt;}})
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns &amp;#39;{&amp;#34;add_two&amp;#34;: {&amp;#34;result&amp;#34;: 5}}&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run({&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;add_one&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;value&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;}, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;add_two&amp;#34;&lt;/span&gt;: {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;add&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;10&lt;/span&gt;}})
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# returns &amp;#39;{&amp;#34;add_two&amp;#34;: {&amp;#34;result&amp;#34;: 12}}&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;One evident difficulty of this API is that it might be challenging to understand what to provide to the run method for each component. This issue has also been considered: the pipeline offers a &lt;code&gt;Pipeline.inputs()&lt;/code&gt; method that returns a structured representation of all the expected input. For our pipeline, it looks like:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;add_one&amp;#34;&lt;/span&gt;: {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;value&amp;#34;&lt;/span&gt;: {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: int, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;is_optional&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#79c0ff&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        }, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;add&amp;#34;&lt;/span&gt;: {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: typing&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;Optional[int], 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;is_optional&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#79c0ff&#34;&gt;True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;add_two&amp;#34;&lt;/span&gt;: {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;add&amp;#34;&lt;/span&gt;: {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: typing&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;Optional[int], 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;is_optional&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#79c0ff&#34;&gt;True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;the-component-api&#34;&gt;
  The Component API
  
&lt;/h2&gt;
&lt;p&gt;Now that we covered the Pipeline&amp;rsquo;s API, let&amp;rsquo;s have a look at what it takes for a Python class to be treated as a pipeline component.&lt;/p&gt;
&lt;p&gt;You are going to need:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A &lt;code&gt;@component&lt;/code&gt; decorator&lt;/strong&gt;. All component classes must be decorated with the &lt;code&gt;@component&lt;/code&gt; decorator. This allows a pipeline to discover and validate them.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A &lt;code&gt;run()&lt;/code&gt; method&lt;/strong&gt;. This is the method where the main functionality of the component should be carried out. It&amp;rsquo;s invoked by &lt;code&gt;Pipeline.run()&lt;/code&gt; and has a few constraints, which we will describe later.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;A &lt;code&gt;@component.output_types()&lt;/code&gt; decorator for the &lt;code&gt;run()&lt;/code&gt; method&lt;/strong&gt;. This allows the pipeline to validate the connections between components.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Optionally, &lt;strong&gt;a &lt;code&gt;warm_up()&lt;/code&gt; method&lt;/strong&gt;. It can be used to defer the loading of a heavy resource (think a local LLM or an embedding model) to the warm-up stage that occurs right before the first execution of the pipeline. Components that use &lt;code&gt;warm_up()&lt;/code&gt; can be added to a Pipeline and connected before the heavy operations are carried out. In this way, the validation that a &lt;code&gt;Pipeline&lt;/code&gt; performs can happen before resources are wasted.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To summarize, a minimal component can look like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;canals&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; component
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;@component&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#f0883e;font-weight:bold&#34;&gt;Double&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;@component.output_types&lt;/span&gt;(result&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;int)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;run&lt;/span&gt;(self, value: int):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff7b72&#34;&gt;return&lt;/span&gt; {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;result&amp;#34;&lt;/span&gt;: value &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;2&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;pipeline-validation&#34;&gt;
  Pipeline Validation
  
&lt;/h3&gt;
&lt;p&gt;Note how the &lt;code&gt;run()&lt;/code&gt; method has a few peculiar features. One is that all the method parameters need to be typed: if &lt;code&gt;value&lt;/code&gt; was not declared as &lt;code&gt;value: int&lt;/code&gt;, the pipeline would raise an exception demanding for typing.&lt;/p&gt;
&lt;p&gt;This is the way components declare to the pipeline which inputs they expect and of which type: this is the first half of the information needed to perform the validation that &lt;code&gt;Pipeline.connect()&lt;/code&gt; carries out.&lt;/p&gt;
&lt;p&gt;The other half of the information comes from the &lt;code&gt;@component.output_types&lt;/code&gt; decorator. Pipelines demand that components declare how many outputs the component will produce and of what type. One may ask why not rely on typing for the outputs, just as we&amp;rsquo;ve done for the inputs. So why not simply declare components as:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;@component&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#f0883e;font-weight:bold&#34;&gt;Double&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;run&lt;/span&gt;(self, value: int) &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;-&amp;gt;&lt;/span&gt; int:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff7b72&#34;&gt;return&lt;/span&gt; value &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For &lt;code&gt;Double&lt;/code&gt;, this is a legitimate solution. However, let&amp;rsquo;s see an example with another component called &lt;code&gt;CheckParity&lt;/code&gt;: if a component&amp;rsquo;s input value is even, it sends it unchanged over the &lt;code&gt;even&lt;/code&gt; output, while if it&amp;rsquo;s odd, it will send it over the &lt;code&gt;odd&lt;/code&gt; output. The following clearly doesn&amp;rsquo;t work: we&amp;rsquo;re not communicating anywhere to Canals which output is even and which one is odd.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;@component&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#f0883e;font-weight:bold&#34;&gt;CheckParity&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;run&lt;/span&gt;(self, value: int) &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;-&amp;gt;&lt;/span&gt; int:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; value &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#ff7b72&#34;&gt;return&lt;/span&gt; value
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff7b72&#34;&gt;return&lt;/span&gt; value
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;How about this instead?&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;@component&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#f0883e;font-weight:bold&#34;&gt;CheckParity&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;run&lt;/span&gt;(self, value: int) &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;-&amp;gt;&lt;/span&gt; Dict[str, int]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; value &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#ff7b72&#34;&gt;return&lt;/span&gt; {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;even&amp;#34;&lt;/span&gt;: value}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff7b72&#34;&gt;return&lt;/span&gt; {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;odd&amp;#34;&lt;/span&gt;: value}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This approach carries all the information required. However, such information is only available after the &lt;code&gt;run()&lt;/code&gt; method is called. Unless we parse the method to discover all return statements and their keys (which is not always possible), pipelines cannot know all the keys the return dictionary may have. So, it can&amp;rsquo;t validate the connections when &lt;code&gt;Pipeline.connect()&lt;/code&gt; is called.&lt;/p&gt;
&lt;p&gt;The decorator bridges the gap by allowing the class to declare in advance what outputs it will produce and of which type. Pipeline trusts this information to be correct and validates the connections accordingly.&lt;/p&gt;
&lt;p&gt;Okay, but what if the component is very dynamic? The output type may depend on the input type. Perhaps the number of inputs depends on some initialization parameter. In these cases, pipelines allow components to declare the inputs and output types in their init method as such:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;@component&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#f0883e;font-weight:bold&#34;&gt;HighlyDynamicComponent&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;__init__&lt;/span&gt;(self, &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;...&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;set_input_types(self, input_name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;input_type, &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;...&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;set_output_types(self, output_name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;output_type, &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;...&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;run&lt;/span&gt;(self, &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;**&lt;/span&gt;kwargs):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that there&amp;rsquo;s no more typing on &lt;code&gt;run()&lt;/code&gt;, and the decorator is gone. The information provided in the init method is sufficient for the pipeline to validate the connections.&lt;/p&gt;
&lt;p&gt;One more feature of the inputs and output declarations relates to optional and variadic values. Pipelines in Haystack 2.0 support this both through a mix of type checking and signature inspection. For example, let&amp;rsquo;s have a look at how the &lt;code&gt;AddFixedValue&lt;/code&gt; we&amp;rsquo;ve seen earlier looks like:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;typing&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; Optional
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;canals&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; component
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;@component&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#f0883e;font-weight:bold&#34;&gt;AddFixedValue&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;    Adds two values together.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;__init__&lt;/span&gt;(self, add: int &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; add
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;@component.output_types&lt;/span&gt;(result&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;int)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;run&lt;/span&gt;(self, value: int, add: Optional[int] &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#79c0ff&#34;&gt;None&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;        Adds two values together.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;        &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; add &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#79c0ff&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            add &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff7b72&#34;&gt;return&lt;/span&gt; {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;result&amp;#34;&lt;/span&gt;: value &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; add}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can see that &lt;code&gt;add&lt;/code&gt;, the optional parameter we met before, has a default value. Adding a default value to a parameter in the &lt;code&gt;run()&lt;/code&gt; signature tells the pipeline that the parameter itself is optional, so the component can run even if that specific input doesn&amp;rsquo;t receive any value from the pipeline&amp;rsquo;s input or other components.&lt;/p&gt;
&lt;p&gt;Another component that generalizes the sum operation is &lt;code&gt;Sum&lt;/code&gt;, which instead looks like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;canals&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; component
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;canals.component.types&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; Variadic
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;@component&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#f0883e;font-weight:bold&#34;&gt;Sum&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;    Adds all its inputs together.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;@component.output_types&lt;/span&gt;(total&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;int)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;run&lt;/span&gt;(self, values: Variadic[int]):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;        :param values: the values to sum
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;        &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff7b72&#34;&gt;return&lt;/span&gt; {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;total&amp;#34;&lt;/span&gt;: sum(v &lt;span style=&#34;color:#ff7b72&#34;&gt;for&lt;/span&gt; v &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;in&lt;/span&gt; values &lt;span style=&#34;color:#ff7b72&#34;&gt;if&lt;/span&gt; v &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#79c0ff&#34;&gt;None&lt;/span&gt;)}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In this case, we used the special type &lt;code&gt;Variadic&lt;/code&gt; to tell the pipeline that the &lt;code&gt;values&lt;/code&gt; input can receive data from multiple producers, instead of just one. Therefore, &lt;code&gt;values&lt;/code&gt; is going to be a list type, but it can be connected to single &lt;code&gt;int&lt;/code&gt; outputs, making it a valuable aggregator.&lt;/p&gt;
&lt;h2 id=&#34;serialization&#34;&gt;
  Serialization
  
&lt;/h2&gt;
&lt;p&gt;Just like old Haystack Pipelines, the new pipelines can be serialized. However, this feature suffered from similar problems plaguing the execution model,  so it was changed radically.&lt;/p&gt;
&lt;p&gt;The original pipeline gathered intrusive information about each of its components when initialized, leveraging the shared &lt;code&gt;BaseComponent&lt;/code&gt; class. Conversely, the &lt;code&gt;Pipeline&lt;/code&gt; delegates the serialization process entirely to its components.&lt;/p&gt;
&lt;p&gt;If a component wishes to be serializable, it must provide two additional methods, &lt;code&gt;to_dict&lt;/code&gt; and &lt;code&gt;from_dict&lt;/code&gt;, which perform serialization and deserialization to a dictionary. The pipeline limits itself to calling each of its component&amp;rsquo;s methods, collecting their output, grouping them together with some limited extra information (such as the connections between them), and returning the result.&lt;/p&gt;
&lt;p&gt;For example, if &lt;code&gt;AddFixedValue&lt;/code&gt; were serializable, its serialized version could look like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;AddFixedValue&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;init_parameters&amp;#34;&lt;/span&gt;: {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;add&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The entire pipeline we used above would end up as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;max_loops_allowed&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;100&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;components&amp;#34;&lt;/span&gt;: {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;add_one&amp;#34;&lt;/span&gt;: {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;AddFixedValue&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;init_parameters&amp;#34;&lt;/span&gt;: {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;add&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        },
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;add_two&amp;#34;&lt;/span&gt;: {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;AddFixedValue&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;init_parameters&amp;#34;&lt;/span&gt;: {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;add&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    },
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;connections&amp;#34;&lt;/span&gt;: [
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;sender&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;add_one.result&amp;#34;&lt;/span&gt;, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;receiver&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;add_two.value&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Notice how the components are free to perform serialization in the way they see fit. The only requirement imposed by the &lt;code&gt;Pipeline&lt;/code&gt; is the presence of two top-level keys, &lt;code&gt;type&lt;/code&gt; and &lt;code&gt;init_parameters&lt;/code&gt;, which are necessary for the pipeline to deserialize each component into the correct class.&lt;/p&gt;
&lt;p&gt;This is useful, especially if the component&amp;rsquo;s state includes some non-trivial values, such as objects, API keys, or other special values. Pipeline no longer needs to know how to serialize everything the Components may contain: the task is fully delegated to them, which always knows best what needs to be done.&lt;/p&gt;
&lt;h2 id=&#34;but-do-we-need-any-of-this&#34;&gt;
  But&amp;hellip; do we need any of this?
  
&lt;/h2&gt;
&lt;p&gt;Having done a tour of the new &lt;code&gt;Pipeline&lt;/code&gt; features, one might have noticed one detail. There&amp;rsquo;s a bit more work involved in using a Pipeline than there was before: you can&amp;rsquo;t just chain every component after every other. There are connections to be made, validation to perform, graphs to assemble, and so on.&lt;/p&gt;
&lt;p&gt;In exchange, the pipeline is now way more powerful than before. Sure, but so is a plain Python script. Do we &lt;em&gt;really&lt;/em&gt; need the Pipeline object? And what do we need it for?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Validation&lt;/strong&gt;. While components normally validate their inputs and outputs, the pipeline does all the validation before the components run, even before loading heavy resources. This makes the whole system far less likely to fail at runtime for a simple input/output mismatch, which can be priceless for complex applications.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Serialization&lt;/strong&gt;. Redistributing code is always tricky: redistributing a JSON file is much safer. Pipelines make it possible to represent complex systems in a readable JSON file that can be edited, shared, stored, deployed, and re-deployed on different backends at need.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Drawing&lt;/strong&gt;: The new Pipeline offers a way to see your system clearly and automatically, which is often very handy for debugging, inspecting the system, and collaborating on the pipeline&amp;rsquo;s design.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;On top of this, the pipeline abstraction promotes flatter API surfaces by discouraging components nesting one within the other and providing easy-to-use, single-responsibility components that are easy to reason about.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Having said all of this, however, we don&amp;rsquo;t believe that the pipeline design makes Haystack win or lose. Pipelines are just a bonus on top of what provides the real value: a broad set of components that reliably perform well-defined tasks. That&amp;rsquo;s why the Component API does not make the &lt;code&gt;run()&lt;/code&gt; method awkward to use outside of a Pipeline: calling &lt;code&gt;Sum.run(values=[1, 2, 3])&lt;/code&gt; feels Pythonic outside of a pipeline and always will.&lt;/p&gt;
&lt;p&gt;In the following posts, I will explore the world of Haystack components, starting from our now familiar use cases: RAG Pipelines.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Next: &lt;a href=&#34;https://www.zansara.dev/posts/2023-10-27-haystack-series-rag&#34; &gt;RAG Pipelines from scratch&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Previous: &lt;a href=&#34;https://www.zansara.dev/posts/2023-10-15-haystack-series-pipeline&#34; &gt;Haystack&amp;rsquo;s Pipeline&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;See the entire series here: &lt;a href=&#34;https://www.zansara.dev/series/haystack-2.0-series/&#34; &gt;Haystack 2.0 series&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Haystack 2.0: RAG Pipelines</title>
      <link>https://www.zansara.dev/demos/2023-10-25-haystack-2.0-rag-pipelines/</link>
      <pubDate>Wed, 25 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/demos/2023-10-25-haystack-2.0-rag-pipelines/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Haystack&#39;s Pipeline - A Deep Dive</title>
      <link>https://www.zansara.dev/posts/2023-10-15-haystack-series-pipeline/</link>
      <pubDate>Sun, 15 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2023-10-15-haystack-series-pipeline/</guid>
      <description>&lt;p&gt;If you&amp;rsquo;ve ever looked at Haystack before, you must have come across the &lt;a href=&#34;https://docs.haystack.deepset.ai/docs/pipelines&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pipeline&lt;/a&gt;, one of the most prominent concepts of the framework. However, this abstraction is by no means an obvious choice when it comes to NLP libraries. Why did we adopt this concept, and what does it bring us?&lt;/p&gt;
&lt;p&gt;In this post, I go into all the details of how the Pipeline abstraction works in Haystack now, why it works this way, and its strengths and weaknesses. This deep dive into the current state of the framework is also a premise for the next episode, where I will explain how Haystack 2.0 addresses this version&amp;rsquo;s shortcomings.&lt;/p&gt;
&lt;p&gt;If you think you already know how Haystack Pipelines work, give this post a chance: I might manage to change your mind.&lt;/p&gt;
&lt;h2 id=&#34;a-bit-of-history&#34;&gt;
  A Bit Of History
  
&lt;/h2&gt;
&lt;p&gt;Interestingly, in the very first releases of Haystack, Pipelines were not a thing. Version 0.1.0 was released with a simpler object, the &lt;a href=&#34;https://github.com/deepset-ai/haystack/blob/d2c77f307788899eb562d3cb6e42c69b968b9f2a/haystack/__init__.py#L16&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Finder&lt;/a&gt;, that did little more than gluing together a &lt;a href=&#34;https://docs.haystack.deepset.ai/docs/retriever&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Retriever&lt;/a&gt; and a &lt;a href=&#34;https://docs.haystack.deepset.ai/docs/reader&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reader&lt;/a&gt;, the two fundamental building blocks of a &lt;a href=&#34;https://docs.haystack.deepset.ai/docs/glossary#semantic-search&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;semantic search&lt;/a&gt; application.&lt;/p&gt;
&lt;p&gt;In the next few months, however, the capabilities of language models expanded to enable many more use cases. One hot topic was &lt;a href=&#34;https://haystack.deepset.ai/blog/hybrid-retrieval&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;hybrid retrieval&lt;/a&gt;: a system composed of two different Retrievers, an optional &lt;a href=&#34;https://docs.haystack.deepset.ai/docs/ranker&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ranker&lt;/a&gt;, and an optional Reader. This kind of application clearly didn&amp;rsquo;t fit the Finder&amp;rsquo;s design, so in &lt;a href=&#34;https://github.com/deepset-ai/haystack/releases/tag/v0.6.0&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;version 0.6.0&lt;/a&gt; the &lt;a href=&#34;https://docs.haystack.deepset.ai/docs/pipelines&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pipeline&lt;/a&gt; object was introduced: a new abstraction that helped users build applications as a graph of components.&lt;/p&gt;
&lt;p&gt;Pipeline&amp;rsquo;s API was a huge step forward from Finder. It instantly enabled seemingly endless combinations of components, unlocked almost all use cases conceivable, and became a foundational Haystack concept meant to stay for a very long time. In fact, the API offered by the first version of Pipeline changed very little since its initial release.&lt;/p&gt;
&lt;p&gt;This is the snippet included in the release notes of version 0.6.0 to showcase hybrid retrieval. Does it look familiar?&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;p &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;p&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;es_retriever, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ESRetriever&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Query&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;p&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;dpr_retriever, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DPRRetriever&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Query&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;p&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;JoinDocuments(join_mode&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;concatenate&amp;#34;&lt;/span&gt;), name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;JoinResults&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ESRetriever&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DPRRetriever&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;p&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;reader, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;QAReader&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;JoinResults&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;res &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; p&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run(query&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;What did Einstein work on?&amp;#34;&lt;/span&gt;, top_k_retriever&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;a-powerful-abstraction&#34;&gt;
  A Powerful Abstraction
  
&lt;/h2&gt;
&lt;p&gt;One fascinating aspect of this Pipeline model is the simplicity of its user-facing API. In almost all examples, you see only two or three methods used:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;add_node&lt;/code&gt;: to add a component to the graph and connect it to the others.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;run&lt;/code&gt;: to run the Pipeline from start to finish.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;draw&lt;/code&gt;: to draw the graph of the Pipeline to an image.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At this level, users don&amp;rsquo;t need to know what kind of data the components need to function, what they produce, or even what the components &lt;em&gt;do&lt;/em&gt;: all they need to know is the place they must occupy in the graph for the system to work.&lt;/p&gt;
&lt;p&gt;For example, as long as the users know that their hybrid retrieval pipeline should look more or less like this (note: this is the output of &lt;code&gt;Pipeline.draw()&lt;/code&gt;), translating it into a Haystack Pipeline object using a few &lt;code&gt;add_node&lt;/code&gt; calls is mostly straightforward.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-15-haystack-series-pipeline/hybrid-retrieval.png&#34; alt=&#34;Hybrid Retrieval&#34;&gt;&lt;/p&gt;
&lt;p&gt;This fact is reflected by the documentation of the various components as well. For example, this is how the documentation page for Ranker opens:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-15-haystack-series-pipeline/ranker-docs.png&#34; alt=&#34;Ranker Documentation&#34;&gt;&lt;/p&gt;
&lt;p&gt;Note how the first information about this component is &lt;em&gt;where to place it&lt;/em&gt;. Right after, it specifies its inputs and outputs, even though it&amp;rsquo;s not immediately clear why we need this information, and then lists which specific classes can cover the role of a Ranker.&lt;/p&gt;
&lt;p&gt;The message is clear: all Ranker classes are functionally interchangeable, and as long as you place them correctly in the Pipeline, they will fulfill the function of Ranker as you expect them to. Users don&amp;rsquo;t need to understand what distinguishes &lt;code&gt;CohereRanker&lt;/code&gt; from &lt;code&gt;RecentnessReranker&lt;/code&gt; unless they want to: the documentation promises that you can swap them safely, and thanks to the Pipeline abstraction, this statement mostly holds true.&lt;/p&gt;
&lt;h2 id=&#34;ready-made-pipelines&#34;&gt;
  Ready-made Pipelines
  
&lt;/h2&gt;
&lt;p&gt;But how can the users know which sort of graph they have to build?&lt;/p&gt;
&lt;p&gt;Most NLP applications are made by a relatively limited number of high-level components: Retriever, Readers, Rankers, plus the occasional Classifier, Translator, or Summarizer. Systems requiring something more than these components used to be really rare, at least when talking about &amp;ldquo;query&amp;rdquo; pipelines (more on this later).&lt;/p&gt;
&lt;p&gt;Therefore, at this level of abstraction, there are just a few graph topologies possible. Better yet, they could each be mapped to high-level use cases such as semantic search, language-agnostic document search, hybrid retrieval, and so on.&lt;/p&gt;
&lt;p&gt;But the crucial point is that, in most cases, tailoring the application did not require any changes to the graph&amp;rsquo;s shape. Users only need to identify their use case, find an example or a tutorial defining the shape of the Pipeline they need, and then swap the single components with other instances from the same category until they find the best combination for their exact requirements.&lt;/p&gt;
&lt;p&gt;This workflow was evident and encouraged: it was the philosophy behind Finder as well, and from version 0.6.0, Haystack immediately provided what are called &amp;ldquo;&lt;a href=&#34;https://docs.haystack.deepset.ai/docs/ready_made_pipelines&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ready-made Pipelines&lt;/a&gt;&amp;rdquo;: objects that initialized the graph on the user&amp;rsquo;s behalf, and expected as input the components to place in each point of the graph: for example a Reader and a Retriever, in case of simple Extractive QA.&lt;/p&gt;
&lt;p&gt;With this further abstraction on top of Pipeline, creating an NLP application became an action that doesn&amp;rsquo;t even require the user to be aware of the existence of the graph. In fact:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; ExtractiveQAPipeline(reader, retriever)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;is enough to get your Extractive QA applications ready to answer your questions. And you can do so with just another line.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;answers &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run(query&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;What did Einstein work on?&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;flexibility-powered-by-dags&#34;&gt;
  &amp;ldquo;Flexibility powered by DAGs&amp;rdquo;
  
&lt;/h2&gt;
&lt;p&gt;This abstraction is extremely powerful for the use cases that it was designed for. There are a few layers of ease of use vs. customization the user can choose from depending on their expertise, which help them progress from a simple ready-made Pipeline to fully custom graphs.&lt;/p&gt;
&lt;p&gt;However, the focus was oriented so much on the initial stages of the user&amp;rsquo;s journey that power-users&amp;rsquo; needs were sometimes forgotten. Such issues didn&amp;rsquo;t show immediately, but quickly added friction as soon as the users tried to customize their system beyond the examples from the tutorials and the documentation.&lt;/p&gt;
&lt;p&gt;For an example of these issues, let&amp;rsquo;s talk about pipelines with branches. Here are two small, apparently very similar pipelines.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-15-haystack-series-pipeline/branching-query-pipelines.png&#34; alt=&#34;Query Classification vs Hybrid Retrieval&#34;&gt;&lt;/p&gt;
&lt;p&gt;The first Pipeline represents the Hybrid Retrieval use case we&amp;rsquo;ve met with before. Here, the Query node sends its outputs to both retrievers, and they both produce some output. For the Reader to make sense of this data, we need a Join node that merges the two lists into one and a Ranker that takes the lists and sorts them again by similarity to the query. Ranker then sends the rearranged list to the Reader.&lt;/p&gt;
&lt;p&gt;The second Pipeline instead performs a simpler form of Hybrid Retrieval. Here, the Query node sends its outputs to a Query Classifier, which then triggers only one of the two retrievers, the one that is expected to perform better on it. The triggered Retriever then sends its output directly to the Reader, which doesn&amp;rsquo;t need to know which Retriever the data comes from. So, in this case, we don&amp;rsquo;t need the Join node.&lt;/p&gt;
&lt;p&gt;The two pipelines are built as you would expect, with a bunch of &lt;code&gt;add_node&lt;/code&gt; calls. You can even run them with the same identical code, which is the same code needed for every other Pipeline we&amp;rsquo;ve seen so far.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline_1 &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline_1&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;sparse_retriever, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;SparseRetriever&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Query&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline_1&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;dense_retriever, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DenseRetriever&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Query&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline_1&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;join_documents, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;JoinDocuments&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;SparseRetriever&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DenseRetriever&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline_1&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;rerank, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Ranker&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;JoinDocuments&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline_1&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;reader, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Reader&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;SparseRetriever&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DenseRetriever&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;answers &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; pipeline_1&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run(query&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;What did Einstein work on?&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline_2 &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline_2&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;query_classifier, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;QueryClassifier&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Query&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline_2&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;sparse_retriever, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DPRRetriever&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;QueryClassifier&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline_2&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;dense_retriever, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ESRetriever&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;QueryClassifier&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline_2&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;reader, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Reader&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;SparseRetriever&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DenseRetriever&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;answers &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; pipeline_2&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run(query&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;What did Einstein work on?&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Both pipelines run as you would expect them to. Hooray! Pipelines can branch and join!&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s take the first Pipeline and customize it further.&lt;/p&gt;
&lt;p&gt;For example, imagine we want to expand language support to include French. The dense Retriever has no issues handling several languages as long as we select a multilingual model; however, the sparse Retriever needs the keywords to match, so we must translate the queries to English to find some relevant documents in our English-only knowledge base.&lt;/p&gt;
&lt;p&gt;Here is what the Pipeline ends up looking like. Language Classifier sends all French queries over &lt;code&gt;output_1&lt;/code&gt; and all English queries over &lt;code&gt;output_2&lt;/code&gt;. In this way, the query passes through the Translator node only if it is written in French.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-15-haystack-series-pipeline/multilingual-hybrid-retrieval.png&#34; alt=&#34;Multilingual Hybrid Retrieval&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;language_classifier, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Query&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;translator, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Translator&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier.output_1&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;sparse_retriever, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;SparseRetriever&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Translator&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier.output_2&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;dense_retriever, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DenseRetriever&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier.output_1&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier.output_2&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;join_documents, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;JoinDocuments&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;SparseRetriever&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DenseRetriever&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;rerank, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Ranker&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;JoinDocuments&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;reader, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Reader&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Ranker&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;But&amp;hellip; wait. Let&amp;rsquo;s look again at the graph and at the code. DenseRetriever should receive &lt;em&gt;two&lt;/em&gt; inputs from Language Classifier: both &lt;code&gt;output_1&lt;/code&gt; and &lt;code&gt;output_2&lt;/code&gt;, because it can handle both languages. What&amp;rsquo;s going on? Is this a bug in &lt;code&gt;draw()&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;Thanks to the &lt;code&gt;debug=True&lt;/code&gt; parameter of &lt;code&gt;Pipeline.run()&lt;/code&gt;, we start inspecting what each node saw during the execution, and we realize quickly that our worst fears are true: this is a bug in the Pipeline implementation. The underlying library powering the Pipeline&amp;rsquo;s graphs takes the definition of Directed Acyclic Graphs very seriously and does not allow two nodes to be connected by more than one edge. There are, of course, other graph classes supporting this case, but Haystack happens to use the wrong one.&lt;/p&gt;
&lt;p&gt;Interestingly, Pipeline doesn&amp;rsquo;t even notice the problem and does not fail. It runs as the drawing suggests: when the query happens to be in French, only the sparse Retriever will process it.&lt;/p&gt;
&lt;p&gt;Clearly, this is not good for us.&lt;/p&gt;
&lt;p&gt;Well, let&amp;rsquo;s look for a workaround. Given that we&amp;rsquo;re Haystack power users by now, we realize that we can use a Join node with a single input as a &amp;ldquo;no-op&amp;rdquo; node. If we put it along one of the edges, that edge won&amp;rsquo;t directly connect Language Classifier and Dense Retriever, so the bug should be solved.&lt;/p&gt;
&lt;p&gt;So here is our current Pipeline:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-15-haystack-series-pipeline/multilingual-hybrid-retrieval-with-noop.png&#34; alt=&#34;Multilingual Hybrid Retrieval with No-Op Joiner&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;language_classifier, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Query&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;translator, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Translator&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier.output_1&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;sparse_retriever, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;SparseRetriever&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Translator&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier.output_2&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;no_op_join, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;NoOpJoin&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier.output_1&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;dense_retriever, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DenseRetriever&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;NoOpJoin&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier.output_2&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;join_documents, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;JoinDocuments&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;SparseRetriever&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DenseRetriever&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;rerank, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Ranker&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;JoinDocuments&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;reader, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Reader&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Ranker&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Great news: the Pipeline now runs as we expect! However, when we run a French query, the results are better but still surprisingly bad.&lt;/p&gt;
&lt;p&gt;What now? Is the dense Retriever still not running? Is the Translation node doing a poor job?&lt;/p&gt;
&lt;p&gt;Some debugging later, we realize that the Translator is amazingly good and the Retrievers are both running. But we forgot another piece of the puzzle: Ranker needs the query to be in the same language as the documents. It requires the English version of the query, just like the sparse Retriever does. However, right now, it receives the original French query, and that&amp;rsquo;s the reason for the lack of performance. We soon realize that this is very important also for the Reader.&lt;/p&gt;
&lt;p&gt;So&amp;hellip; how does the Pipeline pass the query down to the Ranker?&lt;/p&gt;
&lt;p&gt;Until this point, we didn&amp;rsquo;t need to know how exactly values are passed from one component to the next. We didn&amp;rsquo;t need to care about their inputs and outputs at all: Pipeline was doing all this dirty work for us. Suddenly, we need to tell the Pipeline which query to pass to the Ranker and we have no idea how to do that.&lt;/p&gt;
&lt;p&gt;Worse yet. There is &lt;em&gt;no way&lt;/em&gt; to reliably do that. The documentation seems to blissfully ignore the topic, docstrings give us no pointers, and looking at &lt;a href=&#34;https://github.com/deepset-ai/haystack/blob/aaee03aee87e96acd8791b9eff999055a8203237/haystack/pipelines/base.py#L483&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the routing code of Pipeline&lt;/a&gt; we quickly get dizzy and cut the chase. We dig through the Pipeline API several times until we&amp;rsquo;re confident that there&amp;rsquo;s nothing that can help.&lt;/p&gt;
&lt;p&gt;Well, there must be at least some workaround. Maybe we can forget about this issue by rearranging the nodes.&lt;/p&gt;
&lt;p&gt;One easy way out is to translate the query for both retrievers instead of only for the sparse one. This solution also eliminates the NoOpJoin node we introduced earlier, so it doesn&amp;rsquo;t sound too bad.&lt;/p&gt;
&lt;p&gt;The Pipeline looks like this now.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-15-haystack-series-pipeline/multilingual-hybrid-retrieval-two-translators.png&#34; alt=&#34;Multilingual Hybrid Retrieval with two Translators&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;language_classifier, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Query&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;translator, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Translator&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier.output_1&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;sparse_retriever, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;SparseRetriever&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Translator&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier.output_2&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;translator_2, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Translator2&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier.output_1&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;dense_retriever, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DenseRetriever&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Translator2&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier.output_2&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;join_documents, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;JoinDocuments&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;SparseRetriever&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DenseRetriever&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;rerank, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Ranker&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;JoinDocuments&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;reader, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Reader&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Ranker&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We now have two nodes that contain identical translator components. Given that they are stateless, we can surely place the same instance in both places, with different names, and avoid doubling its memory footprint just to work around a couple of Pipeline bugs. After all, Translator nodes use relatively heavy models for machine translation.&lt;/p&gt;
&lt;p&gt;This is what Pipeline replies as soon as we try.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;PipelineConfigError: Cannot add node &amp;#39;Translator2&amp;#39;. You have already added the same 
instance to the Pipeline under the name &amp;#39;Translator&amp;#39;.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Okay, so it seems like we can&amp;rsquo;t re-use components in two places: there is an explicit check against this, for some reason. Alright, let&amp;rsquo;s rearrange &lt;em&gt;again&lt;/em&gt; this Pipeline with this new constraint in mind.&lt;/p&gt;
&lt;p&gt;How about we first translate the query and then distribute it?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-15-haystack-series-pipeline/multilingual-hybrid-retrieval-translate-and-distribute.png&#34; alt=&#34;Multilingual Hybrid Retrieval, translate-and-distribute&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;language_classifier, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Query&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;translator, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Translator&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier.output_1&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;sparse_retriever, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;SparseRetriever&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Translator&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier.output_2&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;dense_retriever, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DenseRetriever&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Translator&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier.output_2&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;join_documents, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;JoinDocuments&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;SparseRetriever&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DenseRetriever&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;rerank, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Ranker&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;JoinDocuments&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;reader, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Reader&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Ranker&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Looks neat: there is no way now for the original French query to reach Ranker now. Right?&lt;/p&gt;
&lt;p&gt;We run the pipeline again and soon realize that nothing has changed. The query received by Ranker is still in French, untranslated. Shuffling the order of the &lt;code&gt;add_node&lt;/code&gt; calls and the names of the components in the &lt;code&gt;inputs&lt;/code&gt; parameters seems to have no effect on the graph. We even try to connect Translator directly with Ranker in a desperate attempt to forward the correct value, but Pipeline now starts throwing obscure, apparently meaningless error messages like:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;BaseRanker.run() missing 1 required positional argument: &amp;#39;documents&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Isn&amp;rsquo;t Ranker receiving the documents from JoinDocuments? Where did they go?&lt;/p&gt;
&lt;p&gt;Having wasted far too much time on this relatively simple Pipeline, we throw the towel, go to Haystack&amp;rsquo;s Discord server, and ask for help.&lt;/p&gt;
&lt;p&gt;Soon enough, one of the maintainers shows up and promises a workaround ASAP. You&amp;rsquo;re skeptical at this point, but the workaround, in fact, exists.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s just not very pretty.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-15-haystack-series-pipeline/multilingual-hybrid-retrieval-workaround.png&#34; alt=&#34;Multilingual Hybrid Retrieval, working version&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;language_classifier, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Query&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;translator_workaround, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;TranslatorWorkaround&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier.output_2&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;sparse_retriever, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;SparseRetriever&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier.output_1&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;TranslatorWorkaround&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;dense_retriever, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DenseRetriever&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;LanguageClassifier.output_1&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;TranslatorWorkaround&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;join_documents, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;JoinDocuments&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;SparseRetriever&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DenseRetriever&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;join_query_workaround, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;JoinQueryWorkaround&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;TranslatorWorkaround&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;JoinDocuments&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;rerank, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Ranker&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;JoinQueryWorkaround&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;reader, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Reader&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Ranker&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that you need two custom nodes: a wrapper for the Translator and a brand-new Join node.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#f0883e;font-weight:bold&#34;&gt;TranslatorWorkaround&lt;/span&gt;(TransformersTranslator):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    outgoing_edges &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;run&lt;/span&gt;(self, query):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        results, edge &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; super()&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run(query&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;query)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff7b72&#34;&gt;return&lt;/span&gt; {&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;**&lt;/span&gt;results, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;documents&amp;#34;&lt;/span&gt;: [] }, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;output_1&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;run_batch&lt;/span&gt;(self, queries):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff7b72&#34;&gt;pass&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#f0883e;font-weight:bold&#34;&gt;JoinQueryWorkaround&lt;/span&gt;(JoinNode):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;run_accumulated&lt;/span&gt;(self, inputs, &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;*&lt;/span&gt;args, &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;**&lt;/span&gt;kwargs):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff7b72&#34;&gt;return&lt;/span&gt; {&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;query&amp;#34;&lt;/span&gt;: inputs[&lt;span style=&#34;color:#a5d6ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;get(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;query&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#79c0ff&#34;&gt;None&lt;/span&gt;), &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;documents&amp;#34;&lt;/span&gt;: inputs[&lt;span style=&#34;color:#a5d6ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;get(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;documents&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#79c0ff&#34;&gt;None&lt;/span&gt;)}, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;output_1&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;run_batch_accumulated&lt;/span&gt;(self, inputs):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ff7b72&#34;&gt;pass&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Along with this beautiful code, we also receive an explanation about how the &lt;code&gt;JoinQueryWorkaround&lt;/code&gt; node works only for this specific Pipeline and is pretty hard to generalize, which is why it&amp;rsquo;s not present in Haystack right now. I&amp;rsquo;ll spare you the details: you will have an idea why by the end of this journey.&lt;/p&gt;
&lt;p&gt;Wanna play with this Pipeline yourself and try to make it work in another way? Check out the &lt;a href=&#34;https://drive.google.com/file/d/18Gqfd0O828T71Gc-IHeU4v7OXwaPk7Fc/view?usp=sharing&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colab&lt;/a&gt; or the &lt;a href=&#34;https://gist.github.com/ZanSara/33020a980f2f535e2529df4ca4e8f08a&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gist&lt;/a&gt; and have fun.&lt;/p&gt;
&lt;p&gt;Having learned only that it&amp;rsquo;s better not to implement unusual branching patterns with Haystack unless you&amp;rsquo;re ready for a fight, let&amp;rsquo;s now turn to the indexing side of your application. We&amp;rsquo;ll stick to the basics this time.&lt;/p&gt;
&lt;h2 id=&#34;indexing-pipelines&#34;&gt;
  Indexing Pipelines
  
&lt;/h2&gt;
&lt;p&gt;Indexing pipelines&amp;rsquo; main goal is to transform files into Documents from which a query pipeline can later retrieve information. They mostly look like the following.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-15-haystack-series-pipeline/indexing-pipeline.png&#34; alt=&#34;Indexing Pipeline&#34;&gt;&lt;/p&gt;
&lt;p&gt;And the code looks just like how you would expect it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;file_type_classifier, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;FileTypeClassifier&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;File&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;text_converter, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;TextConverter&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;FileTypeClassifier.output_1&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;pdf_converter, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;PdfConverter&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;FileTypeClassifier.output_2&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;docx_converter, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DocxConverter&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;FileTypeClassifier.output_4&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;join_documents, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;JoinDocuments&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;TextConverter&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;PdfConverter&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DocxConverter&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;preprocessor, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Preprocessor&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;JoinDocuments&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;document_store, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;DocumentStore&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Preprocessor&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pipeline&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run(file_paths&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;paths)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;There is no surprising stuff here. The starting node is File instead of Query, which seems logical given that this Pipeline expects a list of files, not a query. There is a document store at the end which we didn&amp;rsquo;t use in query pipelines so far, but it&amp;rsquo;s not looking too strange. It&amp;rsquo;s all quite intuitive.&lt;/p&gt;
&lt;p&gt;Indexing pipelines are run by giving them the paths of the files to convert. In this scenario, more than one Converter may run, so we place a Join node before the PreProcessor to make sense of the merge. We make sure that the directory contains only files that we can convert, in this case, .txt, .pdf, and .docx, and then we run the code above.&lt;/p&gt;
&lt;p&gt;The code, however, fails.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ValueError: Multiple non-default file types are not allowed at once.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The more we look at the error, the less it makes sense. What are non-default file types? Why are they not allowed at once, and what can I do to fix that?&lt;/p&gt;
&lt;p&gt;We head for the documentation, where we find a lead.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-15-haystack-series-pipeline/filetypeclassifier-docs.png&#34; alt=&#34;FileTypeClassifier documentation&#34;&gt;&lt;/p&gt;
&lt;p&gt;So it seems like the File Classifier can only process the files if they&amp;rsquo;re all of the same type.&lt;/p&gt;
&lt;p&gt;After all we&amp;rsquo;ve been through with the Hybrid Retrieval pipelines, this sounds wrong. We know that Pipeline can run two branches at the same time. We&amp;rsquo;ve been doing it all the time just a moment ago. Why can&amp;rsquo;t FileTypeClassifier send data to two converters just like LanguageClassifier sends data to two retrievers?&lt;/p&gt;
&lt;p&gt;Turns out, this is &lt;em&gt;not&lt;/em&gt; the same thing.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s compare the three pipelines and try to spot the difference.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-15-haystack-series-pipeline/all-branching-pipelines.png&#34; alt=&#34;All branching pipelines, side by side&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the first case, Query sends the same identical value to both Retrievers. So, from the component&amp;rsquo;s perspective, there&amp;rsquo;s a single output being produced: the Pipeline takes care of copying it for all nodes connected to it.&lt;/p&gt;
&lt;p&gt;In the second case, QueryClassifier can send the query to either Retriever but never to both. So, the component can produce two different outputs, but at every run, it will always return just one.&lt;/p&gt;
&lt;p&gt;In the third case, FileTypeClassifier may need to produce two different outputs simultaneously: for example, one with a list of text files and one with a list of PDFs. And it turns out this can&amp;rsquo;t be done. This is, unfortunately, a well-known limitation of the Pipeline/BaseComponent API design.
The output of a component is defined as a tuple, &lt;code&gt;(output_values, output_edge)&lt;/code&gt;, and nodes can&amp;rsquo;t produce a list of these tuples to send different values to different nodes.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s the end of the story. This time, there is no workaround. You must pass the files individually or forget about using a Pipeline for this task.&lt;/p&gt;
&lt;h2 id=&#34;validation&#34;&gt;
  Validation
  
&lt;/h2&gt;
&lt;p&gt;On top of these challenges, other tradeoffs had to be taken for the API to look so simple at first impact. One of these is connection validation.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s imagine we quickly skimmed through a tutorial and got one bit of information wrong: we mistakenly believe that in an Extractive QA Pipeline, you need to place a Reader in front of a Retriever. So we sit down and write this.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;p &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; Pipeline()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;p&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;reader, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Reader&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Query&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;p&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;add_node(component&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;retriever, name&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Retriever&amp;#34;&lt;/span&gt;, inputs&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Reader&amp;#34;&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Up to this point, running the script raises no error. Haystack is happy to connect these two components in this order. You can even &lt;code&gt;draw()&lt;/code&gt; this Pipeline just fine.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-15-haystack-series-pipeline/swapped-retriever-reader.png&#34; alt=&#34;Swapper Retriever/Reader Pipeline&#34;&gt;&lt;/p&gt;
&lt;p&gt;Alright, so what happens when we run it?&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;res &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; p&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run(query&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;What did Einstein work on?&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;BaseReader.run() missing 1 required positional argument: &amp;#39;documents&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This is the same error we&amp;rsquo;ve seen in the translating hybrid retrieval pipeline earlier, but fear not! Here, we can follow the suggestion of the error message by doing:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;res &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; p&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;run(query&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;What did Einstein work on?&amp;#34;&lt;/span&gt;, documents&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;document_store&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;get_all_documents())
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And to our surprise, this Pipeline doesn&amp;rsquo;t crash. It just hangs there, showing an insanely slow progress bar, telling us that some inference is in progress. A few hours later, we kill the process and consider switching to another framework because this one is clearly very slow.&lt;/p&gt;
&lt;p&gt;What happened?&lt;/p&gt;
&lt;p&gt;The cause of this issue is the same that makes connecting Haystack components in a Pipeline so effortless, and it&amp;rsquo;s related to the way components and Pipeline communicate. If you check &lt;code&gt;Pipeline.run()&lt;/code&gt;&amp;rsquo;s signature, you&amp;rsquo;ll see that it looks like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;run&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    self,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    query: Optional[str] &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#79c0ff&#34;&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    file_paths: Optional[List[str]] &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#79c0ff&#34;&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    labels: Optional[MultiLabel] &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#79c0ff&#34;&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    documents: Optional[List[Document]] &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#79c0ff&#34;&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    meta: Optional[Union[dict, List[dict]]] &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#79c0ff&#34;&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    params: Optional[dict] &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#79c0ff&#34;&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    debug: Optional[bool] &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#79c0ff&#34;&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;which mirrors the &lt;code&gt;BaseComponent.run()&lt;/code&gt; signature, the base class nodes have to inherit from.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;@abstractmethod&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;run&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    self,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    query: Optional[str] &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#79c0ff&#34;&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    file_paths: Optional[List[str]] &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#79c0ff&#34;&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    labels: Optional[MultiLabel] &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#79c0ff&#34;&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    documents: Optional[List[Document]] &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#79c0ff&#34;&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    meta: Optional[dict] &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#79c0ff&#34;&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;) &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;-&amp;gt;&lt;/span&gt; Tuple[Dict, str]:
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This match means a few things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Every component can be connected to every other because their inputs are identical.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Every component can only output the same variables received as input.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It&amp;rsquo;s impossible to tell if it makes sense to connect two components because their inputs and outputs always match.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Take this with a grain of salt: the actual implementation is far more nuanced than what I just showed you, but the problem is fundamentally this: components are trying to be as compatible as possible with all others and they have no way to signal, to the Pipeline or to the users, that they&amp;rsquo;re meant to be connected only to some nodes and not to others.&lt;/p&gt;
&lt;p&gt;In addition to this problem, to respect the shared signature, components often take inputs that they don&amp;rsquo;t use. A Ranker only needs documents, so all the other inputs required by the run method signature go unused. What do components do with the values? It depends:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some have them in the signature and forward them unchanged.&lt;/li&gt;
&lt;li&gt;Some have them in the signature and don&amp;rsquo;t forward them.&lt;/li&gt;
&lt;li&gt;Some don&amp;rsquo;t have them in the signature, breaking the inheritance pattern, and Pipeline reacts by assuming that they should be added unchanged to the output dictionary.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you check closely the two workaround nodes for the Hybrid Retrieval pipeline we tried to build before, you&amp;rsquo;ll notice the fix entirely focuses on altering the routing of the unused parameters &lt;code&gt;query&lt;/code&gt; and &lt;code&gt;documents&lt;/code&gt; to make the Pipeline behave the way the user expects. However, this behavior does not generalize: a different pipeline would require another behavior, which is why the components behave differently in the first place.&lt;/p&gt;
&lt;h2 id=&#34;wrapping-up&#34;&gt;
  Wrapping up
  
&lt;/h2&gt;
&lt;p&gt;I could go on for ages talking about the shortcomings of complex Pipelines, but I&amp;rsquo;d rather stop here.&lt;/p&gt;
&lt;p&gt;Along this journey into the guts of Haystack Pipelines, we&amp;rsquo;ve seen at the same time some beautiful APIs and the ugly consequences of their implementation. As always, there&amp;rsquo;s no free lunch: trying to over-simplify the interface will bite back as soon as the use cases become nontrivial.&lt;/p&gt;
&lt;p&gt;However, we believe that this concept has a huge potential and that this version of Pipeline can be improved a lot before the impact on the API becomes too heavy. In Haystack 2.0, armed with the experience we gained working with this implementation of Pipeline, we reimplemented it in a fundamentally different way, which will prevent many of these issues.&lt;/p&gt;
&lt;p&gt;In the next post, we&amp;rsquo;re going to see how.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Next: &lt;a href=&#34;https://www.zansara.dev/posts/2023-10-26-haystack-series-canals&#34; &gt;Canals: a new concept of Pipeline&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Previous: &lt;a href=&#34;https://www.zansara.dev/posts/2023-10-11-haystack-series-why&#34; &gt;Why rewriting Haystack?!&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;See the entire series here: &lt;a href=&#34;https://www.zansara.dev/series/haystack-2.0-series/&#34; &gt;Haystack 2.0 series&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Office Hours: RAG Pipelines</title>
      <link>https://www.zansara.dev/talks/2023-10-12-office-hours-rag-pipelines/</link>
      <pubDate>Thu, 12 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/talks/2023-10-12-office-hours-rag-pipelines/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1UXGi4raiCQmrxOfOexL-Qh0CVbtiSm89/view?usp=drive_link&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recording&lt;/a&gt;, &lt;a href=&#34;https://gist.github.com/ZanSara/5975901eea972c126f8e1c2341686dfb&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;notebook&lt;/a&gt;. All the material can also be found &lt;a href=&#34;https://drive.google.com/drive/folders/17CIfoy6c4INs0O_X6YCa3CYXkjRvWm7X?usp=drive_link&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;div class=&#39;iframe-wrapper&#39;&gt;
  &lt;iframe src=&#34;https://drive.google.com/file/d/1UXGi4raiCQmrxOfOexL-Qh0CVbtiSm89/preview&#34; width=100% height=100% allow=&#34;autoplay&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;In this &lt;a href=&#34;https://discord.com/invite/VBpFzsgRVF&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Office Hours&lt;/a&gt; I walk through the LLM support offered by Haystack 2.0 to this date: Generator, PromptBuilder, and how to connect them to different types of Retrievers to build Retrieval Augmented Generation (RAG) applications.&lt;/p&gt;
&lt;p&gt;In under 40 minutes we start from a simple query to ChatGPT up to a full pipeline that retrieves documents from the Internet, splits them into chunks and feeds them to an LLM to ground its replies.&lt;/p&gt;
&lt;p&gt;The talk indirectly shows also how Pipelines can help users compose these systems quickly, to visualize them, and helps them connect together different parts by producing verbose error messages.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why rewriting Haystack?!</title>
      <link>https://www.zansara.dev/posts/2023-10-11-haystack-series-why/</link>
      <pubDate>Wed, 11 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2023-10-11-haystack-series-why/</guid>
      <description>&lt;p&gt;Before even diving into what Haystack 2.0 is, how it was built, and how it works, let&amp;rsquo;s spend a few words about the whats and the whys.&lt;/p&gt;
&lt;p&gt;First of all, &lt;em&gt;what is&lt;/em&gt; Haystack?&lt;/p&gt;
&lt;p&gt;And next, why on Earth did we decide to rewrite it from the ground up?&lt;/p&gt;
&lt;h3 id=&#34;a-pioneer-framework&#34;&gt;
  A Pioneer Framework
  
&lt;/h3&gt;
&lt;p&gt;Haystack is a relatively young framework, its initial release dating back to &lt;a href=&#34;https://github.com/deepset-ai/haystack/releases/tag/0.1.0&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;November 28th, 2019&lt;/a&gt;. Back then, Natural Language Processing was a field that had just started moving its first step outside of research labs, and Haystack was one of the first libraries that promised enterprise-grade, production-ready NLP features. We were proud to enable use cases such as &lt;a href=&#34;https://medium.com/deepset-ai/what-semantic-search-can-do-for-you-ea5b1e8dfa7f&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;semantic search&lt;/a&gt;, &lt;a href=&#34;https://medium.com/deepset-ai/semantic-faq-search-with-haystack-6a03b1e13053&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FAQ matching&lt;/a&gt;, document similarity, document summarization, machine translation, language-agnostic search, and so on.&lt;/p&gt;
&lt;p&gt;The field was niche but constantly moving, and research was lively. &lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The BERT paper&lt;/a&gt; had been published a few months before Haystack&amp;rsquo;s first release, unlocking a small revolution. In the shade of much larger research labs, &lt;a href=&#34;https://www.deepset.ai/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;deepset&lt;/a&gt;, then just a pre-seed stage startup, was also pouring effort into &lt;a href=&#34;https://arxiv.org/abs/2104.12741&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;research&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/deepset&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;model training&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In those times, competition was close to non-existent. The field was still quite technical, and most people didn&amp;rsquo;t fully understand its potential. We were free to explore features and use cases at our own pace and set the direction for our product. This allowed us to decide what to work on, what to double down on, and what to deprioritize, postpone, or ignore. Haystack was nurturing its own garden in what was fundamentally a green field.&lt;/p&gt;
&lt;h3 id=&#34;chatgpt&#34;&gt;
  ChatGPT
  
&lt;/h3&gt;
&lt;p&gt;This rather idyllic situation came to an end all too abruptly at the end of November 2022, when &lt;a href=&#34;https://openai.com/blog/chatgpt&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ChatGPT was released&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For us in the NLP field, everything seemed to change overnight. Day by day. For &lt;em&gt;months&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The speed of progress went from lively to faster-than-light all at once. Every company with the budget to train an LLM seemed to be doing so, and researchers kept releasing new models just as quickly. Open-source contributors pushed to reduce the hardware requirements for inference lower and lower. My best memory of those times is the drama of &lt;a href=&#34;https://github.com/facebookresearch/llama/pull/73&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LlaMa&amp;rsquo;s first &amp;ldquo;release&amp;rdquo;&lt;/a&gt;: I remember betting on March 2nd that within a week I would be running LlaMa models on my laptop, and I wasn&amp;rsquo;t even surprised when my prediction &lt;a href=&#34;https://news.ycombinator.com/item?id=35100086&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;turned out true&lt;/a&gt; with the release of &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;llama.cpp&lt;/a&gt; on March 10th.&lt;/p&gt;
&lt;p&gt;Of course, keeping up with this situation was far beyond us. Competitors started to spawn like mushrooms, and our space was quickly crowded with new startups, far more agile and aggressive than us. We suddenly needed to compete and realized we weren&amp;rsquo;t used to it.&lt;/p&gt;
&lt;h3 id=&#34;promptnode-vs-farmreader&#34;&gt;
  PromptNode vs FARMReader
  
&lt;/h3&gt;
&lt;p&gt;Luckily, Haystack seemed capable of keeping up, at least for a while. Thanks to the efforts of &lt;a href=&#34;https://twitter.com/vladblagoje&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vladimir Blagojevic&lt;/a&gt;, a few weeks after ChatGPT became a sensation, we added some decent support for LLMs in the form of &lt;a href=&#34;https://github.com/deepset-ai/haystack/pull/3665&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PromptNode&lt;/a&gt;. Our SaaS team could soon bring new LLM-powered features to our customers. We even managed to add support for &lt;a href=&#34;https://github.com/deepset-ai/haystack/pull/3925&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Agents&lt;/a&gt;, another hot topic in the wake of ChatGPT.&lt;/p&gt;
&lt;p&gt;However, the go-to library for LLMs was not Haystack in the mind of most developers. It was &lt;a href=&#34;https://docs.langchain.com/docs/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LangChain&lt;/a&gt;, and for a long time, it seemed like we would never be able to challenge their status and popularity. Everyone was talking about it, everyone was building demos, products, and startups on it, its development speed was unbelievable and, in the day-to-day discourse of the newly born LLM community, Haystack was nowhere to be found.&lt;/p&gt;
&lt;p&gt;Why?&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s because no one even realized that Haystack, the semantic search framework from 2019, also supported LLMs. All our documentation, tutorials, blog posts, research efforts, models on HuggingFace, &lt;em&gt;everything&lt;/em&gt; was pointing towards semantic search. LLMs were nowhere to be seen.&lt;/p&gt;
&lt;p&gt;And semantic search was going down &lt;em&gt;fast&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.zansara.dev/posts/2023-10-11-haystack-series-why/reader-model-downloads.png&#34; alt=&#34;Reader Models downloads graph&#34;&gt;&lt;/p&gt;
&lt;p&gt;The image above shows today&amp;rsquo;s monthly downloads for one of deepset&amp;rsquo;s most successful models on HuggingFace,
&lt;a href=&#34;https://huggingface.co/deepset/roberta-base-squad2&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;deepset/roberta-base-squad2&lt;/a&gt;. This model performs &lt;a href=&#34;https://huggingface.co/tasks/question-answering&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;extractive Question Answering&lt;/a&gt;, our former primary use case before the release of ChatGPT. Even with more than one and a half million downloads monthly, this model is experiencing a disastrous collapse in popularity, and in the current landscape, it is unlikely to ever recover.&lt;/p&gt;
&lt;h3 id=&#34;a-sort-of-pivot&#34;&gt;
  A (Sort Of) Pivot
  
&lt;/h3&gt;
&lt;p&gt;In this context, around February 2023, we decided to bet on the rise of LLMs and committed to focus all our efforts towards becoming the #1 framework powering production-grade LLM applications.&lt;/p&gt;
&lt;p&gt;As we quickly realized, this was by far not an easy proposition. Extractive QA was not only ingrained deeply in our public image but in our codebase as well: implementing and maintaining PromptNode was proving more and more painful by the day, and when we tried to fit the concept of Agents into Haystack, it felt uncomfortably like trying to force a square peg into a round hole.&lt;/p&gt;
&lt;p&gt;Haystack pipelines made extractive QA straightforward for the users and were highly optimized for this use case. But supporting LLMs was nothing like enabling extractive QA. Using Haystack for LLMs was quite a painful experience, and at the same time, modifying the Pipeline class to accommodate them seemed like the best way to mess with all the users that relied on the current Pipeline for their existing, value-generating applications. Making mistakes with Pipeline could ruin us.&lt;/p&gt;
&lt;p&gt;With this realization in mind, we took what seemed the best option for the future of Haystack: a rewrite. The knowledge and experience we gained while working on Haystack 1 could fuel the design of Haystack 2 and act as a reference frame for it. Unlike our competitors, we already knew a lot about how to make NLP work at scale. We made many mistakes we would avoid in our next iteration. We knew that focusing on the best possible developer experience fueled the growth of Haystack 1 in the early days, and we were committed to doing the same for the next version of it.&lt;/p&gt;
&lt;p&gt;So, the redesign of Haystack started, and it started from the concept of Pipeline.&lt;/p&gt;
&lt;h3 id=&#34;fast-forward&#34;&gt;
  Fast-forward
  
&lt;/h3&gt;
&lt;p&gt;Haystack 2.0 hasn&amp;rsquo;t been released yet, but for now, it seems that we have made the right decision at the start of the year.&lt;/p&gt;
&lt;p&gt;Haystack&amp;rsquo;s name is starting to appear more often in discussions around LLMs. The general tone of the community is steadily shifting, and scaling up, rather than experimenting, is now the focus. Competitors are re-orienting themselves toward production-readiness, something we&amp;rsquo;re visibly more experienced with. At the same time, LangChain is becoming a victim of its own success, collecting more and more criticism for its lack of documentation, leaky abstractions, and confusing architecture. Other competitors are gaining steam, but the overall landscape no longer feels as hostile.&lt;/p&gt;
&lt;p&gt;In the next post, I will explore the technical side of Haystack 2.0 and delve deeper into the concept of Pipelines: what they are, how to use them, how they evolved from Haystack 1 to Haystack 2, and why.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Next: &lt;a href=&#34;https://www.zansara.dev/posts/2023-10-15-haystack-series-pipeline&#34; &gt;Haystack&amp;rsquo;s Pipeline - A Deep Dive&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Previous: &lt;a href=&#34;https://www.zansara.dev/posts/2023-10-10-haystack-series-intro&#34; &gt;Haystack 2.0: What is it?&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;See the entire series here: &lt;a href=&#34;https://www.zansara.dev/series/haystack-2.0-series/&#34; &gt;Haystack 2.0 series&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Haystack 2.0: What is it?</title>
      <link>https://www.zansara.dev/posts/2023-10-10-haystack-series-intro/</link>
      <pubDate>Tue, 10 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2023-10-10-haystack-series-intro/</guid>
      <description>&lt;p&gt;December is finally approaching, and with it the release of a &lt;a href=&#34;https://github.com/deepset-ai/haystack&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haystack&lt;/a&gt; 2.0. At &lt;a href=&#34;https://www.deepset.ai/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;deepset&lt;/a&gt;, we’ve been talking about it for months, we’ve been iterating on the core concepts what feels like a million times, and it looks like we’re finally getting ready for the approaching deadline.&lt;/p&gt;
&lt;p&gt;But what is it that makes this release so special?&lt;/p&gt;
&lt;p&gt;In short, Haystack 2.0 is a complete rewrite. A huge, big-bang style change. Almost no code survived the migration unmodified: we’ve been across the entire 100,000+ lines of the codebase and redone everything in under a year. For our small team, this is a huge accomplishment.&lt;/p&gt;
&lt;p&gt;In this series, I want to explain what Haystack 2 is from the perspective of the team that developed it. I&amp;rsquo;m gonna talk about what makes the new Pipeline so different from the old one, how to use new components and features, how these compare with the equivalent in Haystack 1 (when possible) and the principles that led the redesign. I had the pleasure (and sometimes the burden) of being involved in nearly all aspects of this process, from the requirements definition to the release, and I drove many of them through several iterations. In these posts, you can expect a mix of technical details and some diversions on the history and rationale behind each decision, as I’ve seen and understood them.&lt;/p&gt;
&lt;p&gt;For the curious readers, we have already released a lot of information about Haystack 2.0: check out this &lt;a href=&#34;https://github.com/deepset-ai/haystack/discussions/5568&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this Github Discussion&lt;/a&gt;, or join us on &lt;a href=&#34;https://discord.com/invite/VBpFzsgRVF&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haystack&amp;rsquo;s Discord server&lt;/a&gt; and peek into the &lt;code&gt;haystack-2.0&lt;/code&gt; channel for regular updates. We are also slowly building &lt;a href=&#34;https://docs.haystack.deepset.ai/v2.0/docs&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;brand new documentation&lt;/a&gt; for everything, and don’t worry: we’ll make sure to make it as outstanding as the Haystack 1.x version is.&lt;/p&gt;
&lt;p&gt;We also regularly feature 2.0 features in our Office Hours on Discord. Follow &lt;a href=&#34;https://twitter.com/Haystack_AI&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Haystack_AI&lt;/a&gt; or &lt;a href=&#34;https://twitter.com/deepset_ai&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@deepset_ai&lt;/a&gt; on Twitter to stay up-to-date, or &lt;a href=&#34;https://www.linkedin.com/company/deepset-ai&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;deepset&lt;/a&gt; on Linkedin. And you’ll find me and the rest of the team on &lt;a href=&#34;https://github.com/deepset-ai/haystack&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt; frantically (re)writing code and filing down the rough edges before the big release.&lt;/p&gt;
&lt;p&gt;Stay tuned!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Next: &lt;a href=&#34;https://www.zansara.dev/posts/2023-10-11-haystack-series-why&#34; &gt;Why rewriting Haystack?!&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;See the entire series here: &lt;a href=&#34;https://www.zansara.dev/series/haystack-2.0-series/&#34; &gt;Haystack 2.0 series&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multimodal Retriever: a live coding demo</title>
      <link>https://www.zansara.dev/demos/2023-10-07-multimodal-retriever/</link>
      <pubDate>Sat, 07 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/demos/2023-10-07-multimodal-retriever/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An (unofficial) Python SDK for Verbix</title>
      <link>https://www.zansara.dev/posts/2023-09-10-python-verbix-sdk/</link>
      <pubDate>Sun, 10 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2023-09-10-python-verbix-sdk/</guid>
      <description>&lt;p&gt;PyPI package: &lt;a href=&#34;https://pypi.org/project/verbix-sdk/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://pypi.org/project/verbix-sdk/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GitHub Repo: &lt;a href=&#34;https://github.com/ZanSara/verbix-sdk&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/ZanSara/verbix-sdk&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Minimal Docs: &lt;a href=&#34;https://github.com/ZanSara/verbix-sdk/blob/main/README.md&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/ZanSara/verbix-sdk/blob/main/README.md&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;As part of a larger side project which is still in the works (&lt;a href=&#34;https://github.com/ebisu-flashcards&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ebisu Flashcards&lt;/a&gt;), these days I found myself looking for some decent API for verbs conjugations in different languages. My requirements were &amp;ldquo;simple&amp;rdquo;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Supports many languages, including Italian, Portuguese and Hungarian&lt;/li&gt;
&lt;li&gt;Conjugates irregulars properly&lt;/li&gt;
&lt;li&gt;Offers an API access to the conjugation tables&lt;/li&gt;
&lt;li&gt;Refuses to conjugate anything except for known verbs&lt;/li&gt;
&lt;li&gt;(Optional) Highlights the irregularities in some way&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Surprisingly these seem to be a shortage of good alternatives in this field. All websites that host polished conjugation data don&amp;rsquo;t seem to offer API access (looking at you, &lt;a href=&#34;https://conjugator.reverso.net&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reverso&lt;/a&gt; &amp;ndash; you&amp;rsquo;ll get your own post one day), and most of the simples ones use heuristics to conjugate, which makes them very prone to errors. So for now I ended up choosing &lt;a href=&#34;https://verbix.com&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Verbix&lt;/a&gt; to start from.&lt;/p&gt;
&lt;p&gt;Unfortunately the website doesn&amp;rsquo;t inspire much confidence. I attempted to email the creator just to see them &lt;a href=&#34;https://verbix.com/contact.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;close their email account&lt;/a&gt; a while later, an &lt;a href=&#34;https://api.verbix.com/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;update in their API&lt;/a&gt; seems to have stalled half-way, and the &lt;a href=&#34;https://verb-blog.verbix.com/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog seems dead&lt;/a&gt;. I often have the feeling this site might go under any minute, as soon as their domain registration expires.&lt;/p&gt;
&lt;p&gt;But there are pros to it, as long as it lasts. Verbix offers verbs conjugation and nouns declination tables for some &lt;a href=&#34;https://verbix.com/languages/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;very niche languages, dialects and conlangs&lt;/a&gt;, to a degree that many other popular websites does not even come close. To support such variety they use heuristic to create the conjugation tables, which is not the best: for Hungarian, for example, I could easily get it to conjugate for me &lt;a href=&#34;https://verbix.com/webverbix/go.php?T1=meegy&amp;amp;Submit=Go&amp;amp;D1=121&amp;amp;H1=221&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;verbs that don&amp;rsquo;t exist&lt;/a&gt; or that have spelling mistakes. On the other hand their API do have a field that says whether the verb is known or not, which is a great way to filter out false positives.&lt;/p&gt;
&lt;p&gt;So I decided to go the extra mile and I wrote a small Python SDK for their API: &lt;a href=&#34;https://pypi.org/project/verbix-sdk/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;verbix-sdk&lt;/a&gt;. Enjoy it while it lasts&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Office Hours: Haystack 2.0</title>
      <link>https://www.zansara.dev/talks/2023-08-03-office-hours-haystack-2.0-status/</link>
      <pubDate>Thu, 03 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/talks/2023-08-03-office-hours-haystack-2.0-status/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1PyAlvJ22Z6o1bls07Do5kx2WMTdotsM7/view?usp=drive_link&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recording&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/1QFNisUk2HzwRL_27bpr338maxLvDBr9D/preview&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;. All the material can also be found &lt;a href=&#34;https://drive.google.com/drive/folders/1zmXwxsSgqDgvYf2ptjHocdtzOroqaudw?usp=drive_link&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;div class=&#39;iframe-wrapper&#39;&gt;
  &lt;iframe src=&#34;https://drive.google.com/file/d/1PyAlvJ22Z6o1bls07Do5kx2WMTdotsM7/preview&#34; width=100% height=100% allow=&#34;autoplay&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;In this &lt;a href=&#34;https://discord.com/invite/VBpFzsgRVF&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Office Hours&lt;/a&gt; I&amp;rsquo;ve presented for the first time to our Discord community a preview of the upcoming 2.0 release of Haystack, which has been in the works since the start of the year. As rumors started to arise at the presence of a &lt;code&gt;preview&lt;/code&gt; module in the latest Haystack 1.x releases, we took the opportunity to share this early draft of the project to collect early feedback.&lt;/p&gt;
&lt;p&gt;Haystack 2.0 is a total rewrite that rethinks many of the core concepts of the framework and makes LLMs support its primary concern, but makes sure to support all the usecases its predecessor enabled. The rewrite addresses some well-know, old issues about the pipeline&amp;rsquo;s design, the relationship between the pipeline, its components, and the document stores, and aims at improving drastically the developer experience and the framework&amp;rsquo;s extensibility.&lt;/p&gt;
&lt;p&gt;As the main designer of this rewrite, I walked the community through a slightly re-hashed version of the slide deck I&amp;rsquo;ve presented internally just a few days earlier in an All Hands on the same topic.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenNLP Meetup: A Practical Introduction to Image Retrieval</title>
      <link>https://www.zansara.dev/talks/2022-12-01-open-nlp-meetup/</link>
      <pubDate>Thu, 01 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/talks/2022-12-01-open-nlp-meetup/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=7Idjl3OR0FY&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Youtube link&lt;/a&gt;,
&lt;a href=&#34;https://gist.github.com/ZanSara/dc4b22e7ffe2a56647e0afba7537c46b&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;, &lt;a href=&#34;https://gist.github.com/ZanSara/9e8557830cc866fcf43a2c5623688c74&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colab&lt;/a&gt; (live coding).
All the material can also be found &lt;a href=&#34;https://drive.google.com/drive/folders/1_3b8PsvykHeM0jSHsMUWQ-4h_VADutcX?usp=drive_link&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;div class=&#39;iframe-wrapper&#39;&gt;
  &lt;iframe src=&#34;https://drive.google.com/file/d/19mxD-xUJ-14G-2XAqXEVpZfqR2MsSZTn/preview&#34; width=100% height=100% allow=&#34;autoplay&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;a-practical-introduction-to-image-retrieval&#34;&gt;
  A Practical Introduction to Image Retrieval
  
&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;by Sara Zanzottera from deepset&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Search should not be limited to text only. Recently, Transformers-based NLP models started crossing the boundaries of text data and exploring the possibilities of other modalities, like tabular data, images, audio files, and more. Text-to-text generation models like GPT now have their counterparts in text-to-image models, like Stable Diffusion. But what about search? In this talk we&amp;rsquo;re going to experiment with CLIP, a text-to-image search model, to look for animals matching specific characteristics in a dataset of pictures. Does CLIP know which one is &amp;ldquo;The fastest animal in the world&amp;rdquo;?&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;For the 7th &lt;a href=&#34;https://www.meetup.com/open-nlp-meetup/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenNLP meetup&lt;/a&gt; I presented the topic of Image Retrieval, a feature that I&amp;rsquo;ve recently added to Haystack in the form of a &lt;a href=&#34;https://docs.haystack.deepset.ai/docs/retriever#multimodal-retrieval&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MultiModal Retriever&lt;/a&gt; (see the &lt;a href=&#34;https://haystack.deepset.ai/tutorials/19_text_to_image_search_pipeline_with_multimodal_retriever&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tutorial&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The talk consists of 5 parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An introduction of the topic of Image Retrieval&lt;/li&gt;
&lt;li&gt;A mention of the current SOTA model (CLIP)&lt;/li&gt;
&lt;li&gt;An overview of Haystack&lt;/li&gt;
&lt;li&gt;A step-by-step description of how image retrieval applications can be implemented with Haystack&lt;/li&gt;
&lt;li&gt;A live coding session where I start from a blank Colab notebook and build a fully working image retrieval system from the ground up, to the point where I can run queries live.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Towards the end I mention briefly an even more advanced version of this image retrieval system, which I had no time to implement live. However, I later built a notebook implementing such system and you can find it here: &lt;a href=&#34;https://gist.github.com/ZanSara/31ed3fc8252bb74b1952f2d0fe253ed0&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cheetah.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The slides were generated from the linked Jupyter notebook with &lt;code&gt;jupyter nbconvert Dec_1st_OpenNLP_Meetup.ipynb --to slides --post serve&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adopting PyQt For Beam Instrumentation GUI Development At CERN</title>
      <link>https://www.zansara.dev/publications/thpv014/</link>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/publications/thpv014/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;
  Abstract
  
&lt;/h2&gt;
&lt;p&gt;As Java GUI toolkits become deprecated, the Beam Instrumentation (BI)group at CERN has investigated alternatives and selected PyQt as one of the suitable technologies for future GUIs, in accordance with the paper presented at ICALEPCS19. This paper presents tools created, or adapted, to seamlessly integrate future PyQt GUI development alongside current Java oriented workflows and the controls environment. This includes (a) creating a project template and a GUI management tool to ease and standardize our development process, (b) rewriting our previously Java-centric Expert GUI Launcher to be language-agnostic and (c) porting a selection of operational GUIs from Java to PyQt, to test the feasibility of the development process and identify bottlenecks. To conclude, the challenges we anticipate for the BI GUI developer community in adopting this new technology are also discussed.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Get the full text here: &lt;a href=&#34;https://www.zansara.dev/publications/thpv014.pdf&#34; &gt;Adopting PyQt For Beam Instrumentation GUI Development At CERN&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Get the poster: &lt;a href=&#34;https://www.zansara.dev/publications/thpv014-poster.pdf&#34; &gt;PDF&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Publisher&amp;rsquo;s entry: &lt;a href=&#34;https://accelconf.web.cern.ch/icalepcs2021/doi/JACoW-ICALEPCS2021-THPV014.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;THPV014&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Evolution of the CERN Beam Instrumentation Offline Analysis Framework (OAF)</title>
      <link>https://www.zansara.dev/publications/thpv042/</link>
      <pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/publications/thpv042/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;
  Abstract
  
&lt;/h2&gt;
&lt;p&gt;The CERN accelerators require a large number of instruments, measuring different beam parameters like position, losses, current etc. The instruments’ associated electronics and software also produce information about their status. All these data are stored in a database for later analysis. The Beam Instrumentation group developed the Offline Analysis Framework some years ago to regularly and systematically analyze these data. The framework has been successfully used for nearly 100 different analyses that ran regularly by the end of the LHC run 2. Currently it is being updated for run 3 with modern and efficient tools to improve its usability and data analysis power. In particular, the architecture has been reviewed to have a modular design to facilitate the maintenance and the future evolution of the tool. A new web based application is being developed to facilitate the users’ access both to online configuration and to results. This paper will describe all these evolutions and outline possible lines of work for further improvements.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Get the full text here: &lt;a href=&#34;https://www.zansara.dev/publications/thpv042.pdf&#34; &gt;Evolution of the CERN Beam Instrumentation Offline Analysis Framework (OAF)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Publisher&amp;rsquo;s entry: &lt;a href=&#34;https://accelconf.web.cern.ch/icalepcs2021/doi/JACoW-ICALEPCS2021-THPV042.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;THPV042&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My Dotfiles</title>
      <link>https://www.zansara.dev/posts/2021-12-11-dotfiles/</link>
      <pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/posts/2021-12-11-dotfiles/</guid>
      <description>&lt;p&gt;GitHub Repo: &lt;a href=&#34;https://github.com/ZanSara/dotfiles&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/ZanSara/dotfiles&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;What Linux developer would I be if I didn&amp;rsquo;t also have my very own dotfiles repo?&lt;/p&gt;
&lt;p&gt;After many years of iterations I finally found a combination that lasted quite a while, so I figured it&amp;rsquo;s time to treat them as a real project. It was originally optimized for my laptop, but then I realized it works quite well on my three-monitor desk setup as well without major issues.&lt;/p&gt;
&lt;p&gt;It sports:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Airblader/i3&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;i3-wm&lt;/a&gt; as window manager (of course, with gaps),&lt;/li&gt;
&lt;li&gt;The typical trio of &lt;a href=&#34;https://github.com/polybar/polybar&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;polybar&lt;/a&gt; , &lt;a href=&#34;https://github.com/davatorium/rofi&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rofi&lt;/a&gt; and &lt;a href=&#34;https://github.com/dunst-project/dunst&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dunst&lt;/a&gt; to handle top bar, start menu and notifications respectively,&lt;/li&gt;
&lt;li&gt;The odd choice of &lt;a href=&#34;https://github.com/nullgemm/ly&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ly&lt;/a&gt; as my display manager. I just love the minimal, TUI aesthetics of it. Don&amp;rsquo;t forget to enable Doom&amp;rsquo;s flames!&lt;/li&gt;
&lt;li&gt;A minimalistic animated background from &lt;a href=&#34;https://www.jwz.org/xscreensaver/screenshots/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;xscreensaver&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=spQRFDmDMeg&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grav&lt;/a&gt;. It&amp;rsquo;s configured to leave no trails and stay black and white. An odd choice, and yet it manages to use no resources, stay very minimal, and bring a very (in my opinion) futuristic look to the entire setup.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ohmybash/oh-my-bash/tree/master/themes/font&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OhMyBash&lt;/a&gt; with the &lt;a href=&#34;https://github.com/ohmybash/oh-my-bash/tree/master/themes/font&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;font&lt;/a&gt; theme,&lt;/li&gt;
&lt;li&gt;Other small amenities, like &lt;a href=&#34;https://docs.rockylinux.org/gemstones/nmtui/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nmtui&lt;/a&gt; for network management, Japanese-numerals as workspace indicators, etc..&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Feel free to take what you like. If you end up using any of these, make sure to share the outcomes!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ZanzoCam: An open-source alpine web camera</title>
      <link>https://www.zansara.dev/talks/2021-05-24-zanzocam-pavia/</link>
      <pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/talks/2021-05-24-zanzocam-pavia/</guid>
      <description>&lt;p&gt;Slides: &lt;a href=&#34;https://www.zansara.dev/talks/2021-05-24-zanzocam-pavia.pdf&#34; &gt;ZanzoCam: An open-source alpine web camera&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;On May 24th 2021 I held a talk about the &lt;a href=&#34;https://zanzocam.github.io/en&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ZanzoCam project&lt;/a&gt;
as invited speaker for the &lt;a href=&#34;http://hsw2021.gnudd.com/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Hardware and Software Codesign&amp;rdquo;&lt;/a&gt; course at
&lt;a href=&#34;https://portale.unipv.it/it&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Università di Pavia&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The slides go through the entire lifecycle of the &lt;a href=&#34;https://zanzocam.github.io/en&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ZanzoCam project&lt;/a&gt;,
from the very inception of it, the market research, our decision process, earlier prototypes, and
then goes into a more detailed explanation of the the design and implementation of the project from
a hardware and software perspective, with some notes about our financial situation and project management.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Our Journey From Java to PyQt and Web For CERN Accelerator Control GUIs</title>
      <link>https://www.zansara.dev/publications/tucpr03/</link>
      <pubDate>Sun, 30 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/publications/tucpr03/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;
  Abstract
  
&lt;/h2&gt;
&lt;p&gt;For more than 15 years, operational GUIs for accelerator controls and some lab applications for equipment experts have been developed in Java, first with Swing and more recently with JavaFX. In March 2018, Oracle announced that Java GUIs were not part of their strategy anymore*. They will not ship JavaFX after Java 8 and there are hints that they would like to get rid of Swing as well. This was a wakeup call for us. We took the opportunity to reconsider all technical options for developing operational GUIs. Our options ranged from sticking with JavaFX, over using the Qt framework (either using PyQt or developing our own Java Bindings to Qt), to using Web technology both in a browser and in native desktop applications. This article explains the reasons for moving away from Java as the main GUI technology and describes the analysis and hands-on evaluations that we went through before choosing the replacement.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Get the full text here: &lt;a href=&#34;https://www.zansara.dev/publications/tucpr03.pdf&#34; &gt;Our Journey From Java to PyQt and Web For CERN Accelerator Control GUIs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Publisher&amp;rsquo;s entry: &lt;a href=&#34;https://accelconf.web.cern.ch/icalepcs2019/doi/JACoW-ICALEPCS2019-TUCPR03.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TUCPR03&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ZanzoCam</title>
      <link>https://www.zansara.dev/projects/zanzocam/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/projects/zanzocam/</guid>
      <description>&lt;p&gt;Main website: &lt;a href=&#34;https://zanzocam.github.io/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://zanzocam.github.io/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;ZanzoCam is a low-power, low-frequency camera based on Raspberry Pi, designed to operate autonomously in remote locations and under harsh conditions. It was designed and developed between 2019 and 2021 for &lt;a href=&#34;https://www.cai.it/gruppo_regionale/gr-lombardia/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CAI Lombardia&lt;/a&gt; by a team of two people, with me as the software developer and the other responsible for the hardware design. CAI later deployed several of these devices on their affiliate huts.&lt;/p&gt;
&lt;p&gt;ZanzoCams are designed to work reliably in the harsh conditions of alpine winters, be as power-efficient as possible, and tolerate unstable network connections: they feature a robust HTTP- or FTP-based picture upload strategy which is remotely configurable from a very simple, single-file web panel. The camera software also improves on the basic capabilities of picamera to take pictures in dark conditions, making ZanzoCams able to shoot good pictures for a few hours after sunset.&lt;/p&gt;
&lt;p&gt;The camera is highly configurable: photo size and frequency, server address and protocol, all the overlays (color, size, position, text and images) and several other parameters can be configured remotely without the need to expose any ports of the device to the internet. They work reliably without the need for a VPN and at the same time are quite secure by design.&lt;/p&gt;
&lt;p&gt;ZanzoCams mostly serve CAI and the hut managers for self-promotion, and help hikers and climbers assess the local conditions before attempting a hike. Pictures taken for this purposes are sent to &lt;a href=&#34;https://www.rifugi.lombardia.it/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RifugiLombardia&lt;/a&gt;, and you can see many of them &lt;a href=&#34;https://www.rifugi.lombardia.it/territorio-lombardo/webcam&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;at this page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, it has also been used by glaciologists to monitor glacier conditions, outlook and extension over the years. &lt;a href=&#34;https://www.servizioglaciologicolombardo.it/webcam-3&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Here you can see their webcams&lt;/a&gt;, some of which are ZanzoCams.&lt;/p&gt;
&lt;p&gt;Here is the latest picture from &lt;a href=&#34;https://maps.app.goo.gl/PwdVC82VHwdPZJDE6&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rifugio M. Del Grande - R. Camerini&lt;/a&gt;, the test location for the original prototype:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://webcam.rifugi.lombardia.it/rifugio/00003157/pictures/image__0.jpg&#34; alt=&#34;ZanzoCam of Rifugio M. Del Grande - R. Camerini&#34;&gt;&lt;/p&gt;
&lt;p&gt;And here is one of the cameras serving a local glaciology research group, &lt;a href=&#34;https://www.servizioglaciologicolombardo.it/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Servizio Glaciologico Lombardo&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://webcam.rifugi.lombardia.it/rifugio/90003157/pictures/image__0.jpg&#34; alt=&#34;ZanzoCam of M. Disgrazia&#34;&gt;&lt;/p&gt;
&lt;p&gt;Both of these cameras are fully solar-powered.&lt;/p&gt;
&lt;p&gt;ZanzoCam is fully open-source: check the &lt;a href=&#34;https://github.com/ZanzoCam?view_as=public&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub repo&lt;/a&gt;. Due to this decision of open-sourcing the project, I was invited by &lt;a href=&#34;https://portale.unipv.it/it&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Università di Pavia&lt;/a&gt; to hold a lecture about the project as part of their &lt;a href=&#34;http://hsw2021.gnudd.com/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Hardware and Software Codesign&amp;rdquo;&lt;/a&gt;. Check out the slides of the lecture &lt;a href=&#34;talks/zanzocam-pavia/&#34; &gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Evaluation of Qt as GUI Framework for Accelerator Controls</title>
      <link>https://www.zansara.dev/publications/msc-thesis/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/publications/msc-thesis/</guid>
      <description>&lt;p&gt;This is the full-text of my MSc thesis, written in collaboration with
&lt;a href=&#34;https://www.polimi.it/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Politecnico di Milano&lt;/a&gt; and &lt;a href=&#34;https://home.cern/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CERN&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Get the full text here: &lt;a href=&#34;https://www.zansara.dev/publications/msc-thesis.pdf&#34; &gt;Evaluation of Qt as GUI Framework for Accelerator Controls&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Publisher&amp;rsquo;s entry: &lt;a href=&#34;https://hdl.handle.net/10589/144860&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10589/144860&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CAI Sovico&#39;s Website</title>
      <link>https://www.zansara.dev/projects/booking-system/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/projects/booking-system/</guid>
      <description>&lt;p&gt;Main website: &lt;a href=&#34;https://www.caisovico.it&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.caisovico.it&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Since my bachelor studies I have maintained the IT infrastructure of an alpine hut, &lt;a href=&#34;https://maps.app.goo.gl/PwdVC82VHwdPZJDE6&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rifugio M. Del Grande - R. Camerini&lt;/a&gt;. I count this as my first important project, one that people, mostly older and not very tech savvy, depended on to run a real business.&lt;/p&gt;
&lt;p&gt;The website went through several iterations as web technologies evolved, and well as the type of servers we could afford. Right now it features minimal HTML/CSS static pages, plus a reservations system written on a PHP 8 / MySQL backend with a vanilla JS frontend. It also includes an FTP server that supports a couple of &lt;a href=&#34;https://www.zansara.dev/projects/zanzocam/&#34; &gt;ZanzoCams&lt;/a&gt; and a &lt;a href=&#34;http://www.meteoproject.it/ftp/stazioni/caisovico/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;weather monitoring station&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://www.zansara.dev/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/about/</guid>
      <description>&lt;p&gt;I am a Python and GenAI software engineer based in Portugal, currently working for BNP Paribas as a Senior Developer on internal AI tools. I regularly to give &lt;a href=&#34;https://www.zansara.dev/talks&#34; &gt;talks&lt;/a&gt; on the topic of GenAI, LLMs, and related topics.&lt;/p&gt;
&lt;p&gt;Earlier in my career I&amp;rsquo;ve been Lead AI Engineer for &lt;a href=&#34;https://www.kwal.ai/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kwal&lt;/a&gt; working on voice agents and conversation analysis with LLMs for the recruitment industry.
I&amp;rsquo;ve been NLP Engineer at &lt;a href=&#34;https://www.deepset.ai/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;deepset&lt;/a&gt;, a German startup working on NLP &lt;a href=&#34;https://www.deepset.ai/about&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;since &amp;ldquo;before it was cool&amp;rdquo;&lt;/a&gt;, where I was the &lt;a href=&#34;https://github.com/deepset-ai/haystack/graphs/contributors&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;main contributor&lt;/a&gt; of &lt;a href=&#34;https://haystack.deepset.ai/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haystack&lt;/a&gt;, their open-source framework for building highly customizable,  production-ready NLP and LLM applications.
I started my career at &lt;a href=&#34;https://home.cern/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CERN&lt;/a&gt;, where I had the privilege of driving one &lt;a href=&#34;https://www.zansara.dev/publications/tucpr03/&#34; &gt;major decision&lt;/a&gt; to migrate the graphical interface&amp;rsquo;s software of the accelerator&amp;rsquo;s control systems from Java to PyQt, and then of helping a client department &lt;a href=&#34;https://www.zansara.dev/publications/thpv014/&#34; &gt;migrate&lt;/a&gt; to this stack. I have also worked on other infrastructure and data pipelines, some of which resulted in &lt;a href=&#34;https://www.zansara.dev/publications/thpv042/&#34; &gt;publication&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Outside of work I have more &lt;a href=&#34;https://www.zansara.dev/projects&#34; &gt;pet projects&lt;/a&gt; than free time to dedicate to them, just as everyone else; I love science fiction and space exploration, I enjoy hiking and learning languages, as much as such process can be enjoyed.&lt;/p&gt;
&lt;p&gt;I speak native Italian and fluent English, but I&amp;rsquo;ve also learned French during my time at CERN, I&amp;rsquo;m studying Hungarian for family reasons, and Portuguese because I currently live there. I still can understand some Russian and I have a very basic understanding of Chinese, both from my teenage and university years.&lt;/p&gt;
&lt;p&gt;Check out also my &lt;a href=&#34;https://www.zansara.dev/projects&#34; &gt;projects&lt;/a&gt;, my &lt;a href=&#34;https://www.zansara.dev/publications&#34; &gt;publications&lt;/a&gt; and my &lt;a href=&#34;https://www.zansara.dev/talks&#34; &gt;talks&lt;/a&gt;. If you prefer newsletters you can find my posts also on &lt;a href=&#34;https://zansara.substack.com/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Substack&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The best way to get in touch with me is through &lt;a href=&#34;mailto:blog@zansara.dev&#34; &gt;email&lt;/a&gt; or &lt;a href=&#34;https://www.linkedin.com/in/sarazanzottera&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cards</title>
      <link>https://www.zansara.dev/cards/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.zansara.dev/cards/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.zansara.dev/me/card-light.png&#34; &gt;Light - Wide&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zansara.dev/me/card-dark.png&#34; &gt;Dark - Wide&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zansara.dev/me/card-light-mobile.png&#34; &gt;Light - Mobile&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zansara.dev/me/card-dark-mobile.png&#34; &gt;Dark - Mobile&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>